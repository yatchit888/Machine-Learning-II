{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "## This notebook outlines the main concepts and phases involved in NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP Pipeline](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "- Data Acquisition\n",
    "- Text Extraction\n",
    "- Text Cleaning\n",
    "- Text Pre-processing\n",
    "- Feature Engineering**\n",
    "- Modeling***\n",
    "- Evaluation***\n",
    "- Deployment****\n",
    "- Monitoring & Model Improvement****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **   Will be seen in detail in the next lecture\n",
    "\n",
    "- ***  **Deep LearningI** course\n",
    "\n",
    "- ****   **Full Stack Data Science Systems** course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a public dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Easily available\n",
    "- If found a similar dataset that can work for your problem in hand\n",
    "    - Download, build a model, evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wiki pages\n",
    "- Articles\n",
    "- Webpages\n",
    "\n",
    "    - Data annotation to be done later for labeling the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Product Intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have to work with product team\n",
    "    - collect more data\n",
    "    - very important\n",
    "\n",
    "Pros\n",
    "- Accurate\n",
    "\n",
    "Cons\n",
    "- Takes a long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a small dataset to create more data\n",
    "    - Synonym replacement\n",
    "        - Randomly choose \"k\" words in a sentence that are not stop words\n",
    "        - Replace these words with their synonyms\n",
    "    - Back Translation\n",
    "    - TF-IDF-based word replacement\n",
    "    - Bigram Flipping\n",
    "    - Entity replacement\n",
    "    - Noise addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Process of extracting raw text from input data source\n",
    "    - Remove all unwanted non-textual information\n",
    "        - Markup data\n",
    "        - Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source formats](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Text-formats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping\n",
    "Scrape the following url and extract the text\n",
    "\n",
    "URL: https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\n",
    "\n",
    "\n",
    "![Stack overflow page](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Stackoverflow.png)\n",
    "\n",
    "\n",
    "\n",
    "Task\n",
    "- look at the url\n",
    "- extract question\n",
    "- extract answer\n",
    "- Display them as shown below\n",
    "\n",
    "This is **Text extraction from webpages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question = \n",
      " What is the module/method used to get the current time?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer = \n",
      " Use:\n",
      ">>> import datetime\n",
      ">>> datetime.datetime.now()\n",
      "datetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now())\n",
      "2009-01-06 15:08:24.789150\n",
      "\n",
      "And just the time:\n",
      ">>> datetime.datetime.now().time()\n",
      "datetime.time(15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now().time())\n",
      "15:08:24.789150\n",
      "\n",
      "See the documentation for more information.\n",
      "To save typing, you can import the datetime object from the datetime module:\n",
      ">>> from datetime import datetime\n",
      "\n",
      "Then remove the leading datetime. from all of the above.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
    "html = \n",
    "soupified = \n",
    "question_text = \n",
    "print(f\"Question = \\n {question_text.get_text().strip()}\")\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "answer_text = \n",
    "answer = \n",
    "print(f\"Answer = \\n {answer.get_text().strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction from PDF documents\n",
    "- Use the following PDF to text converstion libraries\n",
    "    - PyPDF\n",
    "    - PDFMiner\n",
    "    - PyPDF2\n",
    "    - Fitz\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction from scanned images\n",
    "- Use Tesseract OCR library\n",
    "- Use wget to download the png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task is to extract text from this url: https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_scanned_image.png\n",
    "\n",
    "Input:\n",
    "\n",
    "![Scanned Image](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_scanned_image.png)\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "\n",
    "‚Äôin the nineteenth century the only Kind of linguistics considered\\nseriously\n",
    "was this comparative and historical study of words in languages\\nknown or\n",
    "believed to Fe cognate‚Äîsay the Semitic languages, or the Indo-\\nEuropean\n",
    "languages. It is significant that the Germans who really made\\nthe subject what\n",
    "it was, used the term Indo-germanisch. Those who know\\nthe popular works of \n",
    "Otto Jespersen will remember how fitmly he\\ndeclares that linguistic \n",
    "science is historical. And those who have noticed‚Äô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Cleaning\n",
    "\n",
    "#### Unicode Removal from text\n",
    "- Remove non-textual symbols and special characters\n",
    "- Use string.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I love this!!! üòä  Let's all be happy !üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love this!!! üòä  Let's all be happy !üòä\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis\n",
    "- Tweets\n",
    "- Articles\n",
    "- Any text\n",
    "- Emoji Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"I love this!!! \\xf0\\x9f\\x98\\x8a  Let's all be happy !\\xf0\\x9f\\x98\\x8a\"\n"
     ]
    }
   ],
   "source": [
    "print(text.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling Correction\n",
    "- textblob\n",
    "- pyspellchecker\n",
    "- Microsoft's API Spell Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### textblob\n",
    "- pip install textblob\n",
    "- Import TextBlob from textblob\n",
    "- Use TextBlob(string).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: tqdm in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.50.2)\n",
      "Requirement already satisfied: regex in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2020.10.15)\n",
      "Requirement already satisfied: joblib in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (0.17.0)\n",
      "Requirement already satisfied: click in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"maching\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = TextBlob(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_word = str(b.correct())\n",
    "corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspellchecker : https://pyspellchecker.readthedocs.io/en/latest/\n",
    "- pip install pyspellchecker\n",
    "- Import SpellChecker from spellchecker\n",
    "- Use SpellChecker.unknown(incorrect_string)\n",
    "- Iterate through misspelled using\n",
    "    - SpellChecker.correction(word)\n",
    "    - SpellChecker.candidates(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Using cached pyspellchecker-0.5.6-py2.py3-none-any.whl (2.5 MB)\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.5.6\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'computr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'computr'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspellings = spell.unknown([word])\n",
    "misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.correction(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute', 'computer'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.candidates(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Microsoft's API Spell Checker\n",
    "- API Request from a Python client application example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "api_key = \"<ENTER-KEY-HERE>\"\n",
    "example_text = \"Hollo, wrld\" \n",
    "\n",
    "data = {'text': example_text}\n",
    "params = {\n",
    "    'mkt':'en-us',\n",
    "    'mode':'proof'\n",
    "    }\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    'Ocp-Apim-Subscription-Key': api_key,\n",
    "    }\n",
    "response = requests.post(endpoint, headers=headers, params=params, data=data)\n",
    "json_response = response.json()\n",
    "print(json.dumps(json_response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text Pre-processing\n",
    "### Process of preparing raw text and extract knowledge\n",
    "- Sentence Segmentation\n",
    "- Word Tokenization\n",
    "- Stop words\n",
    "- Stemming & Lemmatization\n",
    "- Contractions\n",
    "- Whitespace\n",
    "- POS tagging\n",
    "- Parsing\n",
    "- Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Segmentation : Breaking big document into sentences\n",
    "- nltk\n",
    "- sent_tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task: Given a piece of text, split them into sentences, and print them one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"\"\"In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life. If we were asked to build such an application, think about how we would approach doing so at our organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step. This step-by-step processing of text is known as pipeline. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: regex in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from nltk) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.',\n",
       " 'If we were asked to build such an application, think about how we would approach doing so at our organization.',\n",
       " 'We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.',\n",
       " 'Since language processing is involved, we would also list all the forms of text processing needed at each step.',\n",
       " 'This step-by-step processing of text is known as pipeline.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(mytext)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      " In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.\n",
      "Sentence 2\n",
      " If we were asked to build such an application, think about how we would approach doing so at our organization.\n",
      "Sentence 3\n",
      " We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.\n",
      "Sentence 4\n",
      " Since language processing is involved, we would also list all the forms of text processing needed at each step.\n",
      "Sentence 5\n",
      " This step-by-step processing of text is known as pipeline.\n"
     ]
    }
   ],
   "source": [
    "# for sent in sentences:\n",
    "#     print(sent)\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {idx+1}\\n {sentence}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization : Breaking a sentence into words (tokens)\n",
    "- word_tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task: Given a sentence, split them into words and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This step-by-step processing of text is known as pipeline.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'step-by-step',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'text',\n",
       " 'is',\n",
       " 'known',\n",
       " 'as',\n",
       " 'pipeline',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal Stop Words : Removing words that would be useless to our processing\n",
    "- import stopwords from corpus\n",
    "- Check \n",
    "    - if the present word is not a stop word, allow it\n",
    "    - else throw it away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'step-by-step', 'processing', 'text', 'known', 'pipeline', '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words = [word for word in words if word not in stop_words]\n",
    "clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_words = [word if word != \"This\" else 'NA' for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NA',\n",
       " 'step-by-step',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'text',\n",
       " 'is',\n",
       " 'known',\n",
       " 'as',\n",
       " 'pipeline',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming : Reduces the word to a base form\n",
    "- PorterStemmer.stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'airlin'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"airline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization : Reducing the word to a base form (available in dictionary)\n",
    "- WordNetLemmatizer.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'airline'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"airlines\", pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is better ? Stemming or Lemmatization?\n",
    "- Try some samples and make a decision\n",
    "    - well-dressed\n",
    "    - better\n",
    "    - airliner\n",
    "    - was\n",
    "    - meeting\n",
    "    - go\n",
    "    - went\n",
    "    - going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['well-dressed', 'airliner', 'better', 'was', 'meeting', 'uncomfortable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10000 samples in Class A\n",
    "1000 samples in Class B\n",
    "Imbalanced dataset\n",
    "\n",
    "1000 samples in Class A\n",
    "1000 samples in Class B\n",
    "Balanced dataset\n",
    "\n",
    "10000 samples in Class A\n",
    "10000 samples in Class B (duplicate 10 times of your sample size)\n",
    "Balanced dataset\n",
    "\n",
    "5000 samples in Class A (Down sampling - reduce half of your sample size)\n",
    "5000 samples in Class B (Up Sampling - increase your sample size)\n",
    "Balanced dataset\n",
    "\n",
    "SMOTE to take care of imbalanced dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions : Expanding contractions\n",
    "- Use Regular expressions\n",
    "    - don't --> do not\n",
    "    - isn't --> is not\n",
    "    - aren't --> are not\n",
    "    - we're --> we are\n",
    "    - they're --> they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Everything we‚Äôre doing now is great. However, we don't want to relax now. And this isn't the time to relax at all.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything we are doing now is great. However, we don't want to relax now. And this isn't the time to relax at all.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'we[\\‚Äô\\']re'\n",
    "replacement = 'we are'\n",
    "expanded_sentence = re.sub(pattern,replacement,test_sentence)\n",
    "print(expanded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalize the contraction expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything w are doing now is great. However, we don't want to relax now. And this isn't the time to relax at all.\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\w[\\‚Äô\\']\\w'\n",
    "replacement = ' are'\n",
    "expanded_sentence = re.sub(pattern,replacement,test_sentence)\n",
    "print(expanded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write one regular expression for don't types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything we‚Äôre doing now is great. However, we do not want to relax now. And this is not the time to relax at all.\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\w[\\‚Äô\\']t'\n",
    "replacement = ' not'\n",
    "expanded_sentence = re.sub(pattern,replacement,test_sentence)\n",
    "print(expanded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging : Finding the Parts-Of-Speech of words\n",
    "- Use spacy\n",
    "- pip install spacy\n",
    "- python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (2.3.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (7.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.50.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Spacy Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.0 MB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.50.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: setuptools in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.1.post20201107)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=fe80052a42655b2363936cfd0d2bf4fa890c2501170782167dd2ae93767c7f6c\n",
      "  Stored in directory: /private/var/folders/fk/czyq5vyj5hq3k3g6mjkj2xkh0000gn/T/pip-ephem-wheel-cache-123l1qar/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. ‚ÄúI can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn‚Äôt \"\n",
    "        \"worth talking to,‚Äù said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. ‚ÄúI can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn‚Äôt worth talking to,‚Äù said Thrun, in an interview with Recode earlier this week."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find noun phrases in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun\n",
      "self-driving cars\n",
      "Google\n",
      "few people\n",
      "the company\n",
      "him\n",
      "I\n",
      "you\n",
      "very senior CEOs\n",
      "major American car companies\n",
      "my hand\n",
      "I\n",
      "Thrun\n",
      "an interview\n",
      "Recode\n"
     ]
    }
   ],
   "source": [
    "for noun in doc.noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "working\n",
      "driving\n",
      "took\n",
      "can\n",
      "tell\n",
      "would\n",
      "shake\n",
      "turn\n",
      "talking\n",
      "said\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    if word.pos_ == \"VERB\":\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few\n",
      "senior\n",
      "major\n",
      "American\n",
      "worth\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    if word.pos_ == \"ADJ\":\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode LOC\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun Thrun nsubj\n",
      "self-driving cars cars pobj\n",
      "Google Google pobj\n",
      "few people people nsubj\n",
      "the company company pobj\n",
      "him him dobj\n",
      "I I nsubj\n",
      "you you dative\n",
      "very senior CEOs CEOs nsubj\n",
      "major American car companies companies pobj\n",
      "my hand hand dobj\n",
      "I I nsubj\n",
      "Thrun Thrun nsubj\n",
      "an interview interview pobj\n",
      "Recode Recode pobj\n"
     ]
    }
   ],
   "source": [
    "# Sentence Chunks - phrases\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am looking at you. You better watch out.\"\n",
    "text = \"Thrun, who heads GoogleX, was also teaching in MOOC.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"36bc03dbb7b64fea9ce09f24fb5f879d-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Thrun,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">who</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">heads</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">GoogleX,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">also</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">teaching</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">MOOC.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-2\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,266.5 L403.0,254.5 387.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-36bc03dbb7b64fea9ce09f24fb5f879d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun 5 20 PERSON\n",
      "Google 61 67 ORG\n",
      "2007 71 75 DATE\n",
      "American 173 181 NORP\n",
      "Thrun 271 276 PERSON\n",
      "Recode 299 305 LOC\n",
      "earlier this week 306 323 DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working on self-driving cars at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously. ‚ÄúI can tell you very senior CEOs of major \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " car companies would shake my hand and turn away because I wasn‚Äôt worth talking to,‚Äù said \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", in an interview with \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Recode\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    earlier this week\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreference Resolution\n",
    "- Resolving what sets of pronouns or nouns in a set of sentences link to the same person or thing\n",
    "\n",
    "- Use neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/subashgandyer/Library/Caches/pip/wheels/2c/0b/d4/be6e85f480e2a238aaa98182f52eb6fc410c25b705ffb3b1e9/neuralcoref-4.0-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: spacy>=2.1.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from neuralcoref) (2.3.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from neuralcoref) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from neuralcoref) (1.19.2)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.17.3-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 130 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (0.7.4)\n",
      "Requirement already satisfied: setuptools in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (50.3.1.post20201107)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (7.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (4.50.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
      "Collecting botocore<1.21.0,>=1.20.3\n",
      "  Downloading botocore-1.20.3-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.2 MB 5.5 MB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 2.1 MB 2.8 MB/s eta 0:00:02     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 4.1 MB 2.8 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.4-py2.py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.3->boto3->neuralcoref) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/subashgandyer/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.3->boto3->neuralcoref) (1.15.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, neuralcoref\n",
      "Successfully installed boto3-1.17.3 botocore-1.20.3 jmespath-0.10.0 neuralcoref-4.0 s3transfer-0.3.4\n"
     ]
    }
   ],
   "source": [
    "! pip install neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fad81969b20>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "elon_text = \"\"\"Musk was born to a Canadian mother and South African father  and raised in Pretoria, South Africa.  He briefly attended the University of Pretoria before moving to Canada  when he was 17 to attend Queen's University. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "elon_text = \"\"\"Musk was born to a Canadian mother and South African father \n",
    "and raised in Pretoria, South Africa. \n",
    "He briefly attended the University of Pretoria before moving to Canada \n",
    "when he was 17 to attend Queen's University. \n",
    "He transferred to the University of Pennsylvania two years later, \n",
    "where he received dual bachelor's degrees in economics and physics. \n",
    "He moved to California in 1995 to begin a Ph.D. in \n",
    "applied physics and material sciences at Stanford University \n",
    "but dropped out after two days to pursue a business career, \n",
    "co-founding web software company Zip2 with his brother Kimbal. \n",
    "The start-up was acquired by Compaq for $307 million in 1999. Musk co-founded online bank X.com that same year, which merged with Confinity in 2000 to form the company PayPal and was subsequently bought by eBay in 2002 for $1.5 billion.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(elon_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering\n",
    "- Set of methods that will accomplish the task of extracting features for model building\n",
    "- Converting pieces of text into some numeric vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two major categories\n",
    "- Classical NLP / ML Pipeline\n",
    "- Deep Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical NLP\n",
    "![Classical NLP](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Classical_FE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Converts the raw data into a format that can be consumed by a machine\n",
    "- Convert text into **numerical vectors**\n",
    "- In Classical NLP, feature extraction is **handcrafted or hand-engineered** by domain experts who are solving the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning NLP\n",
    "![DL NLP](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_DL_FE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Extraction happens automatically as part of the model training process\n",
    "- **Neurons** extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Modeling\n",
    "- Process of building a model with the data\n",
    "    - Simple Heuristics\n",
    "        - Regular Expressions\n",
    "        - Rule-baased approaches\n",
    "    - Probabilistic models\n",
    "        - HMM\n",
    "        - CRF\n",
    "    - Neural Network models\n",
    "        - RNN\n",
    "        - LSTM\n",
    "    - Ensemble models\n",
    "    - Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modeling Principles](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Modeling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation : Measuring how good the model is\n",
    "- Use the right metric\n",
    "    - 10000 samples of no fraud\n",
    "    - 100 samples of fraud\n",
    "    - 10100 samples -- 99% accuracy \n",
    "- Follow the right evaluation process\n",
    "\n",
    "Types of Evaluation\n",
    "- Intrinsic evaluation\n",
    "- Extrinsic evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intrinsic Evaluation\n",
    "\n",
    "![Intrinsic Evaluation_1](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Intrinsic_Evaluation1.png)\n",
    "\n",
    "\n",
    "![Intrinsic Evaluation_2](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Intrinsic_Evaluation2.png)\n",
    "\n",
    "![Intrinsic Evaluation_3](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Intrinsic_Evaluation3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrinsic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Involves the business metrics outside the AI/ML team\n",
    "\n",
    "Takeway\n",
    "- First, check if you achieve good intrinsic evaluation metric\n",
    "- Then, go for extrinsic evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Deployment : Serving the built models to the customers\n",
    "Major cloud providers\n",
    "- Google Cloud Platform (GCP) \\$300 credit\n",
    "- Amazon Web Services (AWS) \\$300 credit\n",
    "- Microsoft Azure \\$300 credit\n",
    "- Heroku (free)\n",
    "- Python nowhere (free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Monitoring & Model Updation\n",
    "- Monitoring of models' efficiency must be done on a constant real-time basis\n",
    "- Performance dashboards to be included in the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Monitoring](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/NLP_Monitoring.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
