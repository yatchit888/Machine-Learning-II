{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n",
    "## This notebook outlines the concepts behind the face detection in images and videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object detection** is a vital task in image processing and computer vision. It is concerned with detecting instances of an object such as human faces, buildings, trees, cars, etc\n",
    "\n",
    "The primary aim of **face detection algorithms** is to determine whether there is any face in an image or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakthrough in Face Detection\n",
    "- Viola and Jones Seminal Research\n",
    "- **\"Rapid Object Detection using a Boosted Cascade of Simple Features\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How this Algorithm works?\n",
    "Given a grayscale image, the algorithm looks at many **smaller subregions** and tries to find a face by looking for specific **features** in each subregion\n",
    "- Checks many different positions and scales because an image can contain many faces of various sizes\n",
    "- **Haar-like features** to detect faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main steps\n",
    "- Selecting Haar-like features\n",
    "- Creating an integral image\n",
    "- Running AdaBoost training\n",
    "- Creating classifier cascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haar-like feaatures\n",
    "**Alfred Haar** gave the concepts of Haar wavelets, which are a sequence of **rescaled “square-shaped” functions** which together form a wavelet family or basis\n",
    "\n",
    "#### Haar-like features \n",
    "- features used in object recognition\n",
    "- All human faces share some universal properties of the human face\n",
    "    - eyes region is darker than its neighbour pixels\n",
    "    - nose region is brighter than the eye region\n",
    "\n",
    "#### How to find out which region is ligther or darker?\n",
    "- Sum up the pixel values of both regions\n",
    "- Compare them\n",
    "\n",
    "- The sum of pixel values in the darker region will be smaller than the sum of pixels in the lighter region\n",
    "- If one side is lighter than the other, it may be an **edge of an eyebrow** or sometimes the middle portion may be shinier than the surrounding boxes, which can be interpreted as a nose \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Haar-features](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/haar-features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 types of Haar-like features\n",
    "- Edge features (Edges)\n",
    "- Line-features (Lines)\n",
    "- Four-sided features (Diagonal)\n",
    "\n",
    "\n",
    "The **value** of the feature is calculated as a **single number**: \n",
    "- the sum of pixel values in the black area **minus** the sum of pixel values in the white area\n",
    "- The value is **zero** for a **plain surface** in which all the pixels have the same value --> provide no useful information\n",
    "\n",
    "- Since our faces are of complex shapes with darker and brighter spots, a Haar-like feature gives you a **large number** when the areas in the black and white rectangles are very different (vital information)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integral Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate a value for each feature, we need to perform computations on all the pixels inside that particular feature\n",
    "\n",
    "#### Issue:\n",
    "- These calculations can be very **intensive** since the number of pixels would be much greater when we are dealing with a large feature\n",
    "\n",
    "Solution: **Integral Image**\n",
    "\n",
    "\n",
    "An integral image (also known as a **summed-area table**) is the name of both a data structure and an algorithm used to obtain this data structure\n",
    "- It is used as a quick and efficient way to calculate the **sum of pixel values in an image** or rectangular part of an image.\n",
    "\n",
    "In an integral image, the value of each point is the **sum of all pixels above and to the left, including the target pixel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Integral Image](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/integral_image_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these integral images, we save a lot of time calculating the summation of all the pixels in a rectangle as we only have to perform calculations on four edges of the rectangle. See the example below to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Integral image computation](https://raw.githubusercontent.com/subashgandyer/datasets/main/images/integral_image_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add the pixels in the blue box, we get 8 as the sum of all pixels and here we had six elements involved in your calculation. Now to calculate the sum of these same pixels using the integral image, you just need to find the corners of the rectangle and then add the vertices which are green and subtract the vertices in the red boxes.\n",
    "\n",
    "We get the same answer and only four numbers are involved in calculations. No matter how many pixels are in the rectangle box, we will just need to compute on these 4 vertices.\n",
    "\n",
    "Now to calculate the value of any haar-like feature, you have a simple way to calculate the difference between the sums of pixel values of two rectangles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features that are present in the 24×24 detector window is nearly 160,000, but only a few of these features are important to identify a face. So we use the AdaBoost algorithm to identify the best features in the 160,000 features. \n",
    "\n",
    "In the Viola-Jones algorithm, each Haar-like feature represents a weak learner. To decide the type and size of a feature that goes into the final classifier, AdaBoost checks the performance of all classifiers that you supply to it.\n",
    "\n",
    "To calculate the performance of a classifier, you evaluate it on all subregions of all the images used for training. Some subregions will produce a strong response in the classifier. Those will be classified as positives, meaning the classifier thinks it contains a human face. Subregions that don’t provide a strong response don’t contain a human face, in the classifiers opinion. They will be classified as negatives.\n",
    "\n",
    "The classifiers that performed well are given higher importance or weight. The final result is a strong classifier, also called a boosted classifier, that contains the best performing weak classifiers.\n",
    "\n",
    "So when we’re training the AdaBoost to identify important features, we’re feeding it information in the form of training data and subsequently training it to learn from the information to predict. So ultimately, the algorithm is setting a minimum threshold to determine whether something can be classified as a useful feature or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cascading Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the AdaBoost will finally select the best features around say 2500, but it is still a time-consuming process to calculate these features for each region. We have a 24×24 window which we slide over the input image, and we need to find if any of those regions contain the face. The job of the cascade is to quickly discard non-faces, and avoid wasting precious time and computations. Thus, achieving the speed necessary for real-time face detection.\n",
    "\n",
    "We set up a cascaded system in which we divide the process of identifying a face into multiple stages. In the first stage, we have a classifier which is made up of our best features, in other words, in the first stage, the subregion passes through the best features such as the feature which identifies the nose bridge or the one that identifies the eyes. In the next stages, we have all the remaining features.\n",
    "\n",
    "When an image subregion enters the cascade, it is evaluated by the first stage. If that stage evaluates the subregion as positive, meaning that it thinks it’s a face, the output of the stage is maybe.\n",
    "\n",
    "When a subregion gets a maybe, it is sent to the next stage of the cascade and the process continues as such till we reach the last stage.\n",
    "\n",
    "If all classifiers approve the image, it is finally classified as a human face and is presented to the user as a detection.\n",
    "\n",
    "Now how does it help us to increase our speed? Basically, If the first stage gives a negative evaluation, then the image is immediately discarded as not containing a human face. If it passes the first stage but fails the second stage, it is discarded as well. Basically, the image can get discarded at any stage of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK: Detect faces in video feed from live webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Import the libraries\n",
    "- Collect pre-trained HAAR Cascade filters (xml)\n",
    "- Create a Cascade Classifier with those filters\n",
    "- Capture video feed from webcam\n",
    "- Display the video frame\n",
    "- Convert the RGB frame into GrayScale\n",
    "- Detect faces with the Cascade Classifier\n",
    "- Find Face (Region of Interest)\n",
    "- Draw rectangle around face\n",
    "- Detect eyes with the Eyes Cascade classifier\n",
    "- Draw circles for detected eyes\n",
    "- Show the detected face region with detected eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect HAAR Cascade filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascPathface = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "cascPatheyes = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_eye_tree_eyeglasses.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cascade Classifiers\n",
    "- Face\n",
    "- Eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(cascPathface)\n",
    "eyeCascade = cv2.CascadeClassifier(cascPatheyes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Video feed from webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read frame from videocam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yatch\\Desktop\\Machine Learning II\\Module 13_Videos\\17_Face_Detection_Solution.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m video_capture\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mFace Video\u001b[39m\u001b[39m'\u001b[39m, frame)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39m27\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    cv2.imshow('Face Video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the frame to GrayScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    cv2.imshow('Face Video', frame)\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('Gray video', gray)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[281, 258, 208, 208]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw rectangles on the detected faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h),(255, 255, 0), 2)\n",
    "    faceROI = frame[y:y+h,x:x+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect the eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyes = eyeCascade.detectMultiScale(faceROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x2, y2, w2, h2) in eyes:\n",
    "    eye_center = (x + x2 + w2 // 2, y + y2 + h2 // 2)\n",
    "    radius = int(round((w2 + h2) * 0.25))\n",
    "    frame = cv2.circle(frame, eye_center, radius, (0, 0, 255), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yatch\\Desktop\\Machine Learning II\\Module 13_Videos\\17_Face_Detection_Solution.ipynb Cell 42\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Capture frame-by-frame\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m video_capture\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     faces \u001b[39m=\u001b[39m faceCascade\u001b[39m.\u001b[39mdetectMultiScale(gray,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                          scaleFactor\u001b[39m=\u001b[39m\u001b[39m1.1\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                          minNeighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                          minSize\u001b[39m=\u001b[39m(\u001b[39m60\u001b[39m, \u001b[39m60\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                          flags\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mCASCADE_SCALE_IMAGE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h),(255, 255, 0), 2)\n",
    "        faceROI = frame[y:y+h,x:x+w]\n",
    "        eyes = eyeCascade.detectMultiScale(faceROI)\n",
    "        for (x2, y2, w2, h2) in eyes:\n",
    "            eye_center = (x + x2 + w2 // 2, y + y2 + h2 // 2)\n",
    "            radius = int(round((w2 + h2) * 0.25))\n",
    "            frame = cv2.circle(frame, eye_center, radius, (0, 0, 255), 4)\n",
    "\n",
    "        # Display the resulting frame\n",
    "    cv2.imshow('Face Video', frame)\n",
    "    cv2.imshow(\"Face ROI\", faceROI)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yatch\\Desktop\\Machine Learning II\\Module 13_Videos\\17_Face_Detection_Solution.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ret, frame \u001b[39m=\u001b[39m video_capture\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m faces \u001b[39m=\u001b[39m faceCascade\u001b[39m.\u001b[39;49mdetectMultiScale(gray,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                                      scaleFactor\u001b[39m=\u001b[39;49m\u001b[39m1.1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                      minNeighbors\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                      minSize\u001b[39m=\u001b[39;49m(\u001b[39m60\u001b[39;49m, \u001b[39m60\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                      flags\u001b[39m=\u001b[39;49mcv2\u001b[39m.\u001b[39;49mCASCADE_SCALE_IMAGE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m (x,y,w,h) \u001b[39min\u001b[39;00m faces:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yatch/Desktop/Machine%20Learning%20II/Module%2013_Videos/17_Face_Detection_Solution.ipynb#X60sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     cv2\u001b[39m.\u001b[39mrectangle(frame, (x, y), (x \u001b[39m+\u001b[39m w, y \u001b[39m+\u001b[39m h),(\u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "cascPathface = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "cascPatheyes = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_eye_tree_eyeglasses.xml\"\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cascPathface)\n",
    "eyeCascade = cv2.CascadeClassifier(cascPatheyes)\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h),(255, 255, 0), 2)\n",
    "        faceROI = frame[y:y+h,x:x+w]\n",
    "        eyes = eyeCascade.detectMultiScale(faceROI)\n",
    "        for (x2, y2, w2, h2) in eyes:\n",
    "            eye_center = (x + x2 + w2 // 2, y + y2 + h2 // 2)\n",
    "            radius = int(round((w2 + h2) * 0.25))\n",
    "            frame = cv2.circle(frame, eye_center, radius, (0, 0, 255), 4)\n",
    "\n",
    "        # Display the resulting frame\n",
    "    cv2.imshow('Face Video', frame)\n",
    "    cv2.imshow(\"Face ROI\", faceROI)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
