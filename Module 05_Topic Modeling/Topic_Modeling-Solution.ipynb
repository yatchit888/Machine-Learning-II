{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## This notebook outlines the concepts involved in Topic Modeling\n",
    "\n",
    "\n",
    "Topic modeling is a statistical model to **discover** the abstract \"topics\" that occur in a collection of documents\n",
    "\n",
    "It is commonly used in text document. But nowadays, in social media analysis, topic modeling is an emerging research area.\n",
    "\n",
    "One of the most popular algorithms used is **Latent Dirichlet Allocation** which was proposed by\n",
    "David Blei et al in 2003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "https://raw.githubusercontent.com/subashgandyer/datasets/main/kaggledatasets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Install the necessary library\n",
    "- Import the necessary libraries\n",
    "- Download the dataset\n",
    "- Load the dataset\n",
    "- Pre-process the dataset\n",
    "    - Tokenize\n",
    "    - Stop words removal\n",
    "    - Non-alphabetic words removal\n",
    "    - Lowercase\n",
    "- Create a dictionary for the document\n",
    "- Filter low frequency words\n",
    "- Create an Index to word dictionary\n",
    "- Train the Topic Model\n",
    "- Predict on the dataset\n",
    "- Visualize the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `'stopwords''\r\n",
      "/bin/bash: -c: line 0: ` nltk.download('stopwords')'\r\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "! nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-06 20:18:04--  https://raw.githubusercontent.com/subashgandyer/datasets/main/kaggledatasets.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3881130 (3.7M) [text/plain]\n",
      "Saving to: ‘kaggledatasets.csv.2’\n",
      "\n",
      "kaggledatasets.csv. 100%[===================>]   3.70M  4.30MB/s    in 0.9s    \n",
      "\n",
      "2021-03-06 20:18:05 (4.30 MB/s) - ‘kaggledatasets.csv.2’ saved [3881130/3881130]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/subashgandyer/datasets/main/kaggledatasets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Versions</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>License</th>\n",
       "      <th>Views</th>\n",
       "      <th>Download</th>\n",
       "      <th>Kernels</th>\n",
       "      <th>Topics</th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit Card Fraud Detection</td>\n",
       "      <td>Anonymized credit card transactions labeled as...</td>\n",
       "      <td>Machine Learning Group - ULB</td>\n",
       "      <td>1241</td>\n",
       "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
       "      <td>crime\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>442,136 views</td>\n",
       "      <td>53,128 downloads</td>\n",
       "      <td>1,782 kernels</td>\n",
       "      <td>26 topics</td>\n",
       "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
       "      <td>The datasets contains transactions made by cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>European Soccer Database</td>\n",
       "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
       "      <td>Hugo Mathien</td>\n",
       "      <td>1046</td>\n",
       "      <td>Version 10,2016-10-24|Version 9,2016-10-24|Ver...</td>\n",
       "      <td>association football\\neurope</td>\n",
       "      <td>SQLite</td>\n",
       "      <td>299 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>396,214 views</td>\n",
       "      <td>46,367 downloads</td>\n",
       "      <td>1,459 kernels</td>\n",
       "      <td>75 topics</td>\n",
       "      <td>https://www.kaggle.com/hugomathien/soccer</td>\n",
       "      <td>The ultimate Soccer database for data analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TMDB 5000 Movie Dataset</td>\n",
       "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
       "      <td>The Movie Database (TMDb)</td>\n",
       "      <td>1024</td>\n",
       "      <td>Version 2,2017-09-28</td>\n",
       "      <td>film</td>\n",
       "      <td>CSV</td>\n",
       "      <td>44 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>446,255 views</td>\n",
       "      <td>62,002 downloads</td>\n",
       "      <td>1,394 kernels</td>\n",
       "      <td>46 topics</td>\n",
       "      <td>https://www.kaggle.com/tmdb/tmdb-movie-metadata</td>\n",
       "      <td>Background\\nWhat can we say about the success ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global Terrorism Database</td>\n",
       "      <td>More than 170,000 terrorist attacks worldwide,...</td>\n",
       "      <td>START Consortium</td>\n",
       "      <td>789</td>\n",
       "      <td>Version 2,2017-07-19|Version 1,2016-12-08</td>\n",
       "      <td>crime\\nterrorism\\ninternational relations</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>187,877 views</td>\n",
       "      <td>26,309 downloads</td>\n",
       "      <td>608 kernels</td>\n",
       "      <td>11 topics</td>\n",
       "      <td>https://www.kaggle.com/START-UMD/gtd</td>\n",
       "      <td>Context\\nInformation on more than 170,000 Terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bitcoin Historical Data</td>\n",
       "      <td>Bitcoin data at 1-min intervals from select ex...</td>\n",
       "      <td>Zielak</td>\n",
       "      <td>618</td>\n",
       "      <td>Version 11,2018-01-11|Version 10,2017-11-17|Ve...</td>\n",
       "      <td>history\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>119 MB</td>\n",
       "      <td>CC4</td>\n",
       "      <td>146,734 views</td>\n",
       "      <td>16,868 downloads</td>\n",
       "      <td>68 kernels</td>\n",
       "      <td>13 topics</td>\n",
       "      <td>https://www.kaggle.com/mczielinski/bitcoin-his...</td>\n",
       "      <td>Context\\nBitcoin is the longest running and mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title  \\\n",
       "0  Credit Card Fraud Detection   \n",
       "1     European Soccer Database   \n",
       "2      TMDB 5000 Movie Dataset   \n",
       "3    Global Terrorism Database   \n",
       "4      Bitcoin Historical Data   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0  Anonymized credit card transactions labeled as...   \n",
       "1  25k+ matches, players & teams attributes for E...   \n",
       "2                Metadata on ~5,000 movies from TMDb   \n",
       "3  More than 170,000 terrorist attacks worldwide,...   \n",
       "4  Bitcoin data at 1-min intervals from select ex...   \n",
       "\n",
       "                          Owner  Votes  \\\n",
       "0  Machine Learning Group - ULB   1241   \n",
       "1                  Hugo Mathien   1046   \n",
       "2     The Movie Database (TMDb)   1024   \n",
       "3              START Consortium    789   \n",
       "4                        Zielak    618   \n",
       "\n",
       "                                            Versions  \\\n",
       "0          Version 2,2016-11-05|Version 1,2016-11-03   \n",
       "1  Version 10,2016-10-24|Version 9,2016-10-24|Ver...   \n",
       "2                               Version 2,2017-09-28   \n",
       "3          Version 2,2017-07-19|Version 1,2016-12-08   \n",
       "4  Version 11,2018-01-11|Version 10,2017-11-17|Ve...   \n",
       "\n",
       "                                        Tags Data Type    Size License  \\\n",
       "0                             crime\\nfinance       CSV  144 MB    ODbL   \n",
       "1               association football\\neurope    SQLite  299 MB    ODbL   \n",
       "2                                       film       CSV   44 MB   Other   \n",
       "3  crime\\nterrorism\\ninternational relations       CSV  144 MB   Other   \n",
       "4                           history\\nfinance       CSV  119 MB     CC4   \n",
       "\n",
       "           Views          Download        Kernels     Topics  \\\n",
       "0  442,136 views  53,128 downloads  1,782 kernels  26 topics   \n",
       "1  396,214 views  46,367 downloads  1,459 kernels  75 topics   \n",
       "2  446,255 views  62,002 downloads  1,394 kernels  46 topics   \n",
       "3  187,877 views  26,309 downloads    608 kernels  11 topics   \n",
       "4  146,734 views  16,868 downloads     68 kernels  13 topics   \n",
       "\n",
       "                                                 URL  \\\n",
       "0     https://www.kaggle.com/mlg-ulb/creditcardfraud   \n",
       "1          https://www.kaggle.com/hugomathien/soccer   \n",
       "2    https://www.kaggle.com/tmdb/tmdb-movie-metadata   \n",
       "3               https://www.kaggle.com/START-UMD/gtd   \n",
       "4  https://www.kaggle.com/mczielinski/bitcoin-his...   \n",
       "\n",
       "                                         Description  \n",
       "0  The datasets contains transactions made by cre...  \n",
       "1  The ultimate Soccer database for data analysis...  \n",
       "2  Background\\nWhat can we say about the success ...  \n",
       "3  Context\\nInformation on more than 170,000 Terr...  \n",
       "4  Context\\nBitcoin is the longest running and mo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"kaggledatasets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context\n",
      "the [sentiment polarity dataset version 2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/ ) is created by bo pang and lillian lee. this dataset is redistributed with nltk with permission from the authors.\n",
      "this corpus is also used in the document classification section of chapter 6.1.3 of the nltk book.\n",
      "content\n",
      "this dataset contains 1000 positive and 1000 negative processed reviews.\n",
      "citation\n",
      "bo pang and lillian lee. 2004. a sentimental education: sentiment analysis \n",
      "using subjectivity summarization based on minimum cuts. in acl.\n",
      "bibtex:\n",
      "@inproceedings{pang+lee:04a,\n",
      "  author =       {bo pang and lillian lee},\n",
      "  title =        {a sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts},\n",
      "  booktitle =    \"proceedings of the acl\",\n",
      "  year =         2004\n",
      "}\n",
      "context\n",
      "the corpus consists of one million words of american english texts printed in 1961.\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"brown\" name=\"brown corpus\"\n",
      "         author=\"w. n. francis and h. kucera\"\n",
      "         license=\"may be used for non-commercial purposes.\"\n",
      "         webpage=\"http://www.hit.uib.no/icame/brown/bcm.html\"\n",
      "         unzip=\"1\"\n",
      "         />\n",
      "\n",
      "<package id=\"brown_tei\" name=\"brown corpus (tei xml version)\"\n",
      "     author=\"w. n. francis and h. kucera\"\n",
      "     license=\"may be used for non-commercial purposes.\"\n",
      "     webpage=\"http://www.hit.uib.no/icame/brown/bcm.html\"\n",
      "     contact=\"lou burnard -- lou.burnard@oucs.ox.ac.uk\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "description\n",
      "this dataset has two parts, one is created using 100 million sentences from last 18 months of us patent data downloaded from uspto site. second is from 83 million sentences extracted from ncbi pmc open access articles.\n",
      "preparation of data\n",
      "text from original xml extracted using python lxml module. extracted text split into sentences using simple regex. resulting 83 million sentences from pmc articles, 120 million sentences from patents.\n",
      "clustering of sentences\n",
      "sentences with less than 5 words and more than 70 words were removed. stop words and unwanted characters were removed. extracted sentences were fed to the clustering algorithm to generate hierarchical clusters.\n",
      "sentences are clustered as if each sentence is compared with all other sentences, ie all against all match. however, all against all match for 100 million sentences computationally may not be possible, even extracting features would be costly, graph mining algorithms used to cluster the data.\n",
      "files description\n",
      "ncbi pmc\n",
      "file : ncbi_data.json ; mapping : {count: cluster_id, val:[list of line ids] } file : ncbi_lines_map.json ; mapping : {pmc: line id, line: } split pmc with _, first part is the doc id file : ncbi_doc_titles_map.json ; mapping : {pmc: line id, article-title: , journal-title: , article-id:}\n",
      "us patent data\n",
      "file : uspto_data.json ; mapping : {count: cluster_id, val:[list of line ids] } file : uspto_lines_map.json ; mapping : {num: line id, line : } split num with _, first part is the doc id file : uspto_doc_titles_map.json ; mapping : {num: patent id extracted from xml xpath //publication-reference//document-id//doc-number, title: invention title from patent xml }\n",
      "where this data can be used?\n",
      "this can be used in search ranking of similar documents, conceptual search, plagiarism detection etc. having similar sentences can improve the accuracy of the prediction models. machine translation and other areas where alignment is needed, running models on already aligned data can boost the accuracy while reducing the time.\n",
      "details about the uploaded data\n",
      "original data size uncompressed is 32 gb for ncbi pmc data, 30 gb for uspto data. clustered data size is slightly bigger than original, so it cant be uploaded in kaggle.\n",
      "only clusters with less than 5/8 lines were uploaded, consisting of 400k and 358 clusters for ncbi and uspto respectively. each has more than 800k lines in 250k and 135k documents.\n",
      "examples from data\n",
      "ncbi\n",
      "pmc : 3045422_8\n",
      " article-title : association of a-adducin and g-protein b3 genetic polymorphisms with hypertension: a meta-analysis of chinese populations\n",
      " journal-title : plos one\n",
      "line : as the genomic sequences of a-adducin and gnb3 genes are highly polymorphic, it is of added interest to identify which polymorphism(s) in these genes might have functional potentials of affecting their bioavailability\n",
      "\n",
      " pmc : 3142626_5  \n",
      " article-title :evaluation of transforming growth factor beta-1 gene 869t/c polymorphism with hypertension: a meta-analysis\n",
      "  journal-title : international journal of hypertension\n",
      "  line : since the genomic sequence of tgfb1 gene is highly polymorphic, it is of added interest to confirm which tgfb1 polymorphism(s) might have functional potentials to influence the final bioavailability of tgf- b 1, thus the development of hypertension\n",
      "\n",
      "pmc : 3166328_6\n",
      "article-title : an updated meta-analysis of endothelial nitric oxide synthase gene: three well-characterized polymorphisms with hypertension\n",
      "journal-title : plos one\n",
      "line : since the genomic sequence of enos is highly polymorphic, it is of added interest to confirm which polymorphism(s) at enos might have functional potentials to affect the final bioavailability of enos, and thus the development of hypertension\n",
      "pmc : 362850_30\n",
      "article-title : de-orphaning the structural proteome through reciprocal comparison of evolutionarily important structural features\n",
      "journal-title : plos one\n",
      "line : since a small root mean squared deviation (rmsd) alone is not sufficient to guarantee the functional relevance of a match , , a support vector machine (svm) trained on enzymes () considers in addition to rmsd whether the matches also fall on evolutionarily important regions of t i \n",
      "\n",
      "pmc : 4651773_184\n",
      "article-title : crystal structure of group ii intron domain 1 reveals a template for rna assembly\n",
      "journal-title : nature chemical biology\n",
      "line : all of the root mean square deviation (rmsd) values were calculated by pymol without allowing removal of non-fitting residues to minimize rmsd. for calculating the simulated annealing omit map, the region of interest was first deleted from the model, and then this partial model was subject to simulated annealing refinement in phenix.refine\n",
      "us patent data\n",
      "num : 09226555_77\n",
      "title: cane structure\n",
      "line : as such, when the negative terminal of a battery is located at the holder with the l-shaped blocking wall 2112 , the battery will be blocked from being electrically connected with the associated conductive sheet 213 therein to prevent an incorrect electrical connection that may cause a damage or failure to the circuit board 22 \n",
      "\n",
      "num : 09520536_507\n",
      "title : light emitting diode chip having electrode pad\n",
      "line : therefore, the second electrode extensions 39 a and the transparent conductive layer 33 are not electrically connected to each other in the positions in which the holes h are formed, thus, an electrical flow is blocked in the corresponding positions in which the holes h are formed, and the second electrode extensions 39 a are located on the current blocking layer 31 b\n",
      "num : 09233144_346\n",
      "title : tyrosine kinase receptor tyro3 as a therapeutic target in the treatment of cancer\n",
      "line : in order to investigate the role of tyro3 in cell growth and tumorigenic properties, tyro3 expression was blocked using rna interference technology or tyro3 activity was inhibited using a blocking antibody directed against the extracellular domain of tyro3 or a soluble receptor consisting of the recombinant extracellular domain of tyro3 produced in bacteria\n",
      "\n",
      "num : 09283245_194\n",
      "title : composition containing pias3 as an active ingredient for preventing or treating cancer or immune disease\n",
      "line : in order to measure the amount of produced il-17 cytokine, the supernatant of the cell culture medium was collected and level of il-17 expression was investigated using human il-17 and sandwich elisa. after reaction on a 96 well plate with 2 mg/ml of monoclonal anti-il-17 at 4deg c., overnight, non-specific binding was blocked with blocking solution (1% bsa/pbst)\n",
      "\n",
      "num : 09546210_330\n",
      "title : cripto antagonism of activin and tgf-b signaling\n",
      "line : in contrast, the cripto degf mutant blocked roughly half of the luciferase activity induced by activin-b ( fig. 10 ), indicating an independent role for the cfc domain in blocking activin-b signaling\n",
      "context\n",
      "this dataset was built as a supplementary to \"[european soccer database][1]\". it includes data dictionary, extraction of detailed match information previously contains in xml columns.\n",
      "content\n",
      "positionreference.csv: a reference of position x, y and map them to actual position in a play court.\n",
      "datadictionary.xlsx: data dictionary for all xml columns in \"match\" data table.\n",
      "card_detail.csv: detailed xml information extracted form \"card\" column in \"match\" data table.\n",
      "corner_detail.csv: detailed xml information extracted form \"corner\" column in \"match\" data table.\n",
      "cross_detail.csv: detailed xml information extracted form \"cross\" column in \"match\" data table.\n",
      "foulcommit_detail.csv: detailed xml information extracted form \"foulcommit\" column in \"match\" data table.\n",
      "goal_detail.csv: detailed xml information extracted form \"goal\" column in \"match\" data table.\n",
      "possession_detail.csv: detailed xml information extracted form \"possession\" column in \"match\" data table.\n",
      "shotoff_detail.csv: detailed xml information extracted form \"shotoffl\" column in \"match\" data table.\n",
      "shoton_detail.csv: detailed xml information extracted form \"shoton\" column in \"match\" data table.\n",
      "acknowledgements\n",
      "original data comes from [european soccer database][1] by hugo mathien. i personally thank him for all his efforts.\n",
      "inspiration\n",
      "since this is a open dataset with no specific goals / objectives, i would like to explore the following aspects by data analytics / data mining:\n",
      "team statistics including overall team ranking, team points, winning possibility, team lineup, etc. mostly descriptive analysis.\n",
      "team transferring track and study team players transferring in the market. study team's strength and weakness, construct models to suggest best fit players to the team.\n",
      "player statistics summarize player's performance (goal, assist, cross, corner, pass, block, etc). identify key factors of players by position. based on these factors, evaluate player's characteristics.\n",
      "player evolution construct model to predict player's rating of future.\n",
      "new player's template identify template and model player for young players cater to their positions and characteristics.\n",
      "market value prediction predict player's market value based on player's capacity and performance.\n",
      "the winning eleven given a season / league / other criteria, propose the best 11 players as a team based on their capacity and performance.\n",
      "context\n",
      "tailpipe emissions for bmw 3series sedan\n",
      "content\n",
      "analysis for tailpipe omissions data for bmw 3 series\n",
      "data is recorded for car travelling at every second with following variables: vehicle_speed (km/h), hc_tailpipe (g/s), co_tailpipe (g/s) and nox_tailpipe (g/s). it is expected that the value of theses emissions are close to zero, so the recommended emissions are speed = 39.9 , hc = 0.0005, co = 0.0200 𝑎𝑛𝑑 no = 0.0004\n",
      "context\n",
      "kannada basic word set for natural language processing\n",
      "content\n",
      "words which has meaning in kannada language\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "the state of the nation address of the president of south africa (abbreviated sona) is an annual event in the republic of south africa, in which the president of south africa reports on the status of the nation, normally to the resumption of a joint sitting of parliament (the national assembly and the national council of provinces).\n",
      "content\n",
      "full text of all the speeches, from 1990 through to 2018. in years that elections took place, a state of the nation address happens twice, once before and again after the election.\n",
      "title: protein localization sites\n",
      "creator and maintainer: kenta nakai institue of molecular and cellular biology osaka, university 1-3 yamada-oka, suita 565 japan nakai@imcb.osaka-u.ac.jp http://www.imcb.osaka-u.ac.jp/nakai/psort.html donor: paul horton (paulh@cs.berkeley.edu) date: september, 1996 see also: yeast database\n",
      "past usage. reference: \"a probablistic classification system for predicting the cellular localization sites of proteins\", paul horton & kenta nakai, intelligent systems in molecular biology, 109-115. st. louis, usa 1996. results: 81% for e.coli with an ad hoc structured probability model. also similar accuracy for binary decision tree and bayesian classifier methods applied by the same authors in unpublished results.\n",
      "predicted attribute: localization site of protein. ( non-numeric ).\n",
      "the references below describe a predecessor to this dataset and its development. they also give results (not cross-validated) for classification by a rule-based expert system with that version of the dataset.\n",
      "reference: \"expert sytem for predicting protein localization sites in gram-negative bacteria\", kenta nakai & minoru kanehisa,\n",
      "proteins: structure, function, and genetics 11:95-110, 1991.\n",
      "reference: \"a knowledge base for predicting protein localization sites in eukaryotic cells\", kenta nakai & minoru kanehisa, genomics 14:897-911, 1992.\n",
      "number of instances: 336 for the e.coli dataset and\n",
      "number of attributes. for e.coli dataset: 8 ( 7 predictive, 1 name )\n",
      "attribute information.\n",
      "sequence name: accession number for the swiss-prot database\n",
      "mcg: mcgeoch's method for signal sequence recognition.\n",
      "gvh: von heijne's method for signal sequence recognition.\n",
      "lip: von heijne's signal peptidase ii consensus sequence score. binary attribute.\n",
      "chg: presence of charge on n-terminus of predicted lipoproteins. binary attribute.\n",
      "aac: score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.\n",
      "alm1: score of the alom membrane spanning region prediction program.\n",
      "alm2: score of alom program after excluding putative cleavable signal regions from the sequence.\n",
      "missing attribute values: none.\n",
      "class distribution. the class is the localization site. please see nakai & kanehisa referenced above for more details.\n",
      "cp (cytoplasm) 143 im (inner membrane without signal sequence) 77\n",
      "pp (perisplasm) 52 imu (inner membrane, uncleavable signal sequence) 35 om (outer membrane) 20 oml (outer membrane lipoprotein) 5 iml (inner membrane lipoprotein) 2 ims (inner membrane, cleavable signal sequence) 2\n",
      "source: https://archive.ics.uci.edu/ml/datasets/forest+fires\n",
      "citation request: this dataset is public available for research. the details are described in [cortez and morais, 2007]. please include this citation if you plan to use this database:\n",
      "p. cortez and a. morais. a data mining approach to predict forest fires using meteorological data. in j. neves, m. f. santos and j. machado eds., new trends in artificial intelligence, proceedings of the 13th epia 2007 - portuguese conference on artificial intelligence, december, guimaraes, portugal, pp. 512-523, 2007. appia, isbn-13 978-989-95618-0-9. available at: http://www.dsi.uminho.pt/~pcortez/fires.pdf\n",
      "title: forest fires\n",
      "sources created by: paulo cortez and an�bal morais (univ. minho) @ 2007\n",
      "past usage:\n",
      "p. cortez and a. morais. a data mining approach to predict forest fires using meteorological data. in proceedings of the 13th epia 2007 - portuguese conference on artificial intelligence, december, 2007. (http://www.dsi.uminho.pt/~pcortez/fires.pdf)\n",
      "in the above reference, the output \"area\" was first transformed with a ln(x+1) function. then, several data mining methods were applied. after fitting the models, the outputs were post-processed with the inverse of the ln(x+1) transform. four different input setups were used. the experiments were conducted using a 10-fold (cross-validation) x 30 runs. two regression metrics were measured: mad and rmse. a gaussian support vector machine (svm) fed with only 4 direct weather conditions (temp, rh, wind and rain) obtained the best mad value: 12.71 +- 0.01 (mean and confidence interval within 95% using a t-student distribution). the best rmse was attained by the naive mean predictor. an analysis to the regression error curve (rec) shows that the svm model predicts more examples within a lower admitted error. in effect, the svm model predicts better small fires, which are the majority.\n",
      "relevant information:\n",
      "this is a very difficult regression task. it can be used to test regression methods. also, it could be used to test outlier detection methods, since it is not clear how many outliers are there. yet, the number of examples of fires with a large burned area is very small.\n",
      "number of instances: 517\n",
      "number of attributes: 12 + output attribute\n",
      "note: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.\n",
      "attribute information:\n",
      "for more information, read [cortez and morais, 2007].\n",
      "x - x-axis spatial coordinate within the montesinho park map: 1 to 9\n",
      "y - y-axis spatial coordinate within the montesinho park map: 2 to 9\n",
      "month - month of the year: \"jan\" to \"dec\"\n",
      "day - day of the week: \"mon\" to \"sun\"\n",
      "ffmc - ffmc index from the fwi system: 18.7 to 96.20\n",
      "dmc - dmc index from the fwi system: 1.1 to 291.3\n",
      "dc - dc index from the fwi system: 7.9 to 860.6\n",
      "isi - isi index from the fwi system: 0.0 to 56.10\n",
      "temp - temperature in celsius degrees: 2.2 to 33.30\n",
      "rh - relative humidity in %: 15.0 to 100\n",
      "wind - wind speed in km/h: 0.40 to 9.40\n",
      "rain - outside rain in mm/m2 : 0.0 to 6.4\n",
      "area - the burned area of the forest (in ha): 0.00 to 1090.84 (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).\n",
      "missing attribute values: none\n",
      "this dataset contains a collection of the private neighborhoods in argentina.\n",
      "context\n",
      "i created this dataset mainly for a challenge at school on which we had to predict house prices. houses that are located on a private neighborhood, have prices that are completely different than the price of houses that are on the same city, but outside the neighborhoods. i used this dataset to detect whether a house was inside or outside a private neighborhood.\n",
      "github project: https://github.com/harkdev/barrios_privados_argentina\n",
      "source: http://www.guiacountry.com/countries/imagenes/listado.php\n",
      "context\n",
      "these datasets contain estimated speed and estimated trip durations for all trips generated by the fastest route dataset from oscarleo. the estimated trip durations are a better approximation for the actual trip duration than the total_travel_time variable from the fastest route dataset is.\n",
      "how these features are generated in detail can be found in this kernel: https://www.kaggle.com/pepeeee/nyc-estimating-avg-speed-using-fastest-route/notebook\n",
      "content\n",
      "estimated_speed - this variable contains the estimated average speed of the respective trip. the unit of this variable is meter per second.\n",
      "estimated_trip_duration - this variable is generated by dividing the total_distance variable of the fastest route dataset by estimated_speed. the unit of the variable is seconds.\n",
      "acknowledgements\n",
      "i'm very thankful to oscarleo for sharing the great dataset about the fastest routes. without that dataset mine wouldn't have been possible. i would also like to thank saihttam for sharing the holidays package.\n",
      "inspiration\n",
      "the fastest route contains a lot of very valuable data for predicting the trip durations of taxis in new york city. just using the variables total_travel_time, total_distance and number_of_steps didn't feel like using its full potential to me.\n",
      "context\n",
      "simply list of iso_639-1_codes\n",
      "content\n",
      "needed to access through kaggle\n",
      "acknowledgements\n",
      "https://en.wikipedia.org/wiki/list_of_iso_639-1_codes\n",
      "image from goran ivos at unplash\n",
      "trying to name a child? looking for something a little different? something that will force a preschool's database to support unicode? look no farther! from aðdal to ösp to ben, this list has you covered.\n",
      "the icelandic naming committee maintains an official register of approved icelandic given names and is the governing body of introduction of new given names into the culture of iceland. in many cases parents use the database as a guide when choosing a name. if the name they have in mind is not in the register they can fill out a special form and request whether the name will be considered allowable by law. if the committee rules positively on a request the name will be added to the personal names register. if the committee denies the request, the child may not be allowed to get an icelandic passport with that name.\n",
      "the register is stored and maintained at registers iceland and is accessible through the national portal ísland.is.\n",
      "note on the column headers: drengir = boys, millinöfn = girls, stúlkur = either.\n",
      "winning numbers from the new york state lotto since 2001.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the state of new york. you can find the original dataset here.\n",
      "inspiration\n",
      "some other state lotteries have proven to be predictable and ended up being gamed. it's extremely unlikely that any real patterns exist in a large and long running lotto like new york's, but can you find any?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "data sourced from the uk government, and used in the car registration api http://www.regcheck.org.uk - correct as of august 2017\n",
      "context\n",
      "this dataset was downloaded from internet few years ago.\n",
      "content\n",
      "this dataset contains many individual word/ text files describing different places and their history/geography around the world.\n",
      "acknowledgements\n",
      "as this dataset is downloaded from internet few years ago and i don't remember the site/ particular authors of this file.but any how thanks to those people who created these files.\n",
      "inspiration\n",
      "this might be helpful for those who practise nlp/nltk, text mining. each individual file contains many paragraphs which describes about the place in a lengthy manner. with text mining one should be able to remove all stop words and describe the data in a brief way.\n",
      "context\n",
      "the file presents a listing of characters wearing powered armor / mini or giant meccha in movies, comics, animation etc.\n",
      "the purpose was to analyse our imaginaries in a specific field (i.e armors in this case) in order to see what are the macro elements, see how they evolve around time and if they are close to what is used in real life.\n",
      "content\n",
      "each armor is analyzed according to 13 characteristics (uses an ai or not, what kind or power, where is the weapon, its capacities (does is fly, gives enhanced strength etc.). being a social science professor and not a data analysts, i went on marvel wikia, dc wikia etc. to compile it. something like 80 heroes are fully presented, and a list of almost 300 been found.\n",
      "inspiration\n",
      "coming from social science i compiled that data during my free time, but i understand that it is highly limiting and that there must be a way to aggregate much more data, & faster. also, i am sure that it does not meet some of the standards for such work. being a newbie here, please tell me how to improve this & i will.\n",
      "the question after is to know if we can \"predict\" what future armors will look like : is there a trend showing that ai is used more and more ? that they all fly ? once this done, it would allow to \"delineate\" the ideal characteristics of a super hero and hence, where we could innovate if we do not want to reproduce things that already done while imagining them ?\n",
      "the last questions correlate to social trends : do some characteristics appear during a certain period ? if yes, is it correlated to some specific social context ? (new type of wars impacting how we imagine our heroes ?).\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this dataset compiles the titles, publication dates, and other data about all reports published in the official capacities of new york city government agency work are listed in the city hall library catalog. the catalog functions like a city-level equivalent of the national library of congress, and goes back very far --- at least to the 1800s.\n",
      "content\n",
      "columns are provided for the report name and report sub-header, the year the report was issued, the name of the publisher compiling the report, and some other smaller fields.\n",
      "acknowledgements\n",
      "this data was originally published in a pound (\"#\") delimited dataset on the new york city open data portal. it has been restructured as a csv and lightly cleaned up for formatting prior to being uploaded to kaggle.\n",
      "inspiration\n",
      "can you separate reporting publications by the city of new york into topics?\n",
      "who are the most common report issuers, and what causes do they represent?\n",
      "what are some common elements to report titles?\n",
      "context\n",
      "it would be fun to have an system which could generate new cocktails from just the name and if the cocktail already exists produce the right result\n",
      "content\n",
      "the data is a scraped collection of cocktails and their ingredients\n",
      "acknowledgements\n",
      "the idea was made at an ai-first meetup and the scraping idea came from the group who worked on it\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "dataset contains infromation about car accidents in 2013-2016\n",
      "main columns: date time\n",
      "borough latitude\n",
      "longitude\n",
      "location\n",
      "json file with nyc boroughs geoshapes\n",
      "context\n",
      "realtime gtfs data collected from 2017_5_29_11_30_15 till 2017_6_6_13_42_11 at saint-petersburg, russia. data requested from: http://transport.orgp.spb.ru/portal/transport/internalapi/gtfs\n",
      "content\n",
      "archive contain real-time gtfs files (https://developers.google.com/transit/gtfs-realtime/)\n",
      "acknowledgements\n",
      "thanks to http://transport.orgp.spb.ru for implementing api\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "there is no story behind this data.\n",
      "these are just supplementary datasets which i plan on using for plotting county wise data on maps.. (in particular for using with my kernel : https://www.kaggle.com/stansilas/maps-are-beautiful-unemployment-is-not/)\n",
      "as that data set didn't have the info i needed for plotting an interactive map using highcharter .\n",
      "content\n",
      "since i noticed that most demographic datasets here on kaggle, either have state code, state name, or county name + state name but not all of it i.e county name, fips code, state name + state code.\n",
      "using these two datasets one can get any combination of state county codes etc.\n",
      "states.csv has state name + code\n",
      "us counties.csv has county wise data.\n",
      "acknowledgements\n",
      "picture : https://unsplash.com/search/usa-states?photo=-ro2dfpl7we\n",
      "counties : https://www.census.gov/geo/reference/codes/cou.html\n",
      "state :\n",
      "inspiration\n",
      "not applicable.\n",
      "context\n",
      "taken from; http://www.internationalgenome.org/data\n",
      "content\n",
      "coming soon\n",
      "acknowledgements\n",
      "special thanks to international genome org.\n",
      "inspiration\n",
      "coming soon\n",
      "introduction\n",
      "if you love movies, and you love san francisco, you're bound to love this -- a listing of filming locations of movies shot in san francisco starting from 1924. you'll find the titles, locations, fun facts, names of the director, writer, actors, and studio for most of these films.\n",
      "inspiration\n",
      "combine with the popular imdb 5000 dataset on kaggle to see how movies filmed in san francisco are rated. click on \"new kernel\" and add this dataset source by clicking on \"input files\".\n",
      "start a new kernel\n",
      "context\n",
      "this dataset contains a subset of information, pertaining to the weather patterns during januaray 2016 - june 2016 in nyc. this may be important to those who are looking to add columns to their dataset.\n",
      "content\n",
      "preciptation, snowfall, temperatures, latitude and longitude, along with dates. all the important information we need to know. unforunately, this is not minute to minue information, but strictly daily information.\n",
      "acknowledgements\n",
      "i would like to thank mcrepy94 for sharing the link https://www.ncdc.noaa.gov/cdo-web/search?datasetid=ghcnd. please upvote his post if you upvote this.\n",
      "inspiration\n",
      "i want people to look at this dataset, and create more features for their uses.\n",
      "context\n",
      "this data was originally taken from titanic: machine learning from disaster .but its better refined and cleaned & some features have been self engineered typically for logistic regression . if you use this data for other models and benefit from it , i would be happy to receive your comments and improvements.\n",
      "content\n",
      "there are two files namely:- train_data.csv :- typically a data set of 792x16 . the survived column is your target variable (the output you want to predict).the parch & sibsb columns from the original data set has been replaced with a single column called family size.\n",
      "all categorical data like embarked , pclass have been re-encoded using the one hot encoding method .\n",
      "additionally, 4 more columns have been added , re-engineered from the name column to title_1 to title_4 signifying males & females depending on whether they were married or not .(mr , mrs ,master,miss). an additional analysis to see if married or in other words people with social responsibilities had more survival instincts/or not & is the trend similar for both genders.\n",
      "all missing values have been filled with a median of the column values . all real valued data columns have been normalized.\n",
      "test_data.csv :- a data of 100x16 , for testing your model , the arrangement of test_data exactly matches the train_data\n",
      "i am open to feedbacks & suggesstions\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "i found this dataset after exploring numerous datasets for my personal project.\n",
      "content\n",
      "it contains images(in pixel format) along with emotions .\n",
      "acknowledgements\n",
      "well, i found it on my own.\n",
      "inspiration\n",
      "context\n",
      "the college scorecard was created by the u.s. department of education in an attempt to better understand the efficacy of colleges in the united states. the scorecard reports information such as the cost of tuition, undergraduate enrollment size, and the rate of graduation. further details can be found in the file \"fulldatadescription.pdf\".\n",
      "content\n",
      "u.s. college statistics from 1996 to 2015, organized by year. data was pulled from the official website (https://collegescorecard.ed.gov/data/) in june of 2017. it was reportedly last updated in january 2017.\n",
      "context:\n",
      "some words, like “the” or “and” in english, are used a lot in speech and writing. for most natural language processing applications, you will want to remove these very frequent words. this is usually done using a list of “stopwords” which has been complied by hand.\n",
      "content:\n",
      "this project uses the source texts provided by the african storybook project as a corpus and provides a number of tools to extract frequency lists and lists of stopwords from this corpus for the 60+ languages covered by asp.\n",
      "included in this dataset are the following languages:\n",
      "afrikaans: stoplist and word frequency\n",
      "hausa: stoplist and word frequency\n",
      "lugbarati: word frequency only\n",
      "lugbarati (official): word frequency only\n",
      "somali: stoplist and word frequency\n",
      "sesotho: stoplist and word frequency\n",
      "kiswahili: stoplist and word frequency\n",
      "yoruba: stoplist and word frequency\n",
      "isizulu: stoplist and word frequency\n",
      "files are named using the language’s iso code. for each language, code.txt is the list of stopwords, and code_frequency_list.txt is word frequency information. a list of iso codes the the languages associated with them may be found in iso_codes.csv.\n",
      "acknowledgements:\n",
      "this project therefore attempts to fill in the gap in language coverage for african language stoplists by using the freely-available and open-licensed asp source project as a corpus. dual-licensed under cc-by and apache-2.0 license. compiled by liam doherty. more information and the scripts used to generate these files are available here.\n",
      "inspiration:\n",
      "this dataset is mainly helpful for use during nlp analysis, however there may some interesting insights in the data.\n",
      "what qualities do stopwords share across languages? given a novel language, could you predict what its stopwords should be?\n",
      "what stopwords are shared across languages?\n",
      "often, related languages will have words with the same meaning and similar spellings. can you automatically identify any of these pairs of words?\n",
      "you may also like:\n",
      "stopword lists for 19 languages (mainly european and south asian)\n",
      "context:\n",
      "the armed conflict location and event data project is designed for disaggregated conflict analysis and crisis mapping. this dataset codes the dates and locations of all reported political violence and protest events in developing asian countries in. political violence and protest includes events that occur within civil wars and periods of instability, public protest and regime breakdown. the project covers 2015 to the present.\n",
      "content:\n",
      "these data contain information on:\n",
      "dates and locations of conflict events;\n",
      "specific types of events including battles, civilian killings, riots, protests and recruitment activities;\n",
      "events by a range of actors, including rebels, governments, militias, armed groups, protesters and civilians;\n",
      "changes in territorial control; and\n",
      "reported fatalities.\n",
      "event data are derived from a variety of sources including reports from developing countries and local media, humanitarian agencies, and research publications. please review the codebook and user guide for additional information: the codebook is for coders and users of acled, whereas the brief guide for users reviews important information for downloading, reviewing and using acled data. a specific user guide for development and humanitarian practitioners is also available, as is a guide to our sourcing materials.\n",
      "acknowledgements:\n",
      "acled is directed by prof. clionadh raleigh (university of sussex). it is operated by senior research manager andrea carboni (university of sussex) for africa and hillary tanoff for south and south-east asia. the data collection involves several research analysts, including charles vannice, james moody, daniel wigmore-shepherd, andrea carboni, matt batten-carew, margaux pinaud, roudabeh kishi, helen morris, braden fuller, daniel moody and others. please cite:\n",
      "raleigh, clionadh, andrew linke, håvard hegre and joakim karlsen. 2010. introducing acled-armed conflict location and event data. journal of peace research 47(5) 651-660.\n",
      "inspiration:\n",
      "do conflicts in one region predict future flare-ups? how do the individual actors interact across time?\n",
      "context\n",
      "i want to create an app that could generate instrumentals of songs that we listen daily.\n",
      "content\n",
      "i have generated wav files of different notes using garageband. i will use this data to classify musical notes.\n",
      "acknowledgements\n",
      "i took stanford paper on sheet music from audio files by jan dlabal and richard wedeen\n",
      "inspiration\n",
      "can we even generate midi files of complicated melodies just using wav files of the song.\n",
      "this dataset contains run time statistics and details about scores for the first development round of nips 2017 adversarial learning competition\n",
      "content\n",
      "matrices with intermediate results\n",
      "following matrices with intermediate results are provided:\n",
      "accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense\n",
      "error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense\n",
      "hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense\n",
      "in each of these matrices, rows correspond to defenses, columns correspond to attack. also first row and column are headers with kaggle team ids (or baseline id).\n",
      "scores and run time statistics of submissions\n",
      "following files contain scores and run time stats of the submissions:\n",
      "non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks\n",
      "targeted_attack_results.csv - scores and run time statistics of all targeted attacks\n",
      "defense_results.csv - scores and run time statistics of all defenses\n",
      "each row of these files correspond to one submission. columns have following meaning:\n",
      "kaggleteamid - either kaggle team id or id of the baseline.\n",
      "teamname - human readable team name\n",
      "score - raw score of the submission\n",
      "normalizedscore - normalized (to be between 0 and 1) score of the submission\n",
      "minevaltime - minimum evaluation time of 100 images\n",
      "maxevaltime - maximum evaluation time of 100 images\n",
      "medianevaltime - median evaluation time of 100 images\n",
      "meanevaltime - average evaluation time of 100 images\n",
      "notes about the data\n",
      "due to team merging these files contain slightly more submissions than reflected in leaderboard.\n",
      "also not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score).\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one data file for each of these countries:\n",
      "bermuda - bermuda.csv\n",
      "canada - canada.csv\n",
      "curaçao - curaçao.csv\n",
      "jamaica - jamaica.csv\n",
      "mexico - mexico.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets to map weather, crime, or how your next canoing trip.\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one data file for each of these countries:\n",
      "argentina - argentina.csv\n",
      "brazil - brazil.csv\n",
      "chile - chile.csv\n",
      "columbia - columbia.csv\n",
      "uraguay - uraguay.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets to map weather, crime, or plan your next canoeing trip.\n",
      "context\n",
      "this dataset contains lot of historical sales data. it was extracted from a brazilian top retailer and has many skus and many stores. the data was transformed to protect the identity of the retailer.\n",
      "content\n",
      "[tbd]\n",
      "acknowledgements\n",
      "this data would not be available without the full collaboration from our customers who understand that sharing their core and strategical information has more advantages than possible hazards. they also support our continuos development of innovative ml systems across their value chain.\n",
      "inspiration\n",
      "every retail business in the world faces a fundamental question: how much inventory should i carry? in one hand to mush inventory means working capital costs, operational costs and a complex operation. on the other hand lack of inventory leads to lost sales, unhappy customers and a damaged brand.\n",
      "current inventory management models have many solutions to place the correct order, but they are all based in a single unknown factor: the demand for the next periods.\n",
      "this is why short-term forecasting is so important in retail and consumer goods industry.\n",
      "we encourage you to seek for the best demand forecasting model for the next 2-3 weeks. this valuable insight can help many supply chain practitioners to correctly manage their inventory levels.\n",
      "\"first-person narratives of the american south\" is a collection of diaries, autobiographies, memoirs, travel accounts, and ex-slave narratives written by southerners. the majority of materials in this collection are written by those southerners whose voices were less prominent in their time, including african americans, women, enlisted men, laborers, and native americans.\n",
      "the narratives available in this collection offer personal accounts of southern life between 1860 and 1920, a period of enormous change. at the end of the civil war, the south faced the enormous challenge of re-creating their society after their land had been ravaged by war, many of their men were dead or injured, and the economic and social system of slavery had been abolished. many farmers, confronted by periodic depressions and market turmoil, joined political and social protest movements. for african americans, the end of slavery brought hope for unprecedented control of their own lives, but whether they stayed in the south or moved north or west, they continued to face social and political oppression. most african americans in the south were pulled into a darwinistic sharecropper system and saw their lives circumscribed by the rise of segregation. as conservative views faced a growing challenge from modernist thought, southern arts, sciences, and religion also reflected the considerable tensions manifested throughout southern society. admidst these dramatic changes, southerners who had lived in the antebellum south and soldiers who had fought for the confederacy wrote memoirs that and strived to preserve a memory of many different experiences. southerners recorded their stories of these tumultuous times in print and in diaries and letters, but few first-person narratives, other than those written by the social and economic elite found their way into the national print culture. in this online collection, accounts of life on the farm or in the servants' quarters or in the cotton mill have priority over accounts of public lives and leading military battles. each narrative offers a unique perspective on life in the south, and serves as an important primary resource for the study of the american south. the original texts for \"first-person narratives of the american south\" come from the university library of the university of north carolina at chapel hill, which includes the southern historical collection, one of the largest collections of southern manuscripts in the country and the north carolina collection, the most complete printed documentation of a single state anywhere. the docsouth editorial board, composed of faculty and librarians at unc and staff from the unc press, oversees this collection and all other collections on documenting the american south.\n",
      "context\n",
      "the north american slave narratives collection at the university of north carolina contains 344 items and is the most extensive collection of such documents in the world.\n",
      "the physical collection was digitized and transcribed by students and library employees. this means that the text is far more reliable than uncorrected ocr output which is common in digitized archives.\n",
      "more information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh\n",
      "the plain text files have been optimized for use in voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. you may wish to delete these in order to focus your analysis on just the narratives.\n",
      "the .csv file acts as a table of contents for the collection and includes title, author, publication date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with voyant: http://voyant-tools.org/).\n",
      "copyright statement and acknowledgements\n",
      "with the exception of \"fields's observation: the slave narrative of a nineteenth-century virginian,\" which has no known rights, the texts, encoding, and metadata available in open docsouth are made available for use under the terms of a creative commons attribution license (cc by 4.0:http://creativecommons.org/licenses/by/4.0/). users are free to copy, share, adapt, and re-publish any of the content in open docsouth as long as they credit the university library at the university of north carolina at chapel hill for making this material available.\n",
      "if you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. send any feedback to wilsonlibrary@unc.edu.\n",
      "about the docsouth data project\n",
      "doc south data provides access to some of the documenting the american south collections in formats that work well with common text mining and data analysis tools.\n",
      "documenting the american south is one of the longest running digital publishing initiatives at the university of north carolina. it was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.\n",
      "doc south data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. we have made it easy to use tools such as voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis.\n",
      "feel free to use this data.\n",
      "thanks\n",
      "context\n",
      "per capita total expenditure on health at average exchange rate (us$)\n",
      "content\n",
      "per capita total expenditure on health expressed at average exchange rate for that year in us$. current prices.\n",
      "acknowledgements\n",
      "it is downloaded from who, gapminder.\n",
      "inspiration\n",
      "it seems good to me for forecasting the total spending of money on health around the world, it can be good use case for prediction of money spending on health spending.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "information reproduced from the national archives:\n",
      "the korean conflict extract data file of the defense casualty analysis system (dcas) extract files contains records of u.s. military fatal casualties of the korean war. these records were transferred into the custody of the national archives and records administration in 2008. the defense casualty analysis system extract files were created by the defense manpower data center (dmdc) of the office of the secretary of defense. the records correspond to the korean war conflict statistics on the dmdc web site, which is accessible online at https://www.dmdc.osd.mil/dcas/pages/main.xhtml .\n",
      "a full series description for the defense casualty analysis system (dcas) extract files is accessible online via the national archives catalog under the national archives identifier 2240988. the korean war conflict extract data file is also accessible for direct download via the national archives catalog file-level description, national archives identifier 2240988.\n",
      "content\n",
      "the raw data files have been cleaned and labelled as best as i can with reference to the accompanying supplemental code lists. names and id numbers have been removed out of respect and to provide anonymity.\n",
      "data fields: * service_type * service_code * enrollment * branch * rank * pay_grade * position * birth_year * sex * home_city * home_county * home_state * state_code * nationality * marital_status * ethnicity * ethnicity_1 * ethnicity_2 * division * fatality_year * fatality_date * hostility_conditions * fatality * burial_status\n",
      "acknowledgements\n",
      "data provided by the u.s. national archives and records administration.\n",
      "raw data can be accessed via the following link: https://catalog.archives.gov/id/2240988\n",
      "inspiration\n",
      "by cleaning the data i hope to give wider access to this resource.\n",
      "why?\n",
      "https://twitter.com/lindsaylee13/status/826298008824328192\n",
      "i expressed my interest in using my technical skills anywhere i could. one of the suggestions made was to have a running list of representation throughout the nation.\n",
      "my approach for this case was to retrieve a list of zip codes and return representation via google's civic information api.\n",
      "my source for zip codes is: https://www.aggdata.com/node/86\n",
      "this data set currently has just over 29,000 of the total 43,000 listed here. this is due to:\n",
      "1) rate limit 2) zip codes not found\n",
      "my goal is to continue to expand on the list of zip codes to get as comprehensive of a view of representation as possible.\n",
      "this is not possible without google's civic information api!\n",
      "content\n",
      "the bulletins were downloaded from the dane web platform and organized by products in cvs files.\n",
      "acknowledgements\n",
      "sipsa\n",
      "context\n",
      "wine recognition dataset from uc irvine. great for testing out different classifiers\n",
      "labels: \"name\" - number denoting a specific wine class\n",
      "number of instances of each wine class\n",
      "class 1 - 59\n",
      "class 2 - 71\n",
      "class 3 - 48\n",
      "features:\n",
      "alcohol\n",
      "malic acid\n",
      "ash\n",
      "alcalinity of ash\n",
      "magnesium\n",
      "total phenols\n",
      "flavanoids\n",
      "nonflavanoid phenols\n",
      "proanthocyanins\n",
      "color intensity\n",
      "hue\n",
      "od280/od315 of diluted wines\n",
      "proline\n",
      "content\n",
      "\"this data set is the result of a chemical analysis of wines grown in the same region in italy but derived from three different cultivars. the analysis determined the quantities of 13 constituents found in each of the three types of wines\"\n",
      "acknowledgements\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "@misc{lichman:2013 , author = \"m. lichman\", year = \"2013\", title = \"{uci} machine learning repository\", url = \"http://archive.ics.uci.edu/ml\", institution = \"university of california, irvine, school of information and computer sciences\" }\n",
      "uc irvine data base: \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine\"\n",
      "sources: (a) forina, m. et al, parvus - an extendible package for data exploration, classification and correlation. institute of pharmaceutical and food analysis and technologies, via brigata salerno, 16147 genoa, italy. (b) stefan aeberhard, email: stefan@coral.cs.jcu.edu.au (c) july 1991 past usage: (1) s. aeberhard, d. coomans and o. de vel, comparison of classifiers in high dimensional settings, tech. rep. no. 92-02, (1992), dept. of computer science and dept. of mathematics and statistics, james cook university of north queensland. (also submitted to technometrics).\n",
      "the data was used with many others for comparing various classifiers. the classes are separable, though only rda has achieved 100% correct classification. (rda : 100%, qda 99.4%, lda 98.9%, 1nn 96.1% (z-transformed data)) (all results using the leave-one-out technique)\n",
      "(2) s. aeberhard, d. coomans and o. de vel, \"the classification performance of rda\" tech. rep. no. 92-01, (1992), dept. of computer science and dept. of mathematics and statistics, james cook university of north queensland. (also submitted to journal of chemometrics).\n",
      "inspiration\n",
      "this data set is great for drawing comparisons between algorithms and testing out classifications models when learning new techniques\n",
      "context\n",
      "human communication abilities have greatly evolved with time. speech/text/images/videos are the channels we often use to communicate, store/share information.\"text\" is one of the primary modes in formal communication and might continue to be so for quite some time.\n",
      "i wonder, how many words, a person would type in his lifetime, when he sends an email/text message or prepare some documents. the count might run into millions. we are accustomed to key-in words, without worrying much about the 'effort' involved in typing the word. we don't bother much about the origin of the word or the correlation between the meaning and the textual representation. 'big' is actually smaller than 'small' just going by the words' length.\n",
      "i had some questions, which, i thought, could be best answered by analyzing the big data we are surrounded with today. since the data volume growing at such high rates, can we bring about some kind of optimization or restructuring in the word usage, so that, we are benefited in terms of data storage, transmission, processing. can scanning more documents, would provide better automated suggestions in email / chats, based on what word usually follows a particular word, and assist in quicker sentence completion.\n",
      "what set of words, in the available text content globally, if we can identify and condense, would reduce the overall storage space required.\n",
      "what set of words in the regular usage, email/text/documents, if we condense, would reduce the total effort involved in typing (keying-in the text) and reduce the overall size of the text content, which eventually might lead to lesser transmission time, occupy less storage space, lesser processing time for applications which feed on these data for analysis/decision making.\n",
      "to answer these, we may have to parse the entire web and almost every email/message/blog post/tweet/machine generated content that is in or will be generated in every phone/laptop/computer/servers, data generated by every person/bot. considering tones of text lying around in databases across the world webpages/wikipedia/text archives/digital libraries, and the multiple versions/copies of these content. parsing all, would be a humongous task. fresh data is continually generated from various sources. the plate is never empty, if the data is cooked at a rate than the available processing capability.\n",
      "here is an attempt to analyze a tiny chunk of data, to see, if the outcome is significant enough to take a note of, if the finding is generalized and extrapolated to larger databases.\n",
      "content\n",
      "looking out for a reliable source, i could not think of anything better than the wikipedia database of webpages. wiki articles are available for download as html dumps, for any offline processing. https://dumps.wikimedia.org/other/static_html_dumps/, the dump which i downloaded is a ~40 gb compressed file (that turned in ~208 gb folder containing ~15 million files, upon extraction).\n",
      "with my newly acquired r skills, i tried to parse the html pages, extract the distinct words with their total count in the page paragraphs.i could consolidate the output from the \"first million\" of html files out of available 15 million. attached dataset \"wikiwords_firstmillion.csv\" is a comma separated file with the list of words and their count. there are two columns - word and count. \"word\" column contains distinct words as extracted from the paragraphs in the wiki pages and \"count\" column has the count of occurrence in one million wiki pages. non-alphanumeric characters have been removed at the time of text extraction.\n",
      "any array of characters separated by space are included in the list of words and the count has been presented as is without any filters. to get better estimates, it should be ok to make suitable assumptions, like considering root words, ignoring words if they appear more specific to wikipedia pages (welcome, wikipedia, articles, pages, edit, contribution.. ).\n",
      "acknowledgements\n",
      "wikimedia, for providing the offline dumps r community, for the software/packages/blog posts/articles/suggestions and solution on the q & a sites\n",
      "inspiration\n",
      "in case, the entire english language community across the world decides to designate every alphabet as a word [apart from 'a' and 'i' all other alphabets seem to be potential candidates to be a word, a one-lettered word],\n",
      "(a) which of the 24 words from the data set are most eligible to get upgraded as a one letter word. assuming, it is decided to replace the existing words with the newly designated one-lettered word, to achieve storage efficiency.\n",
      "(b) assuming, the word count in the data set is a fair estimate of the composition of the words available in the global text content, (say we do a \"find\" and \"replace\" on global text content). if the current big data size is 3 exabytes (10 ^ 18), and say 30% of it is text content, how much space reduction can be achieved with (a), assuming 1 character requires 1 byte of storage space.\n",
      "(c) suppose, it is decided to accommodate 10 short cut keys for 10 different words,one short cut key for each word. which 10 words would increase the speed of text documentation assuming, (1) same amount of time is taken to type any word (2) time required to type a word increases with its length\n",
      "curious to know the findings!!\n",
      "context\n",
      "publicly available data from the city of chicago. see more here.\n",
      "content\n",
      "data is from jan 1, 2001 - dec. 31, 2016. the original dataset has over 6,000,000 entries (1.4gb)...this is just a 1% random sample to make computation time quicker and because of kaggle's 500mb limit.\n",
      "please visit this dataset's official page for a more complete description and list of caveats.\n",
      "context\n",
      "this dataset is for showing how to visualize od datasets\n",
      "content\n",
      "this dataset contains all the cities where the british queen has visited in her lifetime.\n",
      "acknowledgements\n",
      "the dataset is obtained from the internet.\n",
      "past research\n",
      "no\n",
      "inspiration\n",
      "showing od dataset is very fun.\n",
      "82.558 human instructions in chinese extracted from wikihow\n",
      "step-by-step instructions in chinese extracted from wikihow and decomposed into a formal graph representation in rdf.\n",
      "this is one of multiple dataset repositories for different languages. for more information, additional resources, and other versions of these instructions in other languages see the main kaggle dataset page:\n",
      "https://www.kaggle.com/paolop/human-instructions-multilingual-wikihow\n",
      "to cite this dataset use: paolo pareti, benoit testu, ryutaro ichise, ewan klein and adam barker. integrating know-how into the linked data cloud. knowledge engineering and knowledge management, volume 8876 of lecture notes in computer science, pages 385-396. springer international publishing (2014) (pdf) (bibtex)\n",
      "this dataset is based on original instructions from wikihow accessed on the 3rd of march 2017.\n",
      "for any queries and requests contact: paolo pareti\n",
      "context\n",
      "i recently implemented an iphone app for my younger daughter to learn multiplication. it probably was as much of a learning exercise for me as it was for her. but now i want to tackle the next challenge and add some more smarts with a module to recognise her handwritten digits. instead of taking a pixel based approach, i took the opportunity of the touch-based input device to record the training data as vectorized paths.\n",
      "content\n",
      "the data contains 400 paths for digits along with their matching labels. each path is normalized to 20 vectors (2d) and the total length of the vectors is normalized to ~100. find an example record of the json-structure below:\n",
      "{ \"items\": [ {\"y\":1, \"id\": 1488036701781, \"p\": [[0,0],[1,-2],[7,-9],[3,-5],[6,-9],[2,-3],[3,-4],[1,-1],[1,-1],[0,0],[0,2],[0,10],[0,8],[0,17],[0,6],[0,10],[0,3],[0,3],[0,0],[-1,1]] }] }\n",
      "acknowledgements\n",
      "thanks to my daughter for the training data!\n",
      "inspiration\n",
      "i hope you have fun with learning and i very much welcome hints on how to better capture the data.\n",
      "mnist data from http://neuralnetworksanddeeplearning.com\n",
      "context\n",
      "condensation of data from world bank (gdp) and bacen - banco central do brasil (electricity consumption). gdp is expressed in usd trillion and electricity in terawatt hour year.\n",
      "content\n",
      "three columns : \"year\", \"tw/h\" and \"gdp\"\n",
      "acknowledgements\n",
      "world bank and bacen.\n",
      "context\n",
      "this will act as the base data for the investigation into the possible solutions for the uk energy requirements\n",
      "content\n",
      "a cleaned version of the uk statistics on renewable energy generation.\n",
      "acknowledgements\n",
      "https://www.gov.uk/government/statistics/regional-renewable-statistics7\n",
      "all content is available under the open government licence v3.0,\n",
      "context\n",
      "in progress...\n",
      "content\n",
      "in progress...\n",
      "acknowledgements\n",
      "in progress...\n",
      "inspiration\n",
      "in progress...\n",
      "the indieweb is a people-focused alternative to the \"corporate\" web. participants use their own personal web sites to post, reply, share, organize events and rsvp, and interact in online social networking in ways that have otherwise been limited to centralized silos like facebook and twitter.\n",
      "the indie map dataset is a social network of the 2300 most active indieweb sites, including all connections between sites and number of links in each direction, broken down by type. it includes:\n",
      "5.8m web pages, including raw html, parsed microformats2, and extracted links with metadata.\n",
      "631m links and 706k \"friend\" relationships between sites.\n",
      "380gb of html and http requests in warc format.\n",
      "the zip file here contains a json file for each site, which includes metadata, a list of other sites linked to and from, and the number of links of each type.\n",
      "the complete dataset of 5.8m html pages is available in a publicly accessible google bigquery dataset. the raw pages can also be downloaded as warc files. they're hosted on google cloud storage.\n",
      "more details in the full documentation.\n",
      "indie map is free, open source, and placed into the public domain via cc0. crawled content remains the property of each site's owner and author, and subject to their existing copyrights.\n",
      "you can find the brand, model and version information of almost all automobile manufacturers between 1970 and 2016 in this database.\n",
      "context\n",
      "classification calories and food types\n",
      "content\n",
      "it's useful to analyse the food related queries by finding calories level, preferences, veg type etc.,\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "want to know the food preference quickly ?\n",
      "context\n",
      "the united states constitution provides that the president \"shall nominate, and by and with the advice and consent of the senate, shall appoint ambassadors, other public ministers and consuls, judges of the supreme court, and all other officers of the united states, whose appointments are not herein otherwise provided for...\" (article ii, section 2). this provision, like many others in the constitution, was born of compromise, and, over the more than two centuries since its adoption, has inspired widely varying interpretations.\n",
      "the president nominates all federal judges in the judicial branch and specified officers in cabinet-level departments, independent agencies, the military services, the foreign service and uniformed civilian services, as well as u.s. attorneys and u.s. marshals. the importance of the position, the qualifications of the nominee, and the prevailing political climate influence the character of the senate's response to each nomination. views of the senate's role range from a narrow construction that the senate is obligated to confirm unless the nominee is manifestly lacking in character and competence, to a broad interpretation that accords the senate power to reject for any reason a majority of its members deems appropriate. just as the president is not required to explain why he selected a particular nominee, neither is the senate obligated to give reasons for rejecting a nominee.\n",
      "acknowledgements\n",
      "the confirmation vote records were recorded, compiled, and published by the office of the secretary of the senate.\n",
      "image enhancement\n",
      "the data accompanying the lecture about image enhancement from anders kaestner as part of the quantitative big imaging course.\n",
      "the slides for the lecture are here\n",
      "context\n",
      "this is a small data set of us presidents heights.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "content\n",
      "this is a slightly modified version of the openly available data set 'open data 500 companies - full list' provided by the od500 global network ('http://www.opendata500.com/').\n",
      "i am using this dataset for a kernel project series which will be investigating the value and worth of a company's choice of logo design. therefore, have removed columns such as \"description\" and \"short description\", as well as a few others. if you'd like the entire original dataset please download from the original source here --> 'http://www.opendata500.com/us/download/us_companies.csv'\n",
      "acknowledgements\n",
      "i take no credit for the collection, production, or presentation of this data set. i am simply using it for a person research study. the creators are: http://www.opendata500.com/us/list/\n",
      "context\n",
      "automotive crash data for 2014\n",
      "content\n",
      "data on weather conditions, people involved, locations etc where traffic accidents have occured.\n",
      "acknowledgements\n",
      "this dataset is simply a port to csv from an xslx dataset uploaded by aditi\n",
      "inspiration\n",
      "interested to know what are the factors involved that make auto accidents fatal\n",
      "this codded dataset is from marketing agency and reflact internet users activity at the site page.\n",
      "context\n",
      "calcium, dust and nitrate concentrations in monthly resolution in ice core dml94c07_38 (b38). doi:10.1594/pangaea.837875\n",
      "content\n",
      "acknowledgements\n",
      "schmidt, kerstin; wegner, anna; weller, rolf; leuenberger, daiana; tijm-reijmer, carleen h; fischer, hubertus (2014): calcium, dust and nitrate concentrations in monthly resolution in ice core dml94c07_38 (b38). doi:10.1594/pangaea.837875, in supplement to: schmidt, kerstin; wegner, anna; weller, rolf; leuenberger, daiana; fischer, hubertus (submitted): variability of aerosol tracers in ice cores from coastal dronning maud land, antarctica. the cryosphere\n",
      "inspiration\n",
      "test data\n",
      "context\n",
      "aggregate and visualize data how you want—from tabular to graphical and geographic forms—for more profitable underwriting.\n",
      "perform sophisticated, multi-dimensional analysis to supply any data combination and permutation.\n",
      "respond to claims quickly and improve customer satisfaction with real-time and historical access to catastrophe and hazard data\n",
      "content\n",
      "the sample insurance file contains 36,634 records in florida for 2012 from a sample company that implemented an agressive growth plan in 2012. there are total insured value (tiv) columns containing tiv from 2011 and 2012, so this dataset is great for testing out the comparison feature. this file has address information that you can choose to geocode, or you can use the existing latitude/longitude in the file.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this was the dataset used in the article 'sentiment analysis of portuguese comments from foursquare' (doi:10.1145/2976796.2988180).\n",
      "the dataset is composed of tips referring to localities of the city of são paulo/brazil. to the data collection, we use the foursquare api . the tips belong to the foursquare's categories: food, shop & service and nightlife spot. the database has a total of 179,181 tips.\n",
      "@inproceedings{almeida:2016:sap:2976796.2988180, author = {almeida, thais g. and souza, bruno a. and menezes, alice a.f. and figueiredo, carlos m.s. and nakamura, eduardo f.}, title = {sentiment analysis of portuguese comments from foursquare}, booktitle = {proceedings of the 22nd brazilian symposium on multimedia and the web}, series = {webmedia '16}, year = {2016}, isbn = {978-1-4503-4512-5}, location = {teresina, piau\\&#237; state, brazil}, pages = {355--358}, numpages = {4}, url = {http://doi.acm.org/10.1145/2976796.2988180}, doi = {10.1145/2976796.2988180}, acmid = {2988180}, publisher = {acm}, address = {new york, ny, usa}, keywords = {foursquare, sentiment analysis, supervised learning}, }\n",
      "context\n",
      "lexique v3.81 on www.lexique.org/\n",
      "content\n",
      "words of the french language with pronunciation, grouping and statistics.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "here i have data for last 10 year average indian rupee against usd. it will try to predict the value of a year.\n",
      "i am using leaner regression for this\n",
      "i was new to machine learning. i was thinking of how usd values made changes in india how that changes happen if we passed any year this program will try to predict\n",
      "context\n",
      "increase your earnings through analyzing the admob data set.\n",
      "content\n",
      "it has more than 12 column including earnings, impressions, and other attributes\n",
      "acknowledgements\n",
      "suleman bader owns this data set compiled by google admob.\n",
      "inspiration\n",
      "to increase your earnings\n",
      "council plan performance indicators by coventry city council\n",
      "the council plan sets out coventry city council's vision and priorities for the city.\n",
      "content\n",
      "these are the performance indicators in the council plan performance report in an open data format.\n",
      "acknowledgements\n",
      "coventry city council\n",
      "find out more at: www.coventry.gov.uk/performance/ visualisation: https://smarturl.it/covperformancedata\n",
      "the opennebula is an open source environment which provides cloud resources with the help of haizea as a lease manager. the haizea supports different types of leases from which deadline sensitive lease is one of them. in real time, most of the leases are deadline sensitive leases. these deadline sensitive leases are scheduled by using the backfilling based scheduling algorithms.\n",
      "please don't forget to cite:\n",
      "nayak, suvendu chandan, and chitaranjan tripathy. \"deadline sensitive lease scheduling in cloud computing environment using ahp.\" journal of king saud university-computer and information sciences (2016).\n",
      "import libraries\n",
      "    suppressmessages(library(xlconnect))\n",
      "    suppressmessages(library(xlsx))\n",
      "    suppressmessages(library(rvest))\n",
      "    suppressmessages(library(dplyr))\n",
      "    suppressmessages(library(lubridate))\n",
      "    suppressmessages(library(ggplot2))\n",
      "    suppressmessages(library(plotly))\n",
      "\n",
      "    setwd(\".../r dolartoday/\")\n",
      "\n",
      "    #download & import data from dolartoday.com | the \"url\" changes every 5 minutes aprox\n",
      "    url <- 'https://dolartoday.com/indicadores/'\n",
      "    url <- read_html(url) %>% html_nodes('a') %>% html_attrs()\n",
      "    url <- as.character(url[grep('xlsx',url)])\n",
      "    download.file(url,\"dolartoday.xlsx\",mode=\"wb\")\n",
      "    dt <- read.xlsx(\"dolartoday.xlsx\", sheetname=\"dolartoday\", colindex = 1:2)\n",
      "    dt$fecha <- as.date(dt$fecha, format = \"%m-%d-%y\")\n",
      "    dt$año <- as.numeric(format(dt$fecha,'%y'))\n",
      "    dt$dolartoday <- gsub(\",\",\".\",dt$dolartoday)\n",
      "    dt$dolartoday <- as.numeric(dt$dolartoday)\n",
      "\n",
      "    #add simadi data\n",
      "    simadi <- 'http://cambiobolivar.com/sistema-marginal-de-divisas/'\n",
      "    simadi <- read_html(simadi) %>% html_nodes('.wp-table-reloaded') %>% html_table()\n",
      "    simadi <- rbind(as.data.frame(simadi[1]),\n",
      "                    as.data.frame(simadi[2]),\n",
      "                    as.data.frame(simadi[3]),\n",
      "                    as.data.frame(simadi[4]))\n",
      "    simadi$fecha <- as.date(simadi$fecha,format=\"%d/%m/%y\")\n",
      "    simadi$tasa.simadi <- as.numeric(sub(\",\",\".\",sub(\",\",\".\",simadi$tasa.simadi)))\n",
      "    colnames(simadi) <- c(\"fecha\",\"simadi\")\n",
      "\n",
      "    #merge data\n",
      "    dt <- (merge(dt, simadi, all = true))\n",
      "    dt <- mutate(dt,relación=dolartoday/simadi)\n",
      "\n",
      "    # first plot\n",
      "    plot <- dt[dt$año>=2016,]\n",
      "    plot_ly(plot, x = ~fecha, y = ~dolartoday, name = 'dolartoday',type='scatter',connectgaps=true,mode='lines') %>%\n",
      "      add_trace(y = ~simadi, name='simadi',mode='lines', connectgaps=true) %>%\n",
      "      layout(title=\"dólar paralelo en venezuela\",\n",
      "             xaxis = list(title = \"fecha\"),\n",
      "             yaxis = list(title = \"bolívares por usd\")\n",
      "    )\n",
      "\n",
      "# second plot\n",
      "    dt$mes <- month(dt$fecha)\n",
      "    dt$año_mes <- paste(dt$año,dt$mes,sep=\"-\")\n",
      "    ggplot(filter(dt,\n",
      "                  año >= 2016), \n",
      "           aes(x = as.factor(mes), y = dolartoday)) +\n",
      "      geom_jitter(alpha = 0.7) +\n",
      "      geom_boxplot(fill = 'red', alpha = 0.7) +\n",
      "      ggtitle(\"boxplot: dolartoday por mes\") +\n",
      "      theme_bw() +\n",
      "      scale_y_continuous(name = \"dolartoday\") +\n",
      "      scale_x_discrete(name = \"meses\") +\n",
      "      facet_grid(año ~ .)\n",
      "    summary(filter(dt,año==2017)%>%select(dolartoday))\n",
      "this describe names of girls and boys, include the meaning of there names.\n",
      "the cells are: gender, name, meaning, origin\n",
      "this data can help to enrich other's data sets.\n",
      "connectionist bench (sonar, mines vs. rocks) data set\n",
      "content\n",
      "http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)\n",
      "content\n",
      "money supply m2 (broader money) bric economies in $ 1997-01-01 to 2015-12-01\n",
      "frequency quarterly\n",
      "context\n",
      "this is speeches scraped from unhcr.org website. it includes all speeches made by the high commissioner up until june 2014.\n",
      "content\n",
      "json array of speeches with the following keys: \"author\", \"by\", \"content\", \"id\", \"title\".\n",
      "acknowledgements\n",
      "www.unhcr.org\n",
      "inspiration\n",
      "what words are most used? which countries are mentioned? what do the high commissioners think about innovation? gay rights?\n",
      "this is filtered weather data that includes the date, precipitation, and average precipitation in syracuse, ny from june-september 2011-2016.\n",
      "us domestic flights delay: flight delays in the month of january,august, november and december of 2016\n",
      "summary\n",
      "this dataset (ml-20m) describes 5-star rating and free-text tagging activity from movielens, a movie recommendation service. it contains 20000263 ratings and 465564 tag applications across 27278 movies. these data were created by 138493 users between january 09, 1995 and march 31, 2015. this dataset was generated on october 17, 2016.\n",
      "users were selected at random for inclusion. all selected users had rated at least 20 movies. no demographic information is included. each user is represented by an id, and no other information is provided.\n",
      "the data are contained in six files, genome-scores.csv, genome-tags.csv, links.csv, movies.csv, ratings.csv and tags.csv. more details about the contents and use of all these files follows.\n",
      "usage license\n",
      "neither the university of minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. the data set may be used for any research purposes under the following conditions:\n",
      "the user may not state or imply any endorsement from the university of minnesota or the grouplens research group. - the user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information).\n",
      "the user may not redistribute the data without separate permission.\n",
      "the user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the grouplens research project at the university of minnesota.\n",
      "the executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. the entire risk as to the quality and performance of them is with you. should the program prove defective, you assume the cost of all necessary servicing, repair or correction.\n",
      "in no event shall the university of minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).\n",
      "citation\n",
      "to acknowledge use of the dataset in publications, please cite the following paper:\n",
      "f. maxwell harper and joseph a. konstan. 2015. the movielens datasets: history and context. acm transactions on interactive intelligent systems (tiis) 5, 4, article 19 (december 2015), 19 pages. doi=http://dx.doi.org/10.1145/2827872\n",
      "content and use of files\n",
      "verifying the dataset contents\n",
      "we provide a md5 checksum with the same name as the downloadable .zip file, but with a .md5 file extension. to verify the dataset:\n",
      "on linux\n",
      "md5sum ml-20m.zip; cat ml-20m.zip.md5\n",
      "on osx\n",
      "md5 ml-20m.zip; cat ml-20m.zip.md5\n",
      "windows users can download a tool from microsoft (or elsewhere) that verifies md5 checksums\n",
      "check that the two lines of output contain the same hash value.\n",
      "formatting and encoding\n",
      "the dataset files are written as comma-separated values files with a single header row. columns that contain commas (,) are escaped using double-quotes (\"). these files are encoded as utf-8. if accented characters in movie titles or tag values (e.g. misã©rables, les (1995)) display incorrectly, make sure that any program reading the data, such as a text editor, terminal, or script, is configured for utf-8.\n",
      "user ids\n",
      "movielens users were selected at random for inclusion. their ids have been anonymized. user ids are consistent between ratings.csv and tags.csv (i.e., the same id refers to the same user across the two files).\n",
      "movie ids\n",
      "only movies with at least one rating or tag are included in the dataset. these movie ids are consistent with those used on the movielens web site (e.g., id 1 corresponds to the url https://movielens.org/movies/1). movie ids are consistent between ratings.csv, tags.csv, movies.csv, and links.csv (i.e., the same id refers to the same movie across these four data files).\n",
      "ratings data file structure (ratings.csv)\n",
      "all ratings are contained in the file ratings.csv. each line of this file after the header row represents one rating of one movie by one user, and has the following format: userid,movieid,rating,timestamp\n",
      "the lines within this file are ordered first by userid, then, within user, by movieid.\n",
      "ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
      "timestamps represent seconds since midnight coordinated universal time (utc) of january 1, 1970.\n",
      "tags data file structure (tags.csv)\n",
      "all tags are contained in the file tags.csv. each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:\n",
      "userid,movieid,tag,timestamp the lines within this file are ordered first by userid, then, within user, by movieid.\n",
      "tags are user-generated metadata about movies. each tag is typically a single word or short phrase. the meaning, value, and purpose of a particular tag is determined by each user.\n",
      "timestamps represent seconds since midnight coordinated universal time (utc) of january 1, 1970.\n",
      "movies data file structure (movies.csv)\n",
      "movie information is contained in the file movies.csv. each line of this file after the header row represents one movie, and has the following format:\n",
      "movieid,title,genres movie titles are entered manually or imported from https://www.themoviedb.org/, and include the year of release in parentheses. errors and inconsistencies may exist in these titles.\n",
      "genres are a pipe-separated list, and are selected from the following:\n",
      "action\n",
      "adventure\n",
      "animation\n",
      "children's\n",
      "comedy\n",
      "crime\n",
      "documentary\n",
      "drama\n",
      "fantasy\n",
      "film-noir\n",
      "horror\n",
      "musical\n",
      "mystery\n",
      "romance\n",
      "sci-fi\n",
      "thriller\n",
      "war\n",
      "western\n",
      "(no genres listed)\n",
      "links data file structure (links.csv)\n",
      "identifiers that can be used to link to other sources of movie data are contained in the file links.csv. each line of this file after the header row represents one movie, and has the following format:\n",
      "movieid,imdbid,tmdbid movieid is an identifier for movies used by https://movielens.org. e.g., the movie toy story has the link https://movielens.org/movies/1.\n",
      "imdbid is an identifier for movies used by http://www.imdb.com. e.g., the movie toy story has the link http://www.imdb.com/title/tt0114709/.\n",
      "tmdbid is an identifier for movies used by https://www.themoviedb.org. e.g., the movie toy story has the link https://www.themoviedb.org/movie/862.\n",
      "use of the resources listed above is subject to the terms of each provider.\n",
      "tag genome (genome-scores.csv and genome-tags.csv)\n",
      "this data set includes a current copy of the tag genome.\n",
      "the tag genome is a data structure that contains tag relevance scores for movies. the structure is a dense matrix: each movie in the genome has a value for every tag in the genome.\n",
      "as described in this article, the tag genome encodes how strongly movies exhibit particular properties represented by tags (atmospheric, thought-provoking, realistic, etc.). the tag genome was computed using a machine learning algorithm on user-contributed content including tags, ratings, and textual reviews.\n",
      "the genome is split into two files. the file genome-scores.csv contains movie-tag relevance data in the following format:\n",
      "movieid,tagid,relevance the second file, genome-tags.csv, provides the tag descriptions for the tag ids in the genome file, in the following format:\n",
      "tagid,tag the tagid values are generated when the data set is exported, so they may vary from version to version of the movielens data sets.\n",
      "cross-validation\n",
      "prior versions of the movielens dataset included either pre-computed cross-folds or scripts to perform this computation. we no longer bundle either of these features with the dataset, since most modern toolkits provide this as a built-in feature. if you wish to learn about standard approaches to cross-fold computation in the context of recommender systems evaluation, see lenskit for tools, documentation, and open-source code examples.\n",
      "did you know that donald j. trump for president, inc paid for a subscription to the new york times on november 30th, 2016? curious to see where else over six million usd was spent and for what purposes? this dataset was downloaded from pro publica so you can find out.\n",
      "content\n",
      "here's what you get:\n",
      "line number\n",
      "entity type\n",
      "payee name\n",
      "payee state\n",
      "date\n",
      "amount\n",
      "purpose\n",
      "contributions\n",
      "want to contribute to this dataset? download contributions to donald j. trump, inc here and share on the dataset's discussion forum.\n",
      "acknowledgements\n",
      "this data was downloaded from pro publica. you can read about their data terms of use here.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "uk postcodes with the below additional geospace fields:\n",
      "latitude\n",
      "longitude\n",
      "easting\n",
      "northing\n",
      "positional quality indicator\n",
      "postcode area\n",
      "postcode district\n",
      "postcode sector\n",
      "outcode\n",
      "incode\n",
      "country\n",
      "usecases\n",
      "geocode any dataset containing uk postcodes using either latitude and longitude or northing and easting.\n",
      "geocode older data using terminated postcodes.\n",
      "recognise postcodes in a variety of different formats, for example \"rg2 6aa\" and \"rg26aa\".\n",
      "group records into geographical hierarchies using postcode area, postcode district and postcode sector (see below graphic).\n",
      "format of a uk postcode\n",
      "documentation\n",
      "full documentation is available on the open postcode geo homepage:\n",
      "https://www.getthedata.com/open-postcode-geo\n",
      "version\n",
      "august 2016.\n",
      "updated quarterly, for the most up to date version please see:\n",
      "https://www.getthedata.com/open-postcode-geo\n",
      "api\n",
      "open postcode geo is also available as an api.\n",
      "licence\n",
      "uk open government licence\n",
      "attribution required (see ons licences for more info):\n",
      "contains os data © crown copyright and database right (2016)\n",
      "contains royal mail data © royal mail copyright and database right (2016)\n",
      "contains national statistics data © crown copyright and database right (2016)\n",
      "derived from the ons postcode directory.\n",
      "acknowledgements\n",
      "open postcode geo was created and is maintained by getthedata, an open data portal organising uk open data geographically and signposting the source.\n",
      "context\n",
      "the subject matter of this dataset explores tesla's stock price from its initial public offering (ipo) to yesterday.\n",
      "content\n",
      "within the dataset one will encounter the following:\n",
      "the date - \"date\"\n",
      "the opening price of the stock - \"open\"\n",
      "the high price of that day - \"high\"\n",
      "the low price of that day - \"low\"\n",
      "the closed price of that day - \"close\"\n",
      "the amount of stocks traded during that day - \"volume\"\n",
      "the stock's closing price that has been amended to include any distributions/corporate actions that occurs before next days open - \"adj[usted] close\"\n",
      "acknowledgements\n",
      "through python programming and checking sentdex out, i acquired the data from yahoo finance. the time period represented starts from 06/29/2010 to 03/17/2017.\n",
      "inspiration\n",
      "what happens when the volume of this stock trading increases/decreases in a short and long period of time? what happens when there is a discrepancy between the adjusted close and the next day's opening price?\n",
      "this data set contains recent tweets regarding the trump's call to taiwan. i mined twitter for recent 2000 tweets. it's 3rd dec, 2016, 6:19 indian standard time. can be used for sentiment analysis and future predictions. thanx to me and twitter did you like it ??\n",
      "context\n",
      "crime classification\n",
      "content\n",
      "crime classification\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "osu! is a music rythm game that has 4 modes (check for more infos). in this dataset you can examine the rankings of the standard mode, taken on 26/03/2017 around 12pm. ranking is based on pp (performance points) awarded after every play, which are influenced by play accuracy and score; pps are then summed with weights: your top play will award you the whole pp points of the map, then the percentage is decreased (this can maintain balance between strong players and players who play too much). you can find here many other statistics.\n",
      "content\n",
      "the dataset contains some columns (see below) reporting statistics for every player in the top100 of the game in the standard mode. the ranking are ordered by pp. some players seem to have the same points, but there are decimals which are not shown in the ranking chart on the site\n",
      "acknowledgements\n",
      "i created this dataset on my own, so if you find something wrong please report. the data is public and accessible on this link ranking.\n",
      "inspiration\n",
      "i uploaded this just for newcomers in the world (like me) who want an easy stuff to work with, but that can give a lot of results. check out my notebook that will soon follow\n",
      "context\n",
      "financial contributions to presidential campaigns by the residents of north carolina\n",
      "content\n",
      "i acquired the data for free on the website: http://fec.gov/disclosurep/pnational.do. out of all the states, i chose nc since i currently live in raleigh, nc. the data set mostly consists of categorical data such as the 'candidate name', 'contributor name', etc.\n",
      "acknowledgements\n",
      "udacity - exploratory data analysis online class through which i was taken to the above website and i completed a project analyzing this data.\n",
      "inspiration\n",
      "i have analyzed the data and made new variables too. i'm trying to think how linear models can be applied to such a data set and to my knowledge, it is really a waste of time. if you want to see my project, i will be posting it here and also would love to hear your opinion on what further analyses could be done/how it could be done!\n",
      "thank you!\n",
      "context\n",
      "looking at elections on tv or online is something, but that implies a ton of data. i was curious to see just how much. this is only a tiny bit of what analysts use.\n",
      "content\n",
      "this represents all votes for all polling stations in the 42nd general election, per cadidate.\n",
      "acknowledgements\n",
      "i won't take much credit; data comes straight from the elections canada website. i only rearranged the data.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "acknowledgments\n",
      "this dataset was downloaded from the open source sports website. it did not come with an explicit license, but based on other datasets from open source sports, we treat it as follows:\n",
      "this database is copyright 1996-2015 by sean lahman.\n",
      "this work is licensed under a creative commons attribution-sharealike 3.0 unported license. for details see: http://creativecommons.org/licenses/by-sa/3.0/\n",
      "this is modified to only include players info from 2000-2005\n",
      "context\n",
      "i was recently tasked to put together a list of vr games.\n",
      "content\n",
      "this is a single column list with titles of vr capable games.\n",
      "acknowledgements\n",
      "this list was put together from info available on steam website.\n",
      "inspiration\n",
      "wanted to put together a word cloud and see what words stand out.\n",
      "context\n",
      "air passengers per month. workshop dataset\n",
      "context\n",
      "while studying neural networks in machine learning, i found an ingenious 2-d scatter pattern at the cn231 course by andrej karpathy. decision boundaries for the three classes of points cannot be straight lines. he uses it to compare the behavior of a linear classifier and a neural network classifier. often, we are content with the percentage of accuracy of our prediction or classification algorithm but a visualization tool helps our intuition about the trends and behavior of the classifier. it was definitely useful to catch something strange in the last example of this description. how does your preferred method or algorithm do with this input?\n",
      "content\n",
      "the 2d scatter train pattern is a collection of 300 points with (x,y) coordinates where -1<x,y<1.\n",
      "the 300 points are divided into 3 classes of 100 points each whose labels are 0, 1 and 2. each class is an arm in the spiral.\n",
      "the test file is simply a grid covering a square patch within the (x,y) coordinate boundaries. the idea is to predict and assign labels to each point and visualize the decision boundaries and class areas.\n",
      "the train pattern is provided in two formats: libsvm and csv formats. the test pattern is also provided in the above formats.\n",
      "if you want to reproduce the data yourself or modify parameters, either run the python script in cn231 or you may check piconn out for a c++ version.\n",
      "here's the output i get with the neural network classifier piconn\n",
      "an svm classifier with radial basis kernel yields the following output. it looks better than the above neural network one.\n",
      "an svm classifier with linear kernel yields the following output. one can see it won't handle curved decision regions.\n",
      "an svm classifier with polynomial kernel yields the following output. interestingly it decided to bundle the center of the spiral.\n",
      "an svm classifier with sigmoid kernel yields the following output. there is definitely something going on. a bug in my code? i could had wrongfully discarded this kernel for bad accuracy. visualization was definitely useful for this case.\n",
      "acknowledgements\n",
      "i thank andrej karpathy for making his excellent cn231 machine learning course freely available.\n",
      "inspiration\n",
      "i was happy with the result of the 100-node neural network. i then tested with libsvm obtaining similar results. how do other algorithms do with it? would you draw yourself a different decision boundary?\n",
      "context\n",
      "in this study, large number of national stock exchange(nse), india stocks under different sectors are mined from various financial websites and data analytic steps are followed. primary goal of this work is to explore the hidden context patterns between diverse group of stocks and discover the predictive analytic knowledge using machine learning algorithms. the transaction dataset are captured for nse stocks using statistical computing software r. the price of the stock is determined by the market forces. buyers and sellers quote the preferred price, so there is a dynamic data day by day. though it is difficult to identify when to buy and sell the stock, technical indicators may support us to forecast the future price\n",
      "content\n",
      "a data frame with 8 variables: index, date, time, open, high, low, close and id. for each year from 2013 to 2016, the number of trading data of each minute of given each date. the currency of the price is indian rupee (inr).\n",
      "code : market id\n",
      "date : numerical value (ex. 20151203- to be converted as 2015/12/03)\n",
      "time : factor (ex. 09:16)\n",
      "open : numeric (opening price)\n",
      "high : numeric (high price)\n",
      "low : numeric (low price)\n",
      "close : numeric (closing price)\n",
      "volume : numeric (total volume traded)\n",
      "acknowledgements\n",
      "references [1] brett lantz, machine learning with r . packt publishing ltd., birmingham, uk , 2015. [2] the r project https://www.r-project.org/ [3] https://finance.yahoo.com/ [4] https://www.google.com/finance [5] https://www.nseindia.com/\n",
      "inspiration\n",
      "machine learning (nse stocks)\n",
      "context\n",
      "real estate data set of philly.\n",
      "content\n",
      "data set included addresses, sales price, crime rate and rank by zipcode, school ratings and rank by zipcode, walkscore and rank by zip code, approximate rehab cost,\n",
      "acknowledgements\n",
      "data from phila.gov and other sites\n",
      "inspiration\n",
      "find out how data could impact house price.\n",
      "this dataset does not have a description yet.\n",
      "i have compiled weekly rankings of college teams, and am making the archived data public.\n",
      "fbs football (since 1997): http://www.masseyratings.com/cf/compare.htm\n",
      "fcs football (since 2000): http://www.masseyratings.com/cf/compare1aa.htm\n",
      "basketball (since 2001): http://www.masseyratings.com/cb/compare.htm\n",
      "baseball (since 2010): http://www.masseyratings.com/cbase/compare.htm\n",
      "each line of the .csv files have the following fields:\n",
      "sportyear\n",
      "unique team id #\n",
      "team name\n",
      "ranking system's 2/3 letter code\n",
      "ranking system's name\n",
      "date of weekly ranking (yyyymmdd)\n",
      "ordinal ranking\n",
      "note: in most cases, the rankings were collected on monday or tuesday of each week, and are based on games played through sunday.\n",
      "please explore, test hypotheses, generate various metrics, time series, correlations, etc and please reference masseyratings.com in your research.\n",
      "this dataset is not novel. it is a copy of the student performance dataset available at uci machine learning repository\n",
      "ditrd - dilma impeachment twitter raw data\n",
      "abstract:\n",
      "dilma impeachment twitter raw data from \"dilma impeachment process period dec 2015 to jan 2016 and march 2016\".\n",
      "date donated: 10 feb 2016, 17 mar 2016.\n",
      "source:\n",
      "authors: caio moreno\n",
      "data set information:\n",
      "this data was collected using the the streaming apis.\n",
      "https://dev.twitter.com/streaming/overview\n",
      "dilma impeachment twitter raw data is a free and open source software. it is available under the terms of the apache license version 2.\n",
      "citation:\n",
      "if you publish material based on databases obtained from this repository, then, in your acknowledgements, please note the assistance you received by using this repository. this will help others to obtain the same data sets and replicate your experiments. we suggest the following pseudo-apa reference format for referring to this repository:\n",
      "moreno, c. (2016). ditrd-v1.0.0 - dilma impeachment twitter raw data [https://github.com/caiomsouza/twitterrawdata]. madrid, spain: u-tad, certificate program in data science.\n",
      "link do download ditrd-v1.0.0.\n",
      "https://github.com/caiomsouza/twitterrawdata/releases/download/ditrd-v1.0.0/twitter_raw_data.zip\n",
      "link do download ditrd-v1.0.1.\n",
      "https://github.com/caiomsouza/twitterrawdata/releases/download/ditrd-v1.0.1/ditrd-v1.0.1.zip\n",
      "csv files extracted using r\n",
      "https://github.com/caiomsouza/twitterrawdata/releases/download/dilmatwitterrawdata-extract-r-csv-v1.0.0/twittersdata-extract-r-csv.zip\n",
      "all releases: https://github.com/caiomsouza/twitterrawdata/releases\n",
      "cousin marriage data\n",
      "there was a story on fivethirtyeight.com about the prevalence of marriage to cousins in the united states. this is called consanguinity and is defined as marriages between individuals who are second cousins or closer. the article included data put together in 2001 for a number of countries. the data source and the article are listed below.\n",
      "the raw data behind the story dear mona: how many americans are married to their cousins? on fivethirtyeight.com.\n",
      "link to fivethirtyeight's public github repository.\n",
      "header | definition\n",
      "country | country names\n",
      "percent | percent of marriages that are consanguineous\n",
      "source: cosang.net\n",
      "data flaws: while the data does compile older sources and some self-reported data, it does match the trends of more recent data based on global genomic data.\n",
      "context:\n",
      "in an attempt to provide transparency to the population of brazil, the dadosaberto initiative publishes data on different topics related to the house of deputies. among those informations, is the amount of refunds requested by each deputy, on expenses related to their activities.\n",
      "this dataset compiles those expenses from 2009 to 2017, translate the categories to english, fix some dates (missing, invalid) for analysis purpouses, and discards some information thats not useful.\n",
      "there are some issues with the dataset provided by the initiative dadosabertos;\n",
      "the dates are related to the receipt, not to the actual refund request.\n",
      "parties/states relations to the candidate sometimes are missing\n",
      "social security numbers sometimes are missing.\n",
      "social security should always have length == 11 for a person or == 14 for a company. that doesnt happen.\n",
      "among other issues.\n",
      "ive converted dates that were missing or corrupted with only the year. ive also created a ohe column to help identify/filter those dates.\n",
      "content:\n",
      "bugged_date: (binary) identify wether date had issues receipt_date: (datetime) receipt date // (int year) for when bugged_date == 1 deputy_id: (deputy_id) id number. (didnt check if it changed across year/legislation period for deputies) political_party: (string) deputy political party state_code: (string) brazil's state that elected the deputy deputy_name: (string)\n",
      "receipt_social_security_number: might be a persons ss number (11 digits long) or a business id number (14 digits long). many cases with issues. receipt_description: (str / classes) class of spending under which the receipt fits establishment_name: (string) receipt_value: (int) $br, 3br$ ~ 1usd\n",
      "acknowledgments:\n",
      "this data is from dadosabertos.camara.leg.br\n",
      "** inspiration: **\n",
      "i have only slightly fiddled with the dataset, but there are some funny patterns to say the least. brazil faces a huge problem with corruption and neglect with public funds, this dataset helps to show that.\n",
      "i invite fellow kagglers to toy with the dataset, try to identify suspicious spendings as well as odd patterns.\n",
      "ps: this is a more complete version of a dataset i posted a year ago, which did not include translations or other years spendings.\n",
      "this research study was conducted to analyze the (potential) relationship between hardware and data set sizes. 100 data scientists from france between jan-2016 and aug-2016 were interviewed in order to have exploitable data. therefore, this sample might not be representative of the true population.\n",
      "what can you do with the data?\n",
      "look up whether kagglers has \"stronger\" hardware than non-kagglers\n",
      "whether there is a correlation between a preferred data set size and hardware\n",
      "is proficiency a predictor of specific preferences?\n",
      "are data scientists more intel or amd?\n",
      "how spread is gpu computing, and is there any relationship with kaggling?\n",
      "are you able to predict the amount of euros a data scientist might invest, provided their current workstation details?\n",
      "i did not find any past research on a similar scale. you are free to play with this data set. for re-usage of this data set out of kaggle, please contact the author directly on kaggle (use \"contact user\"). please mention:\n",
      "your intended usage (research? business use? blogging?...)\n",
      "your first/last name\n",
      "arbitrarily, we chose characteristics to describe data scientists and data set sizes.\n",
      "data set size:\n",
      "small: under 1 million values\n",
      "medium: between 1 million and 1 billion values\n",
      "large: over 1 billion values\n",
      "for the data, it uses the following fields (ds = data scientist, w = workstation):\n",
      "ds_1 = are you working with \"large\" data sets at work? (large = over 1 billion values) => yes or no\n",
      "ds_2 = do you enjoy working with large data sets? => yes or no\n",
      "ds_3 = would you rather have small, medium, or large data sets for work? => small, medium, or large\n",
      "ds_4 = do you have any presence at kaggle or any other data science platforms? => yes or no\n",
      "ds_5 = do you view yourself proficient at working in data science? => yes, a bit, or no\n",
      "w_1 = what is your cpu brand? => intel or amd\n",
      "w_2 = do you have access to a remote server to perform large workloads? => yes or no\n",
      "w_3 = how much euros would you invest in data science brand new hardware? => numeric output, rounded by 100s\n",
      "w_4 = how many cores do you have to work with data sets? => numeric output\n",
      "w_5 = how much ram (in gb) do you have to work with data sets? => numeric output\n",
      "w_6 = do you do gpu computing? => yes or no\n",
      "w_7 = what programming languages do you use for data science? => r or python (any other answer accepted)\n",
      "w_8 = what programming languages do you use for pure statistical analysis? => r or python (any other answer accepted)\n",
      "w_9 = what programming languages do you use for training models? => r or python (any other answer accepted)\n",
      "you should expect potential noise in the data set. it might not be \"free\" of internal contradictions, as with all researches.\n",
      "these data sets contain data on current driving licences issued by the driver and vehicle licensing agency (dvla). the dvla is responsible for issuing driving licences in great britain (gb). driving licences issued in northern ireland are the responsibility of the northern ireland driver & vehicle agency and are outside the scope of this release.\n",
      "dvla’s drivers database changes constantly as the agency receives driving licence applications and other information that updates the records of individual drivers. therefore, it is only possible only to provide a snapshot of the state of the record at a particular time.\n",
      "contact dvla for further information about driving licensing which can be found at: https://www.gov.uk/browse/driving/driving-licences\n",
      "this dataset includes the listing prices for the sale of properties (mostly houses) in ontario. they are obtained for a short period of time in july 2016 and include the following fields: - price in dollars - address of the property - latitude and longitude of the address obtained by using google geocoding service - area name of the property obtained by using google geocoding service\n",
      "this dataset will provide a good starting point for analyzing the inflated housing market in canada although it does not include time related information. initially, it is intended to draw an enhanced interactive heatmap of the house prices for different neighborhoods (areas)\n",
      "however, if there is enough interest, there will be more information added as newer versions to this dataset. some of those information will include more details on the property as well as time related information on the price (changes).\n",
      "this is a somehow related articles about the real estate prices in ontario: http://www.canadianbusiness.com/blogs-and-comment/check-out-this-heat-map-of-toronto-real-estate-prices/\n",
      "i am also inspired by this dataset which was provided for king county https://www.kaggle.com/harlfoxem/housesalesprediction\n",
      "this data set only includes the track sections that belong to the standard-gauge track network of the sbb group (sbb infrastructure, sensetalbahn, thurbo).track sections that are managed by other infrastructure operators, for example bls netz ag, deutsche bahn ag (db) or rhätische bahn ag (rhb), are not included.\n",
      "the data on the number of trains includes passenger and freight trains operated by all railway undertakings that have travelled on the track section in question, i.e. also trains run by bls ag, schweizerische südostbahn ag, crossrail ag and railcare ag, for example.\n",
      "https://data.sbb.ch/page/licence\n",
      "the breathing signal is the expansion and contraction of the chest measured using a chest belt. simultaneously, we obtained ventilation from the douglas bag (db) method which is the gold standard. given the breathing signal, we extract the average height (a) between adjacent local minima and maxima and the fundamental period (p). these are our features. we want to find a function f(x) that maps (a,b) into the flow of air calculated (based on the db method) in the time window. the average height seems to have a quadratic/cubic relationship with ventilation while the period seems to be having an inverse relationship with the flow of air. .\n",
      "water pump\n",
      "context: this csv contains the top 100 lap times on the famous nürburgring track in germany.\n",
      "content: - position on the list from 1-100 - model year of the car - car make - car model - lap time\n",
      "acknowledgements: sourced from https://nurburgringlaptimes.com/lap-times-top-100/\n",
      "inspiration: new to the data science world and saw found no data sets related to cars or racing so figured this might be a place to start. hoping to contribute larger, more interesting datasets to the community in the future.\n",
      "campus party bh mg brazil 2016 http://campuse.ro/challenges/hackathon-servicos-para-os-cidadaos/\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "i'm always concerned with the rise of bitcoin price. investing in these coins require data driven knowledge and without the knowledge one is bound to make grave mistakes. it is just the other day bitcoin price hit 1600 usd. what is the future of this coin?\n",
      "content\n",
      "the csv file contains daily closing price of a bitcoin from 28-apr-13 to 3-oct-17. soon i will give the updated data set.\n",
      "date column: the dates of observation\n",
      "close column: the daily closing price in usd of a bitcoin\n",
      "acknowledgements\n",
      "many thanks to block chain info and my friends.\n",
      "inspiration\n",
      "now that bitcoin has hit higher prices we would like to predict the direction of bitcoin price\n",
      "predict actual bitcoin price for a given day. will a neural network model work for my case?\n",
      "context\n",
      "i needed a data set for a hackathon project involving food classification. i gathered this data by scraping various online stores that only sold specific food items (ie only vegan food or only halal food). i then compared those items to walmart's electrobit-backed api that happened to return ingredient information.\n",
      "state wise tree cover can be used to predict area useful for agriculture ,find density of forest cover,number of tree approx. ,environment statistics\n",
      "context\n",
      "this dataset lists the natural rate of unemployment (nairu) in the u.s., which is the rate of unemployment arising from all sources except fluctuations in aggregate demand. estimates of potential gdp are based on the long-term natural rate. the short-term natural rate incorporates structural factors that are temporarily boosting the natural rate beginning in 2008. the short-term natural rate is used to gauge the amount of current and projected slack in labor markets, which is a key input into cbo's projections of inflation.\n",
      "content\n",
      "data includes the date of the quarterly collection and the natural rate of unemployment from january 1, 1949 through october 1, 2016.\n",
      "inspiration\n",
      "what is the general trend of unemployment?\n",
      "can you compare this unemployment data with other factors found in any of the bls databases, such as manufacturing employment rates and gdp?\n",
      "acknowledgement\n",
      "this dataset is part of the us department of labor bureau of labor statistics datasets (the federal reserve economic data database), and the original source can be found here.\n",
      "this is a subset of only asbestos-related maintenance requests for the borough of staten island received by progress queens from the new york city housing authority in response to a request filed under the state's freedom of information law.\n",
      "this subset was derived from the concatenation of the siebel extracts, which were included in the subject foil response. because of the poor condition of the data, the concatenation of the siebel extracts was processed with some possible data loss. a general description of the quality of the data nycha produced was reported in an article published by progress queens.\n",
      "the publisher of progress queens formed this dataset to study how does nycha treat maintenance requests for asbestos, to determine how nycha escalates complaints made by tenants about asbestos to ordering testing for asbestos and to abatement, if necessary.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "narendra modi is a great public speaker. this data set is an opportunity to understand what his speech's contain.\n",
      "content\n",
      "this is an unstructured text data of every month. starting from october 2014 to september 2017 each speech is provided as a text file.\n",
      "acknowledgements\n",
      "mann ki baat\n",
      "inspiration\n",
      "i would like to see what makes his speech's great and attract crores of people.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this data download from kaggle .it's about the fraud in a bank. use some method prevent fraud accident happen is very import for bank.\n",
      "content\n",
      "the fraud id is the positive and the total number is less than the negative sample. so ,please pay more attention to the positive recall and precise.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "how to improve recall in a unbalance is very common status in true business sense. down_sampling? or up_sampling?\n",
      "this is the car sales data set which include information about different cars . this data set is being taken from the analytixlabs for the purpose of prediction in this we have to see two things\n",
      "first we have see which feature has more impact on car sales and carry out result of this\n",
      "secondly we have to train the classifier and to predict car sales and check the accuracy of the prediction.\n",
      "this dataset does not have a description yet.\n",
      "description\n",
      "this is a data set for forecasting growth of investment after a period of some years say after 10 years if data growth pattern remains same.\n",
      "data fields\n",
      "deposit date - gives the date of investment. month - counter on total months pft_perc - percentage of profit earned pft - calculated total profit on investment investment - total investment for month\n",
      "what we already know!\n",
      "their are some deposits made to grow the investment and based on which pft earned is growing.\n",
      "what we need to know.\n",
      "idea here is to know the growth of investment after some years. for instance after 40 months, which will be my investment ?\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this breast cancer databases was obtained from the university of wisconsin hospitals, madison from dr. william h. wolberg.\n",
      "content\n",
      "past usage:\n",
      "attributes 2 through 10 have been used to represent instances. each instance has one of 2 possible classes: benign or malignant.\n",
      "wolberg,~w.~h., \\& mangasarian,~o.~l. (1990). multisurface method of pattern separation for medical diagnosis applied to breast cytology. in {\\it proceedings of the national academy of sciences}, {\\it 87}, 9193--9196. -- size of data set: only 369 instances (at that point in time) -- collected classification results: 1 trial only -- two pairs of parallel hyperplanes were found to be consistent with 50% of the data -- accuracy on remaining 50% of dataset: 93.5% -- three pairs of parallel hyperplanes were found to be consistent with 67% of data -- accuracy on remaining 33% of dataset: 95.9%\n",
      "zhang,~j. (1992). selecting typical instances in instance-based learning. in {\\it proceedings of the ninth international machine learning conference} (pp. 470--479). aberdeen, scotland: morgan kaufmann. -- size of data set: only 369 instances (at that point in time) -- applied 4 instance-based learning algorithms -- collected classification results averaged over 10 trials -- best accuracy result: -- 1-nearest neighbor: 93.7% -- trained on 200 instances, tested on the other 169 -- also of interest: -- using only typical instances: 92.2% (storing only 23.1 instances) -- trained on 200 instances, tested on the other 169\n",
      "relevant information:\n",
      "samples arrive periodically as dr. wolberg reports his clinical cases. the database therefore reflects this chronological grouping of the data. this grouping information appears immediately below, having been removed from the data itself:\n",
      "group 1: 367 instances (january 1989) group 2: 70 instances (october 1989) group 3: 31 instances (february 1990) group 4: 17 instances (april 1990) group 5: 48 instances (august 1990) group 6: 49 instances (updated january 1991) group 7: 31 instances (june 1991) group 8: 86 instances (november 1991)\n",
      "total: 699 points (as of the donated datbase on 15 july 1992)\n",
      "note that the results summarized above in past usage refer to a dataset of size 369, while group 1 has only 367 instances. this is because it originally contained 369 instances; 2 were removed. the following statements summarizes changes to the original group 1's set of data:\n",
      "group 1 : 367 points: 200b 167m (january 1989) revised jan 10, 1991: replaced zero bare nuclei in 1080185 & 1187805 revised nov 22,1991: removed 765878,4,5,9,7,10,10,10,3,8,1 no record : removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial : changed 0 to 1 in field 6 of sample 1219406 : changed 0 to 1 in field 8 of following sample: : 1182404,2,3,1,1,1,2,0,1,1,1\n",
      "number of instances: 699 (as of 15 july 1992)\n",
      "number of attributes: 10 plus the class attribute\n",
      "attribute information: (class attribute has been moved to last column)\n",
      "attribute domain\n",
      "sample code number id number\n",
      "clump thickness 1 - 10\n",
      "uniformity of cell size 1 - 10\n",
      "uniformity of cell shape 1 - 10\n",
      "marginal adhesion 1 - 10\n",
      "single epithelial cell size 1 - 10\n",
      "bare nuclei 1 - 10\n",
      "bland chromatin 1 - 10\n",
      "normal nucleoli 1 - 10\n",
      "mitoses 1 - 10\n",
      "class: (2 for benign, 4 for malignant)\n",
      "missing attribute values: 16\n",
      "there are 16 instances in groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by \"?\".\n",
      "class distribution:\n",
      "benign: 458 (65.5%) malignant: 241 (34.5%)\n",
      "acknowledgements\n",
      "o. l. mangasarian and w. h. wolberg: \"cancer diagnosis via linear programming\", siam news, volume 23, number 5, september 1990, pp 1 & 18.\n",
      "william h. wolberg and o.l. mangasarian: \"multisurface method of pattern separation for medical diagnosis applied to breast cytology\", proceedings of the national academy of sciences, u.s.a., volume 87, december 1990, pp 9193-9196.\n",
      "o. l. mangasarian, r. setiono, and w.h. wolberg: \"pattern recognition via linear programming: theory and application to medical diagnosis\", in: \"large-scale numerical optimization\", thomas f. coleman and yuying li, editors, siam publications, philadelphia 1990, pp 22-30.\n",
      "k. p. bennett & o. l. mangasarian: \"robust linear programming discrimination of two linearly inseparable sets\", optimization methods and software 1, 1992, 23-34 (gordon & breach science publishers).\n",
      "inspiration\n",
      "rouse tek bio informatics cytogenomics project is an attempt to bring the human genome to the understanding of how cancers develop. all of our bodies are composed of cells. the human body has about 100 trillion cells within it. and usually those cells behave in a certain fashion. they observe certain rules, they divide when they’re told to divide, they’re quiescent when they’re told to remain dormant, they stay within a particular position within their tissue and they don’t move out of that.\n",
      "occassionally however, a single cell, of those 100 trillion cells, behave in a different way. that cell keeps dividing when all its signals around it tell it to stop dividing. that cell ignores its counterparts around it and pushes them out of the way. that cell stops observing the rules of the tissue within which it is located and begins to move out of its normal position, invading into the tissues around it and sometimes entering the bloodstream and becoming a metastasis, depositing in another tissue of the body..\n",
      "the reason the cell has gone rogue is because it has acquired within its genome, within its dna, a number of abnormalities that cause it to behave as a cancer cell.\n",
      "all 100 trillion cells in the human body have got a copy of the human genome, they have 2 copies, 1 maternal, 1 paternal. throughout life all those copies of the genome in those 100 trillion cells, are acquiring abnormal changes or somatic mutations. these mutations are present in the cell and are not transmitted from parents to offspring. they are constrained to that individual cell. those mutations occur in every cell of the body, normal and abnormal, for a number of different reasons. they occur because every time a cell divides possibly one letter of code out of 3 billion is replicated incorrectly. and that’s 1 source of somatic mutations.\n",
      "another source is that our 100 trillion cells are being exposed to a number of different onslaughts like radiation, self generated chemicals from inhalation of things like tobacco smoke or even an unhealthy diet over time. occasionally mechanisms in a particular cell make breakdown and the dna of that cell begins to acquire somatic mutations rather more commonly than other cells.\n",
      "so in summary, every cell in the body acquires mutations throughout a lifetime, and as we get older we acquire more and more somatic mutations in which occasionally a particular type of gene is mutated where the protein that it makes is abnormal and drives the cell to behave in a rogue fashion that we call cancer.\n",
      "context\n",
      "the subject of this dataset is multi-instrument observations of solar flares. there are a number of space-based instruments that are able to observe solar flares on the sun; some instruments observe the entire sun all the time, and some only observe part of the sun some of the time. we know roughly where flares occur on the sun but we don't know when they will occur. in this respect solar flares resemble earthquakes on earth. this dataset is a catalog of which solar flares have been observed by currently operational space-based solar observatories.\n",
      "content\n",
      "it includes that start time and end time of each solar flare from 1 may 2010 to 9 october 2017 and which instrument(s) they were observed by. it was collected by doing a retrospective analysis of the known pointing of seven different instruments with the location and times of 12,455 solar flares.\n",
      "acknowledgements\n",
      "the dataset was compiled by dr. ryan milligan based on publicly available data and are freely distributed. the citation of relevance is https://arxiv.org/abs/1703.04412.\n",
      "inspiration\n",
      "this dataset represents the first attempted evaluation of how well space-based instrumentation co-ordinate when it comes to observing solar flares. we are particularly interested in understanding how often combinations of instruments co-observe the same flare. the ultimate purpose is to try to find strategies that optimize the scientific return on solar flare data given the limited space-based instrument resources available. more often than not, our greatest understanding of these explosive events come through simultaneous observations made by multiple instruments.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "the top \"how to\" related searches on google from 2004 to 2017 worldwide. top searches are searches with the highest search interest based on volume.\n",
      "indexed search interest in 'health care' from may 2 to may 4, 2017. 100 is the max value, and every number is relative to that.\n",
      "rankings for halloween costumes in october 2017 in the us\n",
      "context\n",
      "expressionist art works is a very excellent knowledge repository to gather insights about the human aesthetics and perceptions.\n",
      "content\n",
      "this data set is the collection of expressionist art works across the world for the machine learning experiments.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "after participating in many kaggle competitions, we decided to open-source our python automl framework alphapy. you can configure models using yaml, and it's a quick way to develop a classification or regression model with any number of scikit-learn algorithms. more importantly, we needed the ability to easily write custom transformations to engineer features that could be imported by other tools such as h2o.ai and datarobot, which are commercial automl tools.\n",
      "we were very interested in predicting the stock market and sporting events, so we developed some tools for collecting end-of-day stock market data and then transforming that time series data for machine learning. for the sports data, we painstakingly entered game results on a daily basis, and this dataset (albeit limited) is the result. sports apis such as sportradar are becoming more prevalent, but you still need a pipeline to merge all that data to shape it for machine learning.\n",
      "content\n",
      "we collected game results, line, and over/under data for the following sports and seasons:\n",
      "nfl 2014, 2015 (2 seasons)\n",
      "nba 2015-16 (1 season)\n",
      "ncaa football 2015-16 (1 season)\n",
      "ncaa basketball 2015-16 (1 season)\n",
      "using alphapy, we developed a sports pipeline to analyze trend data in the game results. you can find documentation and examples here.\n",
      "inspiration\n",
      "every speculator's dream is to gain an edge, whether it be betting on sports or speculating on stocks. this dataset is just a small step for applying machine learning to the world of sports data.\n",
      "context\n",
      "most of the small to medium business owners are making effective use of gmail based email marketing strategies for offline targeting of converting their prospective customers into leads so that they stay with them in business\n",
      "content\n",
      "we have different aspects of emails to characterize the mail and track the mail is ignored; read; acknowledged by the reader\n",
      "acknowledgements\n",
      "corefactors.in\n",
      "inspiration\n",
      "amount of advertising dollars spent on a product determines the amount of its sales, we could use regression analysis to quantify the precise nature of the relationship between advertising and sales. here we want everyone to experiment with this fun data , what value we can derive from email as a tool for compaign marketing in a multi channel marketing strategy of a small to medium businesses\n",
      "content\n",
      "this data was acquired by scraping truecar.com for used car listings on 9/24/2017. each row represents one used car listing.\n",
      "context\n",
      "penalty kicks in football are the easiest, and perhaps most elegant way of modelling a game theory situation in a real-world scenario. given the limited number of options for both kickers and keepers, it makes for wonderful real-life data, which can be used in a professional context. however, official match records do not record the interesting aspects of the play - the direction of the kicker, the direction the keeper moves, where the ball lands, and so on. i watched all the penalties of the 2016/17 season of the epl (thanks, youtube!) and tagged the direction each player moves in. i believe this dataset will be extremely valuable to those who wish to experiment at the intersection of sports, game theory and data science.\n",
      "content\n",
      "the dataset contains information of all 106 penalty kicks taken during the 2016/17 season of the english premier league, with the following details - teams involved, player who took the kick, his foot, the direction the ball went, and the direction the keeper dove in [important: direction is with reference to the kicker!], and what time the penalty was awarded. the saved column indicates whether the keeper saved it, or the kicker kicked it beyond the goal post.\n",
      "there is missing data for 3 kicks - for one, the penalty was nullified due to a double kick, and for the other two, i simply couldn't find any video evidence of them. (if you do find them, please let me know). also, please let me know if you find any other errors with the data.\n",
      "acknowledgements\n",
      "this entire project was inspired by the brilliant work of ignacio palacios huerta, whose story is wonderful, and whose papers are an absolute joy to read. my current project is basically emulating what huerta has done with his (very vast) dataset.\n",
      "i will be expanding this dataset to previous seasons penalties too, as and when i get time.\n",
      "context\n",
      "as i am trying to learn and build an lstm prediction model for equity prices, i have chosen gold price to begin.\n",
      "content\n",
      "the file composed of simply 2 columns. one is the date (weekend) and the other is gold close price. the period is from 2015-01-04 to 2017-09-24.\n",
      "acknowledgements\n",
      "thanks to jason of his tutorial about lstm forecast: https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
      "inspiration\n",
      "william gann: time is the most important factor in determining market movements and by studying past price records you will be able to prove to yourself history does repeat and by knowing the past you can tell the future. there is a definite relation between price and time.\n",
      "context\n",
      "before joining the federal executive administration, new government appointees must submit, amongst other things, detailed information regarding their finances and previous job history. such disclosure rules are in place in order to prevent conflicts of interest, and are a fundamental part of the work done by government ethics commissions. this dataset is a condensed collection of information recovered from these forms for a selection of top trump administration appointees.\n",
      "content\n",
      "this dataset is split into five separate csv files:\n",
      "names_and_job_titles.csv -- the names and job titles of appointees. not all appointees are included in this dataset.\n",
      "jobs_before_joining_admin.csv -- positions held by administration members immediately prior to their joining the federal government.\n",
      "clients_before_joining_admin.csv -- former appointee clients before joining the federal government.\n",
      "income_sources_and_assets.csv -- all acknowledged and disclosed income sources and assets. the most important file.\n",
      "debts.csv -- known appointee debt obligations. this record is incomplete.\n",
      "employee_agreements.csv -- agreements that the appointee made as part of the conditions of their entering employment with the federal government.\n",
      "acknowledgements\n",
      "propublica, the new york times, the associated press, and others pooled their resources to collect and condense disclosure forms for many prominent members of the trump administration. these were in turn collected into a public spreadsheet. this dataset is a further condensation of this work.\n",
      "inspiration\n",
      "what can you discover about the finances and potential conflicts of interest of members of the trump administration by looking at the raw government record?\n",
      "context\n",
      "in the early 2000s, billy beane and paul depodesta worked for the oakland athletics. while there, they literally changed the game of baseball. they didn't do it using a bat or glove, and they certainly didn't do it by throwing money at the issue; in fact, money was the issue. they didn't have enough of it, but they were still expected to keep up with teams that had much deeper pockets. this is where statistics came riding down the hillside on a white horse to save the day. this data set contains some of the information that was available to beane and depodesta in the early 2000s, and it can be used to better understand their methods.\n",
      "content\n",
      "this data set contains a set of variables that beane and depodesta focused heavily on. they determined that stats like on-base percentage (obp) and slugging percentage (slg) were very important when it came to scoring runs, however they were largely undervalued by most scouts at the time. this translated to a gold mine for beane and depodesta. since these players weren't being looked at by other teams, they could recruit these players on a small budget. the variables are as follows:\n",
      "team\n",
      "league\n",
      "year\n",
      "runs scored (rs)\n",
      "runs allowed (ra)\n",
      "wins (w)\n",
      "on-base percentage (obp)\n",
      "slugging percentage (slg)\n",
      "batting average (ba)\n",
      "playoffs (binary)\n",
      "rankseason\n",
      "rankplayoffs\n",
      "games played (g)\n",
      "opponent on-base percentage (oobp)\n",
      "opponent slugging percentage (oslg)\n",
      "acknowledgements\n",
      "this data set is referenced in the analytics edge course on edx during the lecture regarding the story of moneyball. the data itself is gathered from baseball-reference.com. sports-reference.com is one of the most comprehensive sports statistics resource available, and i highly recommend checking it out.\n",
      "inspiration\n",
      "it is such an important skill in today's world to be able to see the \"truth\" in a data set. that is what depodesta was able to do with this data, and it unsettled the entire system of baseball recruitment. beane and depodesta defined their season goal as making it to playoffs. with that in mind, consider these questions:\n",
      "how does a team make the playoffs?\n",
      "how does a team win more games?\n",
      "how does a team score more runs?\n",
      "they are all simple questions with simple answers, but now it is time to use the data to find the \"truth\" hidden in the numbers.\n",
      "context\n",
      "human activity recognition - har - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community (see picture below, that illustrates the increasing number of publications in har with wearable accelerometers), especially for the development of context-aware systems. there are many potential applications for har, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.\n",
      "content\n",
      "this human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict \"which\" activity was performed at a specific point in time (like with the daily living activities dataset above). the approach we propose for the weight lifting exercises dataset is to investigate \"how (well)\" an activity was performed by the wearer. the \"how (well)\" investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training.\n",
      "ix young health participants were asked to perform one set of 10 repetitions of the unilateral dumbbell biceps curl in five different fashions: exactly according to the specification (class a), throwing the elbows to the front (class b), lifting the dumbbell only halfway (class c), lowering the dumbbell only halfway (class d) and throwing the hips to the front (class e).\n",
      "class a corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. the exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. we made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).\n",
      "acknowledgements\n",
      "this dataset is licensed under the creative commons license (cc by-sa) - wallace ugulino (wugulino at inf dot puc-rio dot br) - eduardo velloso - hugo fuks\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "engineers use numerical models to analyze the behavior of the systems they are studying. by means of numerical models you can prove whether a design is safe or not, instead of making a prototype and testing it. this gives you great flexibility to modify parameters and to find a cheaper design that satisfies all the safety requirements.\n",
      "but when the models are too complex, the numerical simulations can easily last from a few hours to a few days. in addition, during the optimization process you might need a few tens of trials. so in order to simplify the process we can build a simple 'surrogate' model that yields similar results to the original one. here's when machine learning comes in!\n",
      "content\n",
      "the dataset contains the data of about 6000 numerical simulations (finite element models, fem). it must be pointed out that there is no noise in the data, that is, if we run again the simulations we'd get the same results. there are 9 input parameters and 4 output results.\n",
      "inputs (continuous and positive values): (1) load parameters: ecc, n, gammag. (2) material parameters: esoil, econc. (3) geometry parameters: dbot, h1, h2, h3.\n",
      "outputs (continuous values): (1) stress related results: mr_t, mt_t, mr_c, mt_c.\n",
      "acknowledgements\n",
      "the parametric numerical model was self-made.\n",
      "inspiration\n",
      "the data comes from deterministic numerical simulations. under this circumstance, is there any way we can find a regression model that gives accurate results? let's say something like 5% error (true_value / predicted_value within the range of [0.95, 1.05]).\n",
      "what would be the most appropriate regression algorithms? what accuracy can we expect?\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "20 years of daily stock prices of all companies listed on asx (australian securities exchange).\n",
      "date range: 1997-01-02 to 2017-12-31\n",
      "exchange website: https://www.asx.com.au/\n",
      "content\n",
      "total records: 6,475,470\n",
      "total companies: 2,228\n",
      "asx-equity-price.csv fields : ticker, date, open, high, low, close, volume\n",
      "asx-tickers.csv fields: ticker, company, industry\n",
      "acknowledgements\n",
      "the data posted here is merely a concatenation of all files on the below website.\n",
      "all credits to: https://www.asxhistoricaldata.com/\n",
      "inspiration\n",
      "this has been added to help correlate stock prices with my australian news headlines dataset.\n",
      "telecom giant telstra and the mining gaint bhp billiton are highly referenced (1000+ headlines) in the news and would be interesting case studies.\n",
      "context\n",
      "i like to livetweet conferences when i attend them, for my own reference later on and to help other people who aren't attending the conference keep up-to-date. after the last conference i attended, i was curious: do livetweets get more or less engagement than other types of tweets?\n",
      "content\n",
      "this dataset contains information on 314 tweets sent from my personal twitter account between september 29, 2017 and october 26, 2017. for each tweet, the following information is recorded:\n",
      "at_conference?: whether the tweet was sent during the conference\n",
      "day: the day the tweet was sent\n",
      "impressions: how many times the tweet was seen\n",
      "engagements: how many times the tweet was engaged with (sum of the following columns)\n",
      "retweets: how many times the tweet was retweeted\n",
      "likes: how many times the tweet was liked\n",
      "user profile clicks: how many times someone clicked on my profile from the tweet\n",
      "url clicks: how many times someone clicked a url in the tweet (not all tweets have url's)\n",
      "hashtag clicks: how many times someone clicked on a hashtag in the tweet (not all tweets have hashtags)\n",
      "detail expands: how many times someone expanded the tweet\n",
      "follows: how many times someone followed me from the tweet\n",
      "media views: how many times someone viewed media embedded in the tweet (not all tweets have media)\n",
      "media engagements: how many times someone clicked on media embedded in the tweet (not all tweets have media)\n",
      "inspiration\n",
      "do conference tweets get more engagement?\n",
      "does my account get more engagement during a conference?\n",
      "do the types of engagement differ depending on whether i'm at a conference?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "in the modern-world scenario of telecommunication industries, the customers have a range of options to choose from, as far as service providers are concerned. factors such as perceived frequent service disruptions, poor customer service experiences, and better offers from other competing carriers may cause a customer to churn (likely to leave).\n",
      "customer churn includes customers stopping the use of a service, switching to a competitor service, switching to a lower-tier experience in the service or reducing engagement with the service.\n",
      "content\n",
      "necessary shapefiles to create maps of new york city and its boroughs.\n",
      "acknowledgements\n",
      "these files have been made available by the new york city department of city planning and were retrieved from http://www1.nyc.gov/site/planning/data-maps/open-data/districts-download-metadata.page on 27 september, 2017.\n",
      "inspiration\n",
      "these shapefiles might pair nicely with the new york building and elevator data also on here, as well as the nyc tree census i use it for.\n",
      "context\n",
      "i studied on this data set to predict phishing web sites by using:\n",
      "1- just url string 2- the content broadcasts from the url\n",
      "content\n",
      "this data set just has url list.\n",
      "acknowledgements\n",
      "data gathered from openphish.com.\n",
      "inspiration\n",
      "does a url itself or content of the url show us whether it is phishing or not?\n",
      "context\n",
      "data of hitters in mlb's al east\n",
      "content\n",
      "basic fundamental data on al east hitters sorted descending by plate appearances\n",
      "acknowledgements\n",
      "inspiration\n",
      "i love baseball and stats.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "the official archive data set from the dhhs on all reported phi data breaches from medical and dental centers (military and civilian).\n",
      "content\n",
      "the name of the practice, the amount of people effected and the breach description.\n",
      "acknowledgements\n",
      "i did not create this data set, it is from the archives of the department of health and human services.\n",
      "inspiration\n",
      "this can be used to cross reference with osha violations and to determine the most common breach types\n",
      "context\n",
      "the need for music-speech classification is evident in many audio processing tasks which relate to real-life materials such as archives of field recordings, broadcasts and any other contexts which are likely to involve speech and music, concurrent or alternating. segregating the signal into speech and music segments is an obvious first step before applying speech-specific or music-specific algorithms. indeed, speech-music classification has received considerable attention from the research community (for a partial list, see references below) but many of the published algorithms are dataset-specific and are not directly comparable due to non-standardised evaluation.\n",
      "content\n",
      "dataset collected for the purposes of music/speech discrimination. the dataset consists of 120 tracks, each 30 seconds long. each class (music/speech) has 60 examples. the tracks are all 22050hz mono 16-bit audio files in .wav format.\n",
      "this dataset does not have a description yet.\n",
      "content\n",
      "this dataset contains information about players and teams and their statistics from 2005-2013. it also includes every play of every drive for every game played between 2005-2013.\n",
      "acknowledgements\n",
      "thanks to cbfstats.com and j. albert bowden ii for the dataset.\n",
      "context\n",
      "includes data for all candidates from all units of the federation and yours list of property declarations.\n",
      "content\n",
      "data of candidates brazilian national elections of 2014. source: http://www.tse.jus.br/eleitor-e-eleicoes/estatisticas/repositorio-de-dados-eleitorais-1/repositorio-de-dados-eleitorais\n",
      "context\n",
      "i love football and wanted to gather a data-set of a list of football players along with their each game performance from various different sources.\n",
      "content\n",
      "the csv file has the fantasy premier league data of all players who played in 3 seasons and a detailed spreadsheet of each player is provided.\n",
      "acknowledgements\n",
      "thanks to turd from tableau for some of the data.\n",
      "inspiration\n",
      "we all wondered if it is possible to predict the future! well with the player data against each team and conditions we get to check if the future prediction is truly possible!\n",
      "we generated our own dataset (iitm-hetra) from cameras monitoring road traffic in chennai, india. to ensure that data are temporally uncorrelated, we sample a frame every two seconds from multiple video streams. we extracted 2400 frames in total.\n",
      "we manually labeled 2400 frames under different vehicle categories. the number of available frames reduced to 1417 after careful scrutiny and elimination of unclear images. we initially defined eight different vehicle classes commonly seen in indian traffic. few of these classes were similar while two classes had less number of labeled instances; these were merged into similar looking classes. for example, in our dataset, we had different categories for small car, suv, and sedan which were merged under the light motor vehicle (lmv) category.\n",
      "a total of 6319 labeled vehicles are available in the collected dataset. this includes 3294 two-wheelers, 279 heavy motor vehicles (hmv), 2148 cars, and 598 auto-rickshaws. a second dataset was created by merging cars and auto-rickshaws together into light motor vehicle (lmv) class. approximately 25.2\\% of vehicles were occluded.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "interactive hand gesture. here are color images and as well depth images of hand gestures grouped by their classes.\n",
      "copyright: author: chengyin liu; email: destin369y at gmail.com; year: 2015.\n",
      "please acknowledge my name if you use this dataset, thank you.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "rgb-d hand gesture images taken by depth camera. grouped by classes. please refer to \"class.txt\" used for hand gesture recognition evaluation.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "this dataset is used for my hand gesture recognition research at national taiwan university, in the intelligent robot lab under the lead of prof. li-chen fu. more details about our lab, please visit http://robotlab.csie.ntu.edu.tw/\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "automated composition of computer programs has been a standing challenge since the early days of artificial intelligence, and has no clear solutions with modern-day research in deep learning. indeed, modeling latent context representations in language proves to be a difficult task singularly, and combined with applying structured procedural knowledge in a generative fashion quickly becomes intractable for complex domain specific languages. there is a clear need for research in devising efficiently learned combinations of these independent problems. we present this basic dataset of elementary mathematical functions encoded in the python programming language to encourage future research in this field, and to benchmark our own deep learning\n",
      "content\n",
      "this dataset contains a total of 335922 elementary mathematical functions with examples for inputs with corresponding outputs. the first line in the dataset csv file contains a header describing the contents of each column. the first column is labeled function_name, and the following rows contains a function name with a unique integer index. the next twenty columns contain function_input_x and function_output_x for all integers between 0 and 9 inclusively, where the following rows contains string encoded python floating point numbers after executing the corresponding function. the final column is labeled function_code, and contains a single-line lambda statement elementary mathematical function.\n",
      "acknowledgements\n",
      "we thank the guidance from the student instructors of the machine learning @ berkeley research group, and the discussions had with members of the redwood institute of theoretical neuroscience.\n",
      "inspiration\n",
      "we hope to see a neural architecture that is capable of programming new computer code without human supervision, in any programming language, solving even scientific computing tasks.\n",
      "mit license\n",
      "copyright (c) 2017 brandon trabucco\n",
      "permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"software\"), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions:\n",
      "the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software.\n",
      "the software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.\n",
      "my research is taking data from high schooler's 2 mile times and converting it using my formula against other running formulas. i'm trying to find the fatigue decrease by using their time.\n",
      "the actual times are the 1 mile and 2 mile times, the age is the age grading formula, the vo2 max is the formula used for hypothetical running tmes, and the cameron and riegel formulas are for prediction times. taking the average of them all, i incorporated my formula and compared how close my prediction was to the average of all the formulas.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset is a experimentation of how to detect number of pill inside a pill bottle, through audio alone. there are plenty of games that exist that try to guess the number of items inside a container. its usually a game for fun, and just a game of chance. but can we game the system?\n",
      "for this experiment, we decided to use child resistant medicine pill bottles. and filled these bottles with generic acetaminophen gel caps https://www.costco.com/.product.100017017.html\n",
      "the following data contains 10, 10 seconds audio recordings of a generic pill bottle being shaken by one individual with a different number of pills. in our example, we recorded a user shaking a pill bottle with 1 pill, 10 pills, 25 pills and 50 pills. each folder contains 10, 10 second samples of each\n",
      "i work with uk company information on a daily basis, and i thought it would be useful to publish a list of all active companies, in a way that could be used for machine learning.\n",
      "there are 3,838,469 rows in the dataset, one for each active company. each row, has the company name, date of incorporation and the standard industrial classification code.\n",
      "the company list is from the publicly available 1st november 2017 companies house snapshot.\n",
      "the sic code descriptions are from the gov.uk website.\n",
      "in the file allcompanies.csv each row is formatted as follows:\n",
      "companyname - alpha numberic company name\n",
      "incorporationdate - in british date format, dd/mm/yyyy\n",
      "sic - 5 digits or if not known, none - see separate file for description of each code.\n",
      "inspiration\n",
      "possible uses for this data is to use ml to suggest a new unique but suitable name for a company based on what other companies of the same sic are called.\n",
      "perhaps analyse how company names have evolved over time.\n",
      "using ml, perhaps determine what a typical company name looks like, maybe analyse if company names have got longer or more complicated over time.\n",
      "i am sure there are many more possible uses for this data in ways, that i cannot imagine.\n",
      "this is my second go (the first was published a few hours ago) at publishing a dataset on any medium, so any useful tips and hints would be extremely welcome.\n",
      "links to the raw data sources are here:\n",
      "companies house http://download.companieshouse.gov.uk/en_output.html\n",
      "sic codes https://www.gov.uk/government/publications/standard-industrial-classification-of-economic-activities-sic\n",
      "context\n",
      "created for use in the renewable and appropriate energy lab at uc berkeley and lawrence berkeley national laboratory.\n",
      "content\n",
      "geography: all 58 counties of the american state of california\n",
      "time period: 2015\n",
      "unit of analysis: tons per year\n",
      "variables:\n",
      "co: county id as numbered in the county dropdown menu on the california air resources board facility search tool\n",
      "ab\n",
      "facid\n",
      "dis\n",
      "fname\n",
      "fstreet\n",
      "fcity\n",
      "fzip\n",
      "fsic: facility standard industrial classification code specified by the us department of labor\n",
      "coid\n",
      "disn\n",
      "chapis\n",
      "cerr_code\n",
      "togt: total organic gases consist of all hydrocarbons, i.e. compounds containing hydrogen and carbon with or without other chemical elements.\n",
      "rogt: reactive organic gases include all the organic gases exclude methane, ethane, acetone, methyl acetate, methylated siloxanes, and number of low molecular weight halogenated organics that have a low rate of reactivity.\n",
      "cot: the emissions of co are for the single species, carbon monoxide.\n",
      "noxt: the emissions of nox gases (mostly nitric oxide and nitrogen dioxide) are reported as equivalent amounts of no2.\n",
      "soxt: the emissions of sox gases (sulfur dioxide and sulfur trioxide) are reported as equivalent amounts of so2.\n",
      "pmt: particulate matter refers to small solid and liquid particles such as dust, sand, salt spray, metallic and mineral particles, pollen, smoke, mist and acid fumes.\n",
      "pm10t: pm10 refers to the fraction of particulate matter with an aerodynamic diameter of 10 micrometer and smaller. these particles are small enough to penetrate the lower respiratory tract.\n",
      "pm2.5t: pm2.5 refers to the fraction of particulate matter with an aerodynamic diameter of 2.5 micrometer and smaller. these particles are small enough to penetrate the lower respiratory tract.\n",
      "lat: facility latitude geocoded by inputting fstreet, fcity, california fzip into bing’s geocoding service.\n",
      "lon: facility longitude geocoded in the same way.\n",
      "sources: all columns except for lat and lon were scraped from the california air resources board facility search tool using the request module from python’s urllib library. the script used is included below in scripts in case you would like to get additional columns.\n",
      "the lat and lon columns were geocoded using the geocoder library for python with the bing provider.\n",
      "scripts\n",
      "download.py\n",
      "import pandas as pd out_dir = 'arb/' file_ext = '.csv' for i in range(1, 59): facilities = pd.read_csv(\"https://www.arb.ca.gov/app/emsinv/facinfo/faccrit_output.csv?&dbyr=2015&ab_=&dis_=&co_=\" + str(i) + \"&fname_=&city_=&sort=facilitynamea&fzip_=&fsic_=&facid_=&all_fac=c&chapis_only=&cerr=&dd=\") for index, row in facilities.iterrows(): curr_facility = pd.read_csv(\"https://www.arb.ca.gov/app/emsinv/facinfo/facdet_output.csv?&dbyr=2015&ab_=\" + str(row['ab']) + \"&dis_=\" + str(row['dis']) + \"&co_=\" + str(row['co']) + \"&fname_=&city_=&sort=c&fzip_=&fsic_=&facid_=\" + str(row['facid']) + \"&all_fac=&chapis_only=&cerr=&dd=\") facilities.set_value(index, 'pm2.5t', curr_facility.loc[curr_facility['pollutant name'] == 'pm2.5'].iloc[0]['emissions_tons_yr']) facilities.to_csv(out_dir + str(i) + file_ext)\n",
      "geocode.py\n",
      "import geocoder import csv directory = 'arb/' outdirectory = 'arb_out/' for i in range(1, 59): with open(directory + str(i) + \".csv\", 'rb') as csvfile, open(outdirectory + str(i) + '.csv', 'a') as csvout: reader = csv.dictreader(csvfile) fieldnames = reader.fieldnames + ['lat'] + ['lon'] # add new columns writer = csv.dictwriter(csvout, fieldnames) writer.writeheader() for row in reader: address = row['fstreet'] + ', ' + row['fcity'] + ', california ' + row['fzip'] g = geocoder.bing(address, key='api_key') newrow = dict(row) if g.latlng: newrow['lat'] = g.json['lat'] newrow['lon'] = g.json['lng'] writer.writerow(newrow) # only write row if successfully geocoded\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "uk police forces collect data on every vehicle collision in the uk on a form called stats19. data from this form ends up at the dft and is published at https://data.gov.uk/dataset/road-accidents-safety-data\n",
      "content\n",
      "there are 4 csvs and an excel file in this set. accidents is the primary table and has references by accident_index to the other tables.\n",
      "acknowledgements\n",
      "department for transport and the uk's wonderful open gov initiative https://data.gov.uk/\n",
      "inspiration\n",
      "are there patterns for accidents involving different road users?\n",
      "can we predict the safest / most dangerous times to travel\n",
      "can this data help route cyclists around accident hotspots taking into account the time of day, weather, route etc\n",
      "are certain cars more accident prone than others?\n",
      "context\n",
      "train commuter service in stockholm in 2012.\n",
      "content\n",
      "description of the train commuter service during one week-day. the data includes information about: - train timetable. - passengers flow\n",
      "acknowledgements\n",
      "the data is generated with the help of samtrafiken - trafiklab api (see. www.trafiklab.se).\n",
      "inspiration\n",
      "crowdedness: how crowd the trains are? which lines are crowded? how frequent should the train run?\n",
      "context\n",
      "japanese animation, which is known as anime, has become internationally widespread nowadays. this dataset provides data on anime taken from anime news network.\n",
      "content\n",
      "this dataset consists of 4029 anime data in 5 files. all of the csv files use '|' delimiter.\n",
      "- anime title (datatitle-all-share-new.csv)\n",
      "- anime synopsis (datasynopsis-all-share-new.csv)\n",
      "- anime genre (datagenre-all-share-new.csv)\n",
      "- anime staff (datastaff-all-share-new.csv)\n",
      "- anime scores (datascorehist-all-share-new.csv)\n",
      "anime id and staff were taken as what they seen on anime news network system. while the scores are taken based on the histogram of scores on each anime page and normalized.\n",
      "acknowledgements\n",
      "the dataset was collected from http://www.animenewsnetwork.com on 10 may 2016. if you use this dataset in publications, please cite:\n",
      "wibowo, c. p. (2016). a minimum spanning tree representation of anime similarities. arxiv preprint arxiv:1606.03048\n",
      "inspiration\n",
      "this dataset can be used to build recommendation systems, predict a score, visualize anime similarity, etc.\n",
      "context\n",
      "this dataset mainly features the score changes during badminton games in the rally-point system.\n",
      "content\n",
      "the dataset contains 11872 games from 5131 matches in bwf super series tournaments. there are 6 fields:\n",
      "- year: i collected 3 years data: 2015-2017.\n",
      "- tournament: for each year, there are 12 super series tournaments.\n",
      "- round: 1 - round 1; 2 - round 2; q - quarter finals; s - semi-finals; f - finals\n",
      "- match: information about the countries of the players.\n",
      "- type: ms - men's single; ws - women's single; md - men's double; wd - women's double; xd - mixed double\n",
      "- scores: score changes during the games.\n",
      "acknowledgements\n",
      "the dataset was collected from bwfbadminton.com. i wrote codes to scrap the information.\n",
      "inspiration\n",
      "performance of the players is reflected on how the score changes during the games. exploring this information may help us to predict or learn something related to badminton games.\n",
      "context\n",
      "walking around a total wine one day i wondered if there was any data i could find that would help me figure out what new rums to try, i later was able to find some information on rumratings.com. i'm now the hit of every party thanks to this data and a few box plots.\n",
      "content\n",
      "the data was scraped from rumratings, unfortunately a good portion of the data is sparse, i go into more detail about that in the kernel. there is slightly more information that could be grabbed but i didn't want to overburden the site, and well that would take a bit more time to write.\n",
      "acknowledgements\n",
      "mainly i would just like to thank the site for existing as a resource, and of course to any contributors on this data.\n",
      "inspiration\n",
      "i'd like to see some more eda done, i'll be doing my work in python so i like seeing the r kernels, interesting to see another approach.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "the united states census bureau conducts regular surveys to assess education levels in the u.s. these surveys sample participants' highest levels of education (i.e. high school diploma, bachelor's degree, etc.) the attached csv file aggregates data for the years 1995, 2005, and 2015.\n",
      "content\n",
      "data is organized into columns representing the survey year, age range, sex of participants, and education level. for example, [1995, 18_24, male, ...] represents the 1995 survey for men ages 18-24.\n",
      "it's worth noting that the surveys varied somewhat in granularity. the 2015 survey divided categories more finely (18-24, 25-29, 29-34...) while the 2005 and 1995 surveys were coarser (18-24, 25-34, ...). this could create some distortion depending on the analysis used.\n",
      "sources\n",
      "main\n",
      "https://www.census.gov/topics/education/educational-attainment/data/tables.all.html\n",
      "2015\n",
      "table 1. educational attainment of the population 18 years and over, by age, sex, race, and hispanic origin: 2015\n",
      "https://www.census.gov/data/tables/2015/demo/education-attainment/p20-578.html\n",
      "2005\n",
      "table 6. educational attainment of employed civilians 18 to 64 years, by occupation, age, sex, race, and hispanic origin: 2005\n",
      "https://www.census.gov/data/tables/2005/demo/educational-attainment/cps-detailed-tables.html\n",
      "1995\n",
      "educational attainment in the united states: march 1995\n",
      "https://www.census.gov/data/tables/1995/demo/educational-attainment/p20-489.html\n",
      "context\n",
      "many many search queries bypassing the limits of twitter api. these files can easily be read using read_delim in r and using ; as the delimiter. the search query is in the name of the csv. the time runs from this year until ~2012, but each set varies.\n",
      "dental pain, alternatives, opioid prescription, patient visits, dental advertisements, misinformation, and more are found within these datasets.\n",
      "this tweet set is great for text analytics, machine learning, and etc.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "i believe healthcare should be a right for all, and that includes dental care? what alternatives do people turn to when they cannot afford the care they need? i hear many of my friends, in their 20s like myself, not receiving coverage or waiting to go to the dentist. although age is not included in these csv. user profiles if public can be grabbed using the twitter api.\n",
      "this dataset has attributes for all available weapons in fortnite: battle royale as of patch 2.1.0 or as of january 10th 2018. all credit for this data goes to soumydev at http://www.fortnitechests.info/\n",
      "the dataset features the following columns:\n",
      "name: name of the weapon\n",
      "dps: the damage per second of the weapon\n",
      "damage: the damage done by the weapon\n",
      "critical %: the critical hit chance of the weapon\n",
      "crit. damage: the critical hit damage of the weapon\n",
      "fire rate: the fire rate of the weapon\n",
      "mag. size: the size of the magazine of the weapon\n",
      "range: the range of the weapon\n",
      "durability: the durability of the weapon\n",
      "reload time: the reload time of the weapon\n",
      "ammo cost: the cost in ammunition to fire a single projectile\n",
      "impact: the impact of the weapon i.e. the damage it does to buildings\n",
      "rarity: the rarity of the weapon\n",
      "type: what type of weapon is in question\n",
      "in total there are 14 columns with 43 rows or weapons\n",
      "as mentioned previously, all credit for this data goes to soumydev at http://www.fortnitechests.info/\n",
      "potential uses for the dataset:\n",
      "find out is some weapon in the game underutilized\n",
      "what is the best weapon for different situations?\n",
      "are there different ways to classify weapons than by their type and rarity?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "data was grabbed from us-news: https://www.usnews.com\n",
      "the following data points are included in this data set:\n",
      "ranking\n",
      "acceptance-rate\n",
      "act-avg\n",
      "sat-avg\n",
      "photo\n",
      "cost after financial aid\n",
      "city\n",
      "sortname\n",
      "zip\n",
      "percent receiving aid\n",
      "state\n",
      "average high school gpa\n",
      "tied ranking\n",
      "public/private university\n",
      "business reputation score\n",
      "tuition\n",
      "engineering reputation score\n",
      "enrollment size\n",
      "region\n",
      "it includes statistics surrounding the following 311 us universities.\n",
      "princeton university\n",
      "harvard university\n",
      "university of chicago\n",
      "yale university\n",
      "columbia university\n",
      "massachusetts institute of technology\n",
      "stanford university\n",
      "university of pennsylvania\n",
      "duke university\n",
      "california institute of technology\n",
      "dartmouth college\n",
      "johns hopkins university\n",
      "northwestern university\n",
      "brown university\n",
      "cornell university\n",
      "rice university\n",
      "vanderbilt university\n",
      "university of notre dame\n",
      "washington university in st. louis\n",
      "georgetown university\n",
      "emory university\n",
      "university of california--berkeley\n",
      "university of california--los angeles\n",
      "university of southern california\n",
      "carnegie mellon university\n",
      "university of virginia\n",
      "wake forest university\n",
      "university of michigan--ann arbor\n",
      "tufts university\n",
      "new york university\n",
      "university of north carolina--chapel hill\n",
      "boston college\n",
      "college of william and mary\n",
      "brandeis university\n",
      "georgia institute of technology\n",
      "university of rochester\n",
      "boston university\n",
      "case western reserve university\n",
      "university of california--santa barbara\n",
      "northeastern university\n",
      "tulane university\n",
      "rensselaer polytechnic institute\n",
      "university of california--irvine\n",
      "university of california--san diego\n",
      "university of florida\n",
      "lehigh university\n",
      "pepperdine university\n",
      "university of california--davis\n",
      "university of miami\n",
      "university of wisconsin--madison\n",
      "villanova university\n",
      "pennsylvania state university--university park\n",
      "university of illinois--urbana-champaign\n",
      "ohio state university--columbus\n",
      "university of georgia\n",
      "george washington university\n",
      "purdue university--west lafayette\n",
      "university of connecticut\n",
      "university of texas--austin\n",
      "university of washington\n",
      "brigham young university--provo\n",
      "fordham university\n",
      "southern methodist university\n",
      "syracuse university\n",
      "university of maryland--college park\n",
      "worcester polytechnic institute\n",
      "clemson university\n",
      "university of pittsburgh\n",
      "american university\n",
      "rutgers university--new brunswick\n",
      "stevens institute of technology\n",
      "texas a&m university--college station\n",
      "university of minnesota--twin cities\n",
      "virginia tech\n",
      "baylor university\n",
      "colorado school of mines\n",
      "university of massachusetts--amherst\n",
      "miami university--oxford\n",
      "texas christian university\n",
      "university of iowa\n",
      "clark university\n",
      "florida state university\n",
      "michigan state university\n",
      "north carolina state university--raleigh\n",
      "university of california--santa cruz\n",
      "university of delaware\n",
      "binghamton university--suny\n",
      "university of denver\n",
      "university of tulsa\n",
      "indiana university--bloomington\n",
      "marquette university\n",
      "university of colorado--boulder\n",
      "university of san diego\n",
      "drexel university\n",
      "saint louis university\n",
      "yeshiva university\n",
      "rochester institute of technology\n",
      "stony brook university--suny\n",
      "suny college of environmental science and forestry\n",
      "university at buffalo--suny\n",
      "university of oklahoma\n",
      "university of vermont\n",
      "auburn university\n",
      "illinois institute of technology\n",
      "loyola university chicago\n",
      "university of new hampshire\n",
      "university of oregon\n",
      "university of south carolina\n",
      "university of tennessee\n",
      "howard university\n",
      "university of alabama\n",
      "university of san francisco\n",
      "university of the pacific\n",
      "university of utah\n",
      "arizona state university--tempe\n",
      "iowa state university\n",
      "temple university\n",
      "university of kansas\n",
      "university of st. thomas\n",
      "the catholic university of america\n",
      "depaul university\n",
      "duquesne university\n",
      "university of missouri\n",
      "clarkson university\n",
      "colorado state university\n",
      "michigan technological university\n",
      "seton hall university\n",
      "university of arizona\n",
      "university of california--riverside\n",
      "university of dayton\n",
      "university of nebraska--lincoln\n",
      "hofstra university\n",
      "louisiana state university--baton rouge\n",
      "mercer university\n",
      "the new school\n",
      "rutgers university--newark\n",
      "university of arkansas\n",
      "university of cincinnati\n",
      "university of kentucky\n",
      "george mason university\n",
      "new jersey institute of technology\n",
      "san diego state university\n",
      "university of south florida\n",
      "washington state university\n",
      "kansas state university\n",
      "oregon state university\n",
      "st. john fisher college\n",
      "university of illinois--chicago\n",
      "university of mississippi\n",
      "university of texas--dallas\n",
      "adelphi university\n",
      "florida institute of technology\n",
      "ohio university\n",
      "seattle pacific university\n",
      "university at albany--suny\n",
      "oklahoma state university\n",
      "university of massachusetts--lowell\n",
      "university of rhode island\n",
      "biola university\n",
      "illinois state university\n",
      "university of alabama--birmingham\n",
      "university of hawaii--manoa\n",
      "university of la verne\n",
      "university of maryland--baltimore county\n",
      "immaculata university\n",
      "maryville university of st. louis\n",
      "missouri university of science & technology\n",
      "st. john's university\n",
      "university of california--merced\n",
      "university of louisville\n",
      "mississippi state university\n",
      "rowan university\n",
      "university of central florida\n",
      "university of idaho\n",
      "virginia commonwealth university\n",
      "kent state university\n",
      "robert morris university\n",
      "texas tech university\n",
      "union university\n",
      "university of hartford\n",
      "edgewood college\n",
      "lesley university\n",
      "lipscomb university\n",
      "suffolk university\n",
      "university of maine\n",
      "university of wyoming\n",
      "azusa pacific university\n",
      "ball state university\n",
      "montclair state university\n",
      "pace university\n",
      "west virginia university\n",
      "andrews university\n",
      "indiana university-purdue university--indianapolis\n",
      "university of houston\n",
      "university of new mexico\n",
      "university of north dakota\n",
      "widener university\n",
      "new mexico state university\n",
      "north dakota state university\n",
      "nova southeastern university\n",
      "university of north carolina--charlotte\n",
      "bowling green state university\n",
      "california state university--fullerton\n",
      "dallas baptist university\n",
      "university of massachusetts--boston\n",
      "university of nevada--reno\n",
      "central michigan university\n",
      "east carolina university\n",
      "florida a&m university\n",
      "montana state university\n",
      "university of alaska--fairbanks\n",
      "university of colorado--denver\n",
      "university of massachusetts--dartmouth\n",
      "university of montana\n",
      "western michigan university\n",
      "florida international university\n",
      "louisiana tech university\n",
      "south dakota state university\n",
      "southern illinois university--carbondale\n",
      "university of alabama--huntsville\n",
      "university of missouri--kansas city\n",
      "utah state university\n",
      "ashland university\n",
      "benedictine university\n",
      "california state university--fresno\n",
      "gardner-webb university\n",
      "georgia state university\n",
      "shenandoah university\n",
      "university of south dakota\n",
      "wayne state university\n",
      "american international college\n",
      "augusta university\n",
      "barry university\n",
      "boise state university\n",
      "cardinal stritch university\n",
      "clark atlanta university\n",
      "cleveland state university\n",
      "eastern michigan university\n",
      "east tennessee state university\n",
      "florida atlantic university\n",
      "georgia southern university\n",
      "grand canyon university\n",
      "indiana state university\n",
      "indiana university of pennsylvania\n",
      "jackson state university\n",
      "kennesaw state university\n",
      "lamar university\n",
      "liberty university\n",
      "lindenwood university\n",
      "middle tennessee state university\n",
      "morgan state university\n",
      "national louis university\n",
      "north carolina a&t state university\n",
      "northern arizona university\n",
      "northern illinois university\n",
      "oakland university\n",
      "old dominion university\n",
      "portland state university\n",
      "prairie view a&m university\n",
      "regent university\n",
      "sam houston state university\n",
      "san francisco state university\n",
      "spalding university\n",
      "tennessee state university\n",
      "tennessee technological university\n",
      "texas a&m university--commerce\n",
      "texas a&m university--corpus christi\n",
      "texas a&m university--kingsville\n",
      "texas southern university\n",
      "texas state university\n",
      "texas woman's university\n",
      "trevecca nazarene university\n",
      "trinity international university\n",
      "university of akron\n",
      "university of arkansas--little rock\n",
      "university of louisiana--lafayette\n",
      "university of louisiana--monroe\n",
      "university of maryland--eastern shore\n",
      "university of memphis\n",
      "university of missouri--st. louis\n",
      "university of nebraska--omaha\n",
      "university of nevada--las vegas\n",
      "university of new orleans\n",
      "university of north carolina--greensboro\n",
      "university of northern colorado\n",
      "university of north texas\n",
      "university of south alabama\n",
      "university of southern mississippi\n",
      "university of texas--arlington\n",
      "university of texas--el paso\n",
      "university of texas--rio grande valley\n",
      "university of texas--san antonio\n",
      "university of the cumberlands\n",
      "university of toledo\n",
      "university of west florida\n",
      "university of west georgia\n",
      "university of wisconsin--milwaukee\n",
      "valdosta state university\n",
      "wichita state university\n",
      "wright state university\n",
      "alliant international university\n",
      "argosy university\n",
      "california institute of integral studies\n",
      "capella university\n",
      "idaho state university\n",
      "northcentral university\n",
      "trident university international\n",
      "union institute and university\n",
      "university of phoenix\n",
      "walden university\n",
      "wilmington university\n",
      "context\n",
      "i used a bag of words to find 900 public nazi/altright twitter accounts, and this is (up to) 200 tweets scraped from each account, in json. (november 14, 2017)\n",
      "content\n",
      "these are twitter status objects. they contain a variety of twitter user information, hashtags, and other attributes.\n",
      "inspiration\n",
      "i'm working on refining a nazi-detection engine---first by refining the bag of words to help improve the dataset and data collection, then by using the improved dataset to train an ml model: https://github.com/saraislet/sturm/\n",
      "context\n",
      "it is about recognizing songs emotion. generally, for songs, audio features and lyrics could be used. for ground truth data, it tends to use online tags if dataset scale is large. otherwise, music experts or trained candidates could be organized to label songs emotion.\n",
      "content\n",
      "here songs data is from the million song dataset (msd). it contains almost 1 million songs data. and in the stage1, only tempo, loudness and mode are extracted for analysis as audio feature. for checking convenience, artist and title are added.\n",
      "for ground truth data, tags from lastfm is adopted.\n",
      "for two sets of data mentioned above, some preprocessing work is done. if you have more interests, you can find all original data here. https://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset\n",
      "songs data and tags data could be connected by track_id.\n",
      "in the further stage, timbre and pitch could be used in an appropriate way, especially related to time-series analysis. adding genre, similarity or other features to assist songs emotion prediction.\n",
      "acknowledgements\n",
      "the million song dataset was created under a grant from the national science foundation, project iis-0713334. the original data was contributed by the echo nest, as part of an nsf-sponsored goali collaboration. subsequent donations from secondhandsongs.com, musixmatch.com, and last.fm, as well as further donations from the echo nest, are gratefully acknowledged.\n",
      "inspiration\n",
      "use this dataset to do some exploratory data analysis, predict emotion for songs if you like.\n",
      "context\n",
      "disk space captured for several months for a set of windows servers\n",
      "content\n",
      "contents are the server name, disk drive, total disk space, free disk space and percentage of free space\n",
      "acknowledgements\n",
      "thanks to kaggle for providing this development environment\n",
      "inspiration\n",
      "my initial goal is to add a column with the moving average of free disk space (for 7 days), to be used for forcasting\n",
      "context\n",
      "i was searching for the most sought after startups in india and i came to know about these startups. the list contains the overview of the top 50 promising startups in india.\n",
      "content\n",
      "the excel sheet contains the startup and the idea they are working on. data was acquired by economic times newspaper. i have put that data inside this excel sheet to upload on kaggle. this list was published on december 2016. it was meant to represent the startups which are most expected to do well in 2017.\n",
      "the data was collected in 2016. columns contain 1) name of startup 2) industry 3) location 4) founding team 5) business idea\n",
      "acknowledgements\n",
      "the data was collected by economic times india. i have just put that data into an excel sheet. original data : 50 hot startups in 2017\n",
      "inspiration\n",
      "this dataset can be updated to include even more startups. we can have a list of startups, their ideas and whether they succeeded or not. thus this kind of data can be used to predict the possibility of success or failure of a business idea.\n",
      "context\n",
      "my goal with this dataset is to create the largest and most organized dataset of jokes.\n",
      "tools for this dataset are on my github\n",
      "content\n",
      "jokes reduced to only the question and the answer.\n",
      "duplicates not removed\n",
      "offensive jokes not removed\n",
      "acknowledgements\n",
      "question-answer jokes by jiri roznovjak\n",
      "short jokes by abhinav moudgil\n",
      "inspiration\n",
      "humor is one of the most difficult domains of natural language processing.\n",
      "context\n",
      "this dataset has been obtained by scraping ta (the famous tourism website) for information about restaurants for a given city. the scraper goes through the restaurants listing pages and fulfills a raw dataset. the raw datasets for the main cities in europe have been then curated for futher analysis purposes, and aggregated to obtain this dataset.\n",
      "the scraper is a python script, available on the github repository here.\n",
      "it uses principally pandas and beautifulsoup libraries.\n",
      "important: the restaurants list contains the restaurants that are registrered in the ta database only. all the restaurants of a city may not be resgistered in this database.\n",
      "content\n",
      "the dataset contain restaurants information for 31 cities in europe: amsterdam (nl), athens (gr) , barcelona (es) , berlin (de), bratislava (sk), bruxelles (be), budapest (hu), copenhagen (dk), dublin (ie), edinburgh (uk), geneva (ch), helsinki (fi), hamburg (de), krakow (pl), lisbon (pt), ljubljana (si), london (uk), luxembourg (lu), madrid (es), lyon (fr), milan (it), munich (de), oporto (pt), oslo (no), paris (fr), prague (cz), rome (it), stockholm (se), vienna (at), warsaw (pl), zurich (ch).\n",
      "the data is a .csv file comma-separated that contains 125 433 entries (restaurants). it is structured as follow: - name: name of the restaurant\n",
      "city: city location of the restaurant\n",
      "cuisine style: cuisine style(s) of the restaurant, in a python list object (94 046 non-null)\n",
      "ranking: rank of the restaurant among the total number of restaurants in the city as a float object (115 645 non-null)\n",
      "rating: rate of the restaurant on a scale from 1 to 5, as a float object (115 658 non-null)\n",
      "price range: price range of the restaurant among 3 categories , as a categorical type (77 555 non-null)\n",
      "number of reviews: number of reviews that customers have let to the restaurant, as a float object (108 020 non-null)\n",
      "reviews: 2 reviews that are displayed on the restaurants scrolling page of the city, as a list of list object where the first list contains the 2 reviews, and the second le dates when these reviews were written (115 673 non-null)\n",
      "url_ta: part of the url of the detailed restaurant page that comes after 'www.tripadvisor.com' as a string object (124 995 non-null)\n",
      "id_ta: identification of the restaurant in the ta database constructed a one letter and a number (124 995 non-null)\n",
      "missing information for restaurants (for example unrated or unreviewed restaurants) are in the dataset as nan (numpy.nan).\n",
      "acknowledgements\n",
      "this work has been done as a personal interest but also as a training of the skills i got from the datacamp data science bootcamp i have followed.\n",
      "i hope you will find this dataset inspiring and will make great stories out of it that i will be pleased to read :)\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "the data provides customer and date level transactions for few years. it can be used for demonstration of any analysis that require transaction information like rfm. the data also provide response information of customers to a promotion campaign.\n",
      "content\n",
      "transaction data provides customer_id, transaction date and amount of purchase. response data provides the response information of each of the customers. it is a binary variable indicating whether the customer responded to a campaign or not.\n",
      "acknowledgements\n",
      "extremely thankful numerous kernel and data publishers of kaggle and github. learnt a lot from these communities.\n",
      "inspiration\n",
      "more innovative approaches for handling rfm analysis.\n",
      "context\n",
      "fasttext word embeddings trained on english wikipedia\n",
      "fasttext embeddings are enriched with sub-word information useful in dealing with misspelled and out-of-vocabulary words.\n",
      "content\n",
      "each line contains a word followed by 300-dimensional embedding\n",
      "acknowledgements\n",
      "p. bojanowski, e. grave, a. joulin, t. mikolov, \"enriching word vectors with subword information\", arxiv 2016\n",
      "fasttext embeddings: https://github.com/facebookresearch/fasttext/blob/master/pretrained-vectors.md\n",
      "inspiration\n",
      "q1: how does fasttext compare with glove and word2vec embeddings?\n",
      "q2: what are the different approaches for learning embeddings with sub-word information?\n",
      "q3: how does fasttext compare with character-level n-gram representation of words?\n",
      "context\n",
      "i made this dataset for coursera assignment (applied plotting, charting & data representation in python).\n",
      "content\n",
      "price transition of crypto-currencies in 2017. these data were downloaded via poloniex api.\n",
      "many women who are initially thought to have angina turn out to have normal coronary angiograms, that is they are found not to have angina after all. a study was carried out to assess the feasibility of a preliminary screening test. for a large number of patients who were thought to have angina, information on a number of possible risk factors was collected and then their subsequent angina status was recorded. the data is available as an r data frame entitled angina and contains the following information:\n",
      "status: whether woman turns out to have angina (yes/no) age: age of a woman smoke: smoking status (1=current-, 2=ex-, 3=non-smoker) cig: current average number of cigarettes per day hyper: hypertension (1=absent, 2=mild, 3=moderate) angfam: family history of angina (yes/no) myofam: family history of myocardial infarction (yes/no) strokefam: family history of stroke (yes/no) diabetes: does woman have diabetes? (yes/no) missing values are coded as na.\n",
      "the main aim of this study was to try to find out which, if any, of the health variables, are associated with angina and whether some subset of them could be used to help predict the dependent variable angina status. the accompanying document on the `model selection through backward elimination’ is going to be useful for that purpose. more specifically, it would be helpful to be able to estimate the risk/probability that a woman with a particular combination of these health variables truly has angina. if such a scheme of estimating risks can be constructed, is it likely to be useful? i.e. is it good at predicting whether a woman has angina or not (since the treatment of angina is expensive)? in addition, it would be of interest to estimate the individual effects of important variables. for example, if smoking seems to be a risk factor, then what is the odds of a smoker having angina relative to a non-smoker? what about ex-smokers and light smokers?\n",
      "first held in 1959, the international mathematical olympiad is an annual math competition for top high school students around the world. it consists of six problems, divided between two days: on each day, contestants are given 4.5 hours to solve three problems.\n",
      "this dataset contains scores of the imo from 1984 to 2017. the data for years 1959-1984 do not always record the scores for each individual problem, so it is omitted from this dataset.\n",
      "source: scraped from imo-official.org.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "i've always had problems with sleep. i felt bad either undersleeping or oversleeping. so last october i promised a friend of mine to get up at 5 am for one month and started logging sleep data to a google spreadsheet. sometimes i broke my promise, sometimes i was sick or on vacation and overslept but mainly kept my promise. now with the help of kaggle community i want to analyze this data and know how much sleep i need. need to say that i'm fan of biphasic and polyphasic sleep.\n",
      "content\n",
      "i put a reminder on todo list to enter data everyday. i've decided not to capture many features besides times, only how easy felt asleep, how easy got up and how felt afterwards. there're usually more than one row per day. i usually get up by 5-6 am and if feel very sleepy go back to sleep again. sometimes i had a nap in the afternoon or in weekends.\n",
      "inspiration\n",
      "i would like to know minimum amount of sleep i need to feel either \"very good\" (4.5) or \"super good\" (5). i guess this will be an average time for the last n days. by the way after how many days body resets it's sleep clock? it would be nice to know optimal time to go to bed too.\n",
      "this dataset does not have a description yet.\n",
      "dataset containing 173 college common data sets\n",
      "contains common data sets for the following schools:\n",
      "alabama state university\n",
      "angelo state university\n",
      "arapahoe community college\n",
      "arkansas tech university\n",
      "aurora university\n",
      "baldwin wallace university\n",
      "beloit college\n",
      "bemidji state university\n",
      "berea college\n",
      "binghamton university\n",
      "boston university\n",
      "bucknell university\n",
      "cabrini university\n",
      "california baptist university\n",
      "california state university, bakersfield\n",
      "california state university, long beach\n",
      "california state university, los angeles\n",
      "california state university, sacramento\n",
      "carnegie mellon university\n",
      "case western reserve university\n",
      "christopher newport university\n",
      "clark university\n",
      "colby college\n",
      "college of charleston\n",
      "collin college\n",
      "colorado college\n",
      "colorado school of mines\n",
      "colorado state university-pueblo\n",
      "columbia college\n",
      "concordia university texas\n",
      "cornell university\n",
      "davidson college\n",
      "delaware technical community college\n",
      "desales university\n",
      "dickinson college\n",
      "drake university\n",
      "drew university\n",
      "duquesne university\n",
      "east central university\n",
      "eastern washington university\n",
      "embry riddle aeronautical university-daytona beach\n",
      "fairfield university\n",
      "florida gulf coast\n",
      "florida international university\n",
      "fort hays state university\n",
      "georgia institute of technology\n",
      "gettysburg college\n",
      "hamilton college\n",
      "hollins university\n",
      "humboldt state university\n",
      "iowa state university\n",
      "jackson state university\n",
      "john jay college of criminal justice\n",
      "kennesaw state university\n",
      "lafayette college\n",
      "lane college\n",
      "lee university\n",
      "le moyne college\n",
      "lenoir rhyne university\n",
      "life university\n",
      "loyola university maryland\n",
      "lubbock christian university\n",
      "lycoming college\n",
      "lynn university common data set\n",
      "malone university\n",
      "marlboro college\n",
      "maryville university\n",
      "massachusetts maritime academy\n",
      "metropolitan state university of denver\n",
      "michigan technological university\n",
      "middlebury college\n",
      "millersville university\n",
      "mississippi state university\n",
      "mott community college\n",
      "neumann university\n",
      "northeastern state university\n",
      "northern arizona university\n",
      "northern kentucky university\n",
      "nyack college\n",
      "oklahoma christian university\n",
      "oklahoma state university\n",
      "old dominion university\n",
      "oral roberts university\n",
      "pepperdine university\n",
      "pomona college\n",
      "prescott college\n",
      "providence college\n",
      "reed college\n",
      "regis university\n",
      "rensselaer polytechnic institute\n",
      "rice university\n",
      "rochester college\n",
      "rutgers university\n",
      "saint vincent college\n",
      "san francisco state university\n",
      "santa clara university\n",
      "scripps college\n",
      "seton hill university\n",
      "sewanee\n",
      "shippensburg university\n",
      "simpson university\n",
      "slippery rock university\n",
      "smith college\n",
      "sonoma state university\n",
      "southeastern community college\n",
      "southeastern oklahoma state university\n",
      "southwestern oklahoma state university\n",
      "springfield college\n",
      "st. ambrose university\n",
      "stanford university\n",
      "stephen f. austin state university\n",
      "suny oneonta\n",
      "suny potsdam\n",
      "sweet briar college\n",
      "taylor university\n",
      "temple university\n",
      "tennessee wesleyan university\n",
      "texas a&m university - kingsville\n",
      "texas a&m university\n",
      "texas wesleyan university\n",
      "the college at brockport\n",
      "the college of new jersey\n",
      "the university of scranton\n",
      "the university of southern mississippi\n",
      "the university of tennessee\n",
      "trinity university\n",
      "tufts university\n",
      "tulane university\n",
      "tulsa community college\n",
      "university at buffalo\n",
      "university enrollment\n",
      "university of california - davis\n",
      "university of california, riverside\n",
      "university of colorado boulder, 2015\n",
      "university of delaware\n",
      "university of kentucky\n",
      "university of louisville\n",
      "university of maine\n",
      "university of missouri\n",
      "university of montana\n",
      "university of mount olive: 2016\n",
      "university of nebraska kearney\n",
      "university of nebraska-lincoln\n",
      "university of nevada, reno\n",
      "university of new hampshire\n",
      "university of new mexico\n",
      "university of north alabama\n",
      "university of north carolina at charlotte\n",
      "university of pennsylvania\n",
      "university of pikeville\n",
      "university of puget sound\n",
      "university of science and arts\n",
      "university of texas rio\n",
      "university of the sciences in philadelphia\n",
      "university of wisconsin\n",
      "university wide common data set 2015\n",
      "villanova university\n",
      "virginia commonwealth university\n",
      "washburn university\n",
      "washington and lee university\n",
      "washington college\n",
      "weber state university\n",
      "wellesley college\n",
      "wesleyan university\n",
      "westfield state university\n",
      "westminster college\n",
      "wheaton college\n",
      "whitman college\n",
      "widener university\n",
      "worcester polytechnic institute\n",
      "xavier university of louisiana\n",
      "xavier university\n",
      "yale university\n",
      "context\n",
      "database of all walmart and sam's club stores in the united states\n",
      "python wrapper surrounding the dataset\n",
      "we are building a system to detect whether a group of words is written by one writer or more than one writes are involved. we need help in improving the efficiency of the classification. help us in improving the classifier for given files.\n",
      "context\n",
      "this is a huge dataset and takes around 400 seconds to load into kernel. if you need quickly imdb data in keras kernel use the following dataset instead:\n",
      "https://www.kaggle.com/pankrzysiu/keras-imdb-reviews\n",
      "content\n",
      "a set of 50,000 highly-polarized reviews from the internet movie database.\n",
      "usage instructions\n",
      "aclimdb_v1.zip file\n",
      "this file is to be used directly in your code. the .zip file will be automatically uncompressed by kaggle.\n",
      "imdb* files\n",
      "from os import listdir, makedirs\n",
      "from os.path import join, exists, expanduser\n",
      "\n",
      "cache_dir = expanduser(join('~', '.keras'))\n",
      "if not exists(cache_dir):\n",
      "    makedirs(cache_dir)\n",
      "datasets_dir = join(cache_dir, 'datasets')\n",
      "if not exists(datasets_dir):\n",
      "    makedirs(datasets_dir)\n",
      "\n",
      "# if you have multiple input files, change the below cp commands accordingly, typically:\n",
      "# !cp ../input/keras-imdb/imdb* ~/.keras/datasets/\n",
      "!cp ../input/imdb* ~/.keras/datasets/\n",
      "acknowledgements\n",
      "the files are on the net in these locations:\n",
      "https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "they are used by keras imdb.py:\n",
      "https://github.com/keras-team/keras/blob/master/keras/datasets/imdb.py\n",
      "inspiration\n",
      "\"python deep learning\" book example is using this:\n",
      "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb\n",
      "context\n",
      "anonymized data from profiles scraped on linkedin. contains data from about 15000 profiles. profiles came from people predominantly located in australia. includes all their work history as well as analysis of their photo and name.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this dataset does not have a description yet.\n",
      "an extension to kaggle's ted dataset\n",
      "using transcripts provided by ted.com, a dataset that combines combines youtube metadata (at the time of scrapping) and metadata from kaggle's ted dataset.\n",
      "this extension provides not just additional metadata from youtube, but also additional transcripts of the same video, but in different languages (e.g. portugese, french, arabic, chinese, japnese, korean, turkish, dutch...). in total, 111 different languages are available (most videos do not have transcript for all languages).\n",
      "content\n",
      "for each of the 111 languages in teddirector.zip, each language file is a csv file with the following headers:\n",
      "videoid - youtube ids\n",
      "lang - language code\n",
      "title - title of the ted talk\n",
      "transcript - transcript of the ted talk in lang\n",
      "acknowledgements\n",
      "this dataset was developed as part of a larger dataset used for an information retrieval assignment. in that assignment, my team and i used ted talks to evaluate different configuration of search engine algorithms. we also used different languages for the search and retrieve task, to test for reliability of our search engine. more information can be found in our github repository. dataset is downloaded from tedtalksdirector using youtube-dl.\n",
      "code for downloading can be found from the ir project.\n",
      "more about language codes here from w3schools.com.\n",
      "inspiration\n",
      "some of the problem experienced while preparing this dataset: 1. how can we improve the matching of youtube dataset to the data scrapped from ted talk\n",
      "context：\n",
      "this data set contains the communication of students with the smartphones in the schools in a week. we set a evaluation to describe the quality of the user the through the first 3 pieces of information.\n",
      "content：\n",
      "useful information : it means the pieces of useful information between communication. connection frequency: it means the times the user connects to other users. effective connection: it means the successful times of communication with others. evaluation: it is set to judge the popularity of the users.\n",
      "acknowledgements：\n",
      "it's collected by an online survey in several schools.\n",
      "inspiration：\n",
      "can we use the data to find popular student in the school? it's useful for teachers to choose students that good at making friends.\n",
      "context\n",
      "this is the city of new york's list of official dog parks.\n",
      "some ideas for what could be interesting: mapping the most dog-friendly neighborhoods in the city, perhaps to help dog owners find an apartment in an area with access to bigger or multiple dog runs. if one combined this dataset with others like nyc's dog licensing dataset, one could explore the ratio of the area of dog run space to the number of dogs, perhaps by neighborhood. this could show dogs per square foot of dog park space, painting a picture of which neighborhoods are satisfying the needs of canine citizens and which ones have some work to do.\n",
      "content\n",
      "the data were last updated by the city of new york april 11, 2017.\n",
      "these are the fields in the dataset:\n",
      "prop_id: a unique identifier for the property. the first character is a abbreviation of the borough, followed by a 3 digit number. anything after the first 4 characters represents a subproperty. boroughs:\n",
      "x - bronx, b - brooklyn, m - manhattan, q - queens, r - staten island\n",
      "to find more data on each prop_id (exact address, etc.), these parks datasets may be used: http://www.nycgovparks.org/bigapps/dpr_parks_001.xml\n",
      "http://www.nycgovparks.org/bigapps/dpr_parks_001.json\n",
      "name: name of the property\n",
      "address: approximate location of the dog run or off-leash area (note, this is very approximate - to be more precise, cross-reference to the bigger parks dataset above)\n",
      "dogruns_type: dog run or off-leash area\n",
      "accessible: (y)es or (n)o - wheelchair accessible\n",
      "notes: additional notes\n",
      "acknowledgements\n",
      "the dataset comes from the city of new york's open data project.\n",
      "context\n",
      "this dataset originally stems from a walmart recruiting challenge. it is used here for educational purposes only.\n",
      "content\n",
      "the dataset contains anonymized sales by department for 45 walmart stores as well as supporting features.\n",
      "acknowledgements\n",
      "the dataset belongs to walmart and is used here only for educational purposes\n",
      "inspiration\n",
      "try to predict weekly sales.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "some of the most challenging problems in machine learning for me have been in predicting stock market results. so i can use kaggle's kernels on this data, i'm adding the data to kaggle.\n",
      "content\n",
      "acknowledgements\n",
      "this data was gathered using the tidyquant package.\n",
      "context\n",
      "about ease of doing business around the world.\n",
      "content\n",
      "column includes : ease of doing business | starting a business | dealing with construction permits | getting electricity | registering property | getting credit | protecting minority investors | paying taxes | trading across borders | enforcing contracts | resolving insolvency\n",
      "acknowledgements\n",
      "world bank | http://www.doingbusiness.org/rankings\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this directory contains the cross-position activity recognition datasets used in the following paper. please consider citing this article if you want to use the datasets.\n",
      "jindong wang, yiqiang chen, lisha hu, xiaohui peng, and philip s. yu. stratified transfer learning for cross-domain activity recognition. 2018 ieee international conference on pervasive computing and communications (percom).\n",
      "these datasets are secondly constructed based on three public datasets: opportunity (opp) [1], pamap2 (pamap2) [2], and uci dsads (dsads) [3].\n",
      "here are some useful information about this directory. please feel free to contact jindongwang@outlook.com for more information.\n",
      "this is not the raw data, since i have performed feature extraction and normalized the features into [-1,1]. the code for feature extraction can be found in here: https://github.com/jindongwang/activityrecognition/tree/master/code. currently, there are 27 features for a single sensor. there are 81 features for a body part. more information can be found in above percom-18 paper.\n",
      "there are 4 .mat files corresponding to each dataset: dsads.mat for uci dsads, opp_hl.mat and opp_ll.mat for opportunity, and pamap.mat for pamap2. note that opp_hl and opp_loco denotes 'high-level' and 'locomotion' activities, respectively. (1) dsads.mat: 9120 * 408. columns 1~405 are features, listed in the order of 'torso', 'right arm', 'left arm', 'right leg', and 'left leg'. each position contains 81 columns of features. columns 406~408 are labels. column 406 is the activity sequence indicating the executing of activities (usually not used in experiments). column 407 is the activity label (1~19). column 408 denotes the person (1~8). (2) opp_hl.mat and opp_loco.mat: same as dsads.mat. but they contain more body parts: 'back', 'right upper arm', 'right lower arm', 'left upper arm', 'left lower arm', 'right shoe (foot)', and 'left shoe (foot)'. of course we did not use the data of both shoes in our paper. column 460 is the activity label (please refer to opportunity dataset to see the meaning of those activities). column 461 is the activity drill (also check the dataset information). column 462 denotes the person (1~4). (3) pamap.mat: 7312 * 245. columns 1~243 are features, listed in the order of 'wrist', 'chest', and 'ankle'. column 244 is the activity label. column 245 denotes the person (1~9).\n",
      "there are another 3 datasets with the prefix 'cross_', containing only 4 common classes of each dataset. this is for experimenting the cross-dataset activity recognition (see our percom-18 paper). the 4 common classes are lying, standing, walking, and sitting. (1) cross_dsads.mat: 1920*406. columns 1~405 are features. column 406 is labels. (2) cross_opp.mat: 5022*460. columns 1~459 are features. column 460 is labels. (3) cross_pamap.mat: 3063 * 244. columns 1~243 are features. column 244 is labels.\n",
      "-------- original references for the 3 datasets:\n",
      "[1] r. chavarriaga, h. sagha, a. calatroni, s. t. digumarti, g. troster, ¨ j. d. r. millan, and d. roggen, “the opportunity challenge: a bench- ´ mark database for on-body sensor-based activity recognition,” pattern recognition letters, vol. 34, no. 15, pp. 2033–2042, 2013.\n",
      "[2] a. reiss and d. stricker, “introducing a new benchmarked dataset for activity monitoring,” in wearable computers (iswc), 2012 16th international symposium on. ieee, 2012, pp. 108–109.\n",
      "[3] b. barshan and m. c. yuksek, “recognizing daily and sports activities ¨ in two open source machine learning environments using body-worn sensor units,” the computer journal, vol. 57, no. 11, pp. 1649–1667, 2014.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "market segmentation: a company must have different strategies for product promotion for different individuals. not every individual has the same requirement and demand. so the company has to segment, target and position the product according to the tastes of various individuals.\n",
      "segmentation of products based on the revenue generated in different regions is done to understand the market trends.\n",
      "the segmentation will help the company to devise marketing strategies and promotional schemes to position the right product according to the preference of the consumers in any given segment.\n",
      "content\n",
      "the store dataset contains 10,000 observations or we can call it transactions of 5 variables. each row represents the transaction made by the reps. each column contains the attributes of this dataset include: reps - representative who are involved in the promotion and sale of the products in their respective region. products - there are 12 brands of products promoted by the company. qty - quantity sold in units revenue - revenue generated for each transaction region - there are 4 regions - east, north, south and west india.\n",
      "it would also be possible to predict the revenue and make it a regression task.\n",
      "inspiration\n",
      "i am hosting this dataset in order to give it wider exposure, to give the community an opportunity to experiment with different algorithms / models.\n",
      "context\n",
      "introduction: the dataset used for this experiment is real and authentic. the dataset is acquired from uci machine learning repository website [13]. the title of the dataset is ‘crime and communities’. it is prepared using real data from socio-economic data from 1990 us census, law enforcement data from the 1990 us lemas survey, and crimedata from the 1995 fbi ucr [13]. this dataset contains a total number of 147 attributes and 2216 instances.\n",
      "the per capita crimes variables were calculated using population values included in the 1995 fbi data (which differ from the 1990 census values).\n",
      "content\n",
      "the variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units. the crime attributes (n=18) that could be predicted are the 8 crimes considered 'index crimes' by the fbi)(murders, rape, robbery, .... ), per capita (actually per 100,000 population) versions of each, and per capita violent crimes and per capita nonviolent crimes)\n",
      "predictive variables : 125 non-predictive variables : 4 potential goal/response variables : 18\n",
      "acknowledgements\n",
      "http://archive.ics.uci.edu/ml/datasets/communities%20and%20crime%20unnormalized\n",
      "u. s. department of commerce, bureau of the census, census of population and housing 1990 united states: summary tape file 1a & 3a (computer files),\n",
      "u.s. department of commerce, bureau of the census producer, washington, dc and inter-university consortium for political and social research ann arbor, michigan. (1992)\n",
      "u.s. department of justice, bureau of justice statistics, law enforcement management and administrative statistics (computer file) u.s. department of commerce, bureau of the census producer, washington, dc and inter-university consortium for political and social research ann arbor, michigan. (1992)\n",
      "u.s. department of justice, federal bureau of investigation, crime in the united states (computer file) (1995)\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "data available in the dataset may not act as a complete source of information for identifying factors that contribute to more violent and non-violent crimes as many relevant factors may still be missing.\n",
      "however, i would like to try and answer the following questions answered.\n",
      "analyze if number of vacant and occupied houses and the period of time the houses were vacant had contributed to any significant change in violent and non-violent crime rates in communities\n",
      "how has unemployment changed crime rate(violent and non-violent) in the communities?\n",
      "were people from a particular age group more vulnerable to crime?\n",
      "does ethnicity play a role in crime rate?\n",
      "has education played a role in bringing down the crime rate?\n",
      "disclaimer\n",
      "i have prepared the data for rv_forecasting eda and i do have no claim on data's usage rights. all data belongs to jma. please use it at your own risk. i hope you could find this dataset useful as well and share your experiences with me.\n",
      "description\n",
      "since jma does not do all the possible calculations in many weather stations, i have focused on prefecture-centres which nearly all calculations are made in the expense of accuracy in the geographic tagging. it is assumed that all the districts within a prefecture should have similar weather conditions in a given day.\n",
      "content\n",
      "date calendar date in classic y-m-d format.\n",
      "air_district1 could be harvested from air_area_name column via str_split_fixed(air_area_name, \" \", n = 3)[,1] code utilizing stringr package in r\n",
      "avg_temp average temperature data\n",
      "rain_amount amount of precipitation in centimetres\n",
      "sunshine_hours daylight in hours\n",
      "wind_speed wind_speed in kilometres-per-hour\n",
      "snowfall total snowfall during day in centimetres\n",
      "dataset description:\n",
      "this dataset has population data of each indian district from 2001 and 2011 censuses.\n",
      "the special thing about this data is that it has centroids for each district and state.\n",
      "centroids for a district are calculated by mapping border of each district as a polygon of latitude/longitude points in a 2d plane and then calculating their mean center.\n",
      "centroids for a state are calculated by calculating the weighted mean center of all districts that constitutes a state. the population count is the weight assigned to each district.\n",
      "example analysis:\n",
      "the complete code for calculating the centroids and web scraping for the data is shared on github.\n",
      "the purpose of this project was to map population density center for each state.\n",
      "you can also read about the complete project here: https://medium.com/@sumit.arora/plotting-weighted-mean-population-centroids-on-a-country-map-22da408c1397\n",
      "output screenshots: indian districts mapped as polygons\n",
      "mapping centroids for each district\n",
      "mean centers of population by state, 2001 vs. 2011\n",
      "national center of population\n",
      "context\n",
      "i appreciate the thoroughness of stanford mass shootings in america (msa) and the continuous efforts to update us mass shootings last 50 years (1966-2017). this dataset merges the two, adding 14 records to stanford msa.\n",
      "content\n",
      "see original datasets for more information.\n",
      "acknowledgements\n",
      "i acknowledge the contributors of the two original datasets, zeeshan-ul-hassan usmani (us mass shootings) and carlos paradis (stanford msa).\n",
      "context\n",
      "national exam of (higher education) student performance [enade]\n",
      "national exam of upper secondary education [enem]\n",
      "national institute for educational studies and research [inep]\n",
      "content\n",
      "each row is a student that finished higher education education and has his enade and enem scores and more data.\n",
      "for more metadata: see the xlsx files (portuguese).\n",
      "references\n",
      "http://inep.gov.br/microdados\n",
      "http://portal.inep.gov.br/web/guest/about-inep\n",
      "this dataset contains training and testing data for digit recognition which includes hand written images of digits.\n",
      "it contains four zip files which you can easily include in your neural network. so, download all four of them by clicking \"download all\" button.\n",
      "this is the mnist dataset used world-wide to check the performance of neural networks based upon digit recognition.\n",
      "it also contains training and testing labels.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "hepatitis b levels of some random patients that i've collected over the course of 8 years, from random hospitals i've worked at. every patient also had one of the 3 broad categories of diseases. at the time, technical limitations kept me from accurately collecting the exact disease/disorder the patient had, so this has to do.\n",
      "(had to re-upload this dataset because kaggle decided to delete it)\n",
      "context\n",
      "arabic diacritics are often missed in arabic scripts. this feature is a handicap for new learner to read َarabic, text to speech conversion systems, reading and semantic analysis of arabic texts.\n",
      "the automatic diacritization systems are the best solution to handle this issue. but such automation needs resources as diactritized texts to train and evaluate such systems.\n",
      "content\n",
      "data is a collection of arabic vocalized texts, which covers modern and classical arabic language. the data contains over 75 million of fully vocalized words obtained from 97 books, structured in text files.\n",
      "the corpus is collected mostly from islamic classical books [14], and using semi-automatic web crawling process. the modern standard arabic texts crawled from the internet represent 1.15% of the corpus, about 867,913 words, while the most part is collected from shamela library, which represent 98.85%, with 74,762,008 words contained in 97 books\n",
      "acknowledgements\n",
      "we acknowledge the efforts made by shamela library volunteers to write, diacritize and make free texts.\n",
      "inspiration\n",
      "can machine vocalize an arabic text with précision? what semantic data can be extracted from vocalized texts\n",
      "squeezenet 1.0\n",
      "squeezenet: alexnet-level accuracy with 50x fewer parameters and <0.5mb model size\n",
      "recent research on deep neural networks has focused primarily on improving accuracy. for a given accuracy level, it is typically possible to identify multiple dnn architectures that achieve that accuracy level. with equivalent accuracy, smaller dnn architectures offer at least three advantages: (1) smaller dnns require less communication across servers during distributed training. (2) smaller dnns require less bandwidth to export a new model from the cloud to an autonomous car. (3) smaller dnns are more feasible to deploy on fpgas and other hardware with limited memory. to provide all of these advantages, we propose a small dnn architecture called squeezenet. squeezenet achieves alexnet-level accuracy on imagenet with 50x fewer parameters. additionally, with model compression techniques we are able to compress squeezenet to less than 0.5mb (510x smaller than alexnet).\n",
      "authors: forrest n. iandola, song han, matthew w. moskewicz, khalid ashraf, william j. dally, kurt keutzer\n",
      "https://arxiv.org/abs/1602.07360\n",
      "squeezenet architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "vgg-11\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "this database includes the information about the news which appears in the front page of meneame.net, a spanish website similar to reddit. data from 2005/12 to 2017/12/04. there are about 177.000 observations with 17 variables: news, users, post time, positive votes, negative votes, description and so on. the most part of the news are in spanish.\n",
      "esta base de datos incluye la información de las noticias que aparecen en la portada de la web meneame.net, un sitio web similar a reddit. los datos van desde diciembre de 2005 hasta el 4 de diciembre de 2017. hay 177.000 observaciones con 17 variables: noticias, usuarios, hora de envío, votos negativos, votos positivos, descripción, etc. la mayor parte de las noticias están en español.\n",
      "content\n",
      "index - news index\n",
      "  noticia - content of the news\n",
      "  link_noticia - link of the news\n",
      "  web - original website\n",
      "  usuario - user\n",
      "  fecha_envio - date when the post was sent\n",
      "  fecha_publicacion - date when the post was acces to the front page\n",
      "  meneos - positive + anonymous votes\n",
      "  clicks - clicks on the news\n",
      "  comentarios - comments\n",
      "  votos_positivos - positive votes\n",
      "  votos_anonimos - anonymous votes\n",
      "  votos_negativos - negative votes\n",
      "  karma - web points earned\n",
      "  sub - subsection\n",
      "  extracto - summary of the news\n",
      "acknowledgements\n",
      "best regards to meneame.net admins and users.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "content\n",
      "both datasets mononaice and mendotaice are structured the same.\n",
      "winter - the ending year for the winter. for example the the winter of 2016-2017 would be represented by 2017. closed - the date the lake froze over. the lake is considered closed if greater than 50% of the lake's surface is frozen. opened - the date the lake ice melted. the lake is considered opened if less than 50% of the lake's surface is frozen. days - total days in the season that the lake was closed.\n",
      "acknowledgements\n",
      "thank you to the wisconsin state climatology office\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this data was collected to train an autonomous car prototype as our college project.\n",
      "content\n",
      "the data consists of 14x32 grayscale pixel values along with proper labels as in which direction to maneuver the car. the labels are represented as follows: l- left, r- right, f- front, s- stop\n",
      "inspiration\n",
      "udacity autonomous driving nano degree program, waymo, tesla self-driving car\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "txt files for poetry generation with python\n",
      "content\n",
      "txt files of lyrics and poems\n",
      "acknowledgements\n",
      "free lyric hosting websites\n",
      "inspiration\n",
      "txt files for poetry generation with python\n",
      "this dataset does not have a description yet.\n",
      "310 observations, 13 attributes (12 numeric predictors, 1 binary class attribute - no demographics)\n",
      "lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spine. typical sources of low back pain include:\n",
      "the large nerve roots in the low back that go to the legs may be irritated the smaller nerves that supply the low back may be irritated the large paired lower back muscles (erector spinae) may be strained the bones, ligaments or joints may be damaged an intervertebral disc may be degenerating an irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. many lower back problems also cause back muscle spasms, which don't sound like much but can cause severe pain and disability.\n",
      "while lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. a simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.\n",
      "this data set is about to identify a person is abnormal or normal using collected physical spine details/data.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. the sting’s even more painful when you know you’re a good driver. it doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.\n",
      "try to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year! good luck\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "vgg-11\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "this dataset is consolidated from the surveys of alif ailaan & aser pakistan on primary schooling performance.\n",
      "content\n",
      "dataset contains different dimensions and measures across the board that were used to measure the performance. i have compiled individual file of each year into one for your facilitation.\n",
      "acknowledgements\n",
      "this dataset is compiled by me and i am very thankful to aser pakistan & alif ailaan for providing pdfs from which i extracted.\n",
      "inspiration\n",
      "explore this visualization to interpret performance province wise, city wise and identify different needs and room for actionable improvements.\n",
      "for starting see this: https://public.tableau.com/views/pakistaneducationalperformance-dashboard/pakistaneducationperformancedashboard?%3aembed=y&%3ashowvizhome=no&%3adisplay_count=y&%3adisplay_static_image=y&%3abootstrapwhennotified=true\n",
      "context\n",
      "this is an easy-to-use binary classification dataset for predicting whether or not it will rain tomorrow. use for binary classification modeling.\n",
      "content\n",
      "the content is daily weather observations from multiple australian weather stations. the target variable is raintomorrow, which means: did it rain the next day? yes or no. so the dataset is already set up for prediction -- no need to change the target with a lead function.\n",
      "acknowledgements\n",
      "this dataset comes from the rattle package. the rattle package is currently not available to kaggle's kernels.\n",
      "context\n",
      "what makes us, humans, able to tell apart two songs of different genres? maybe you have ever been in the diffcult situation to explain show it sounds the music style that you like to someone. then, could an automatic genre classifcation be possible?\n",
      "content\n",
      "each row is an electronic music song. the dataset contains 100 song for each genre among 23 electronic music genres, they were the top (100) songs of their genres on november 2016. the 71 columns are audio features extracted of a two random minutes sample of the file audio. these features have been extracted using pyaudioanalysis (https://github.com/tyiannak/pyaudioanalysis).\n",
      "context\n",
      "i'm interested in object recognition, and trying to learn how to build cnn models able to recognize specific objects.\n",
      "content\n",
      "so i created this data set by taking pictures around my flat with a basketball in each image. the data is made up of several pairs of image and annotation files (*.annote). the annotation file subdivides the image into 200x200 pixel windows and says whether basketball is present in it or not.\n",
      "acknowledgements\n",
      "the initial analysis has used the mnist deep cnn model given in the tensor flow example adapted for multiple channels and image resolution\n",
      "inspiration\n",
      "i'd like to some feedback both on my model and alternative way of building or cnn models. have fun!\n",
      "extra data for rossman store from http://files.fast.ai/part2/lesson14/rossmann.tgz\n",
      "context\n",
      "fortnite: battle royale has over 20 million unique players, but there are no datasets on kaggle yet! this one is small, but better than nothing! it contains the location coordinates of chests in fortnite: battle royale as of the 1st of december 2017.\n",
      "content\n",
      "there are three columns in the dataset. the first one has an identifier which is a running number starting from 1. the second column has latitudinal coordinates for chests. the third column has longitudinal coordinates for chests.\n",
      "acknowledgements\n",
      "the data originates from: http://www.fortnitechests.info/ which is updated by soumydev (soumyydev@gmail.com)\n",
      "neither soumydev or the uploader of this dataset are affiliated with epic games or any of his partners. all copyrights reserved to their respective owners.\n",
      "inspiration\n",
      "based on chest coordinates, what are the best clusters to land on at the beginning of the match?\n",
      "context\n",
      "this dataset provides the nationalities of passengers on the titanic. it is meant to be merged with the titanic dataset to allow analysis of how a passengers nationality effected survival chances aboard the titanic.\n",
      "the passengers nationality was predicted using the open api from nameprism http://www.name-prism.com/api using the passenger name variable from the titanic dataset.\n",
      "details from site; nameprism is a non-commercial nationality/ethnicity classification tool that aims to support academic research, e.g. sociology and demographic studies. in this project, we learn name embeddings for name parts (first/last names) and classify names to 39 leaf nationalities and 6 u.s. ethnicities.\n",
      "research covered at; nationality classification using name embeddings. junting ye, shuchu han, yifan hu, baris coskun, meizhu liu, hong qin and steven skiena. cikm, singapore, nov. 2017.\n",
      "content\n",
      "the data consists of two variables; passengerid and nationality in two files that constitute training and test data for the titanic dataset.\n",
      "acknowledgements\n",
      "-nameprism\n",
      "inspiration\n",
      "did a passenger nationality on-board the titanic affect their survival chances?\n",
      "lem.json\n",
      "this file contains lementized english words as key holding their original words as value\n",
      "eaxmple content\n",
      "json {\"abbreviation\": [\"abbreviations\", \"abbreviation\"]}\n",
      "stem.json\n",
      "this file contains stemmed english words as key holding their original words as value\n",
      "eaxmple content\n",
      "json {\"abandon\": [\"abandoned\", \"abandonment\", \"abandoning\", \"abandon\", \"abandonable\", \"abandoner\"]}\n",
      "context\n",
      "this data set contains weather data for los angeles (l.a.) during 2014.\n",
      "content\n",
      "day : number of day of an year, type of weather : this column tells type of weather in a day\n",
      "inspiration\n",
      "the goal is to count how many times each type of weather occurred over the course of the year. during this , how to manipulate the data with lists, and make good progress towards that goal.\n",
      "context\n",
      "births in u.s during 1994 to 2003.\n",
      "content\n",
      "the data set has the following structure:\n",
      "year - year\n",
      "month - month\n",
      "date_of_month - day number of the month\n",
      "day_of_week - day of week, where 1 is monday and 7 is sunday\n",
      "births - number of births\n",
      "acknowledgements\n",
      "data set from the centers for disease control and prevention's national national center for health statistics\n",
      "inspiration\n",
      "make a dictionary that shows total number of births on each day of week?\n"
     ]
    }
   ],
   "source": [
    "for i in df['Description'].iteritems():\n",
    "    raw = str(i[1]).lower()\n",
    "    print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the dataset\n",
    "- Tokenize\n",
    "- Stop words removal\n",
    "- Non-alphabetic words removal\n",
    "- Lowercase\n",
    "- Define them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pattern, tokenizer, stop words and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\b[^\\d\\W]+\\b'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "en_stop = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "\n",
    "for i in df['Description'].iteritems():\n",
    "    # clean and tokenize document string\n",
    "    raw = str(i[1]).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n",
    "    \n",
    "    # remove word containing only single char\n",
    "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(new_lemma_tokens)\n",
    "\n",
    "\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter low frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an index to word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus, num_topics=15, id2word = id2word, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.036358196, 'player'),\n",
      "   (0.030080475, 'game'),\n",
      "   (0.028001763, 'team'),\n",
      "   (0.0168796, 'match'),\n",
      "   (0.014556797, 'season')],\n",
      "  -1.1403886634491778),\n",
      " ([(0.01799047, 'company'),\n",
      "   (0.017193004, 'price'),\n",
      "   (0.017030902, 'name'),\n",
      "   (0.015550443, 'year'),\n",
      "   (0.015041812, 'http')],\n",
      "  -1.19307918338903),\n",
      " ([(0.045464605, 'image'),\n",
      "   (0.044691578, 'model'),\n",
      "   (0.037053447, 'trained'),\n",
      "   (0.02627516, 'feature'),\n",
      "   (0.024118548, 'pre')],\n",
      "  -1.3335498778266384),\n",
      " ([(0.025536627, 'city'),\n",
      "   (0.020053511, 'new'),\n",
      "   (0.019710885, 'others'),\n",
      "   (0.018813342, 'inspiration'),\n",
      "   (0.017759247, 'world')],\n",
      "  -1.4411230120715441),\n",
      " ([(0.028886251, 'language'),\n",
      "   (0.018455138, 'file'),\n",
      "   (0.017737139, 'question'),\n",
      "   (0.016496379, 'used'),\n",
      "   (0.015745243, 'kaggle')],\n",
      "  -1.4907629027727687),\n",
      " ([(0.031437837, 'text'),\n",
      "   (0.018139252, 'would'),\n",
      "   (0.018001739, 'like'),\n",
      "   (0.015858034, 'movie'),\n",
      "   (0.015505705, 'one')],\n",
      "  -1.5601442522557547),\n",
      " ([(0.014332684, 'information'),\n",
      "   (0.013726342, 'column'),\n",
      "   (0.010391199, 'file'),\n",
      "   (0.010074124, 'csv'),\n",
      "   (0.009440132, 'activity')],\n",
      "  -1.6449740113144817),\n",
      " ([(0.027332291, 'instance'),\n",
      "   (0.02538774, 'cell'),\n",
      "   (0.019145753, 'number'),\n",
      "   (0.016925598, 'group'),\n",
      "   (0.01553999, 'class')],\n",
      "  -1.714435322271181),\n",
      " ([(0.018252974, 'state'),\n",
      "   (0.0145009365, 'year'),\n",
      "   (0.013399562, 'crime'),\n",
      "   (0.0129979765, 'number'),\n",
      "   (0.012181871, 'population')],\n",
      "  -1.7204536700684567),\n",
      " ([(0.011357258, 'time'),\n",
      "   (0.011111142, 'word'),\n",
      "   (0.010255717, 'contains'),\n",
      "   (0.008845165, 'file'),\n",
      "   (0.008793376, 'tweet')],\n",
      "  -2.001448446553037),\n",
      " ([(0.4220194, 'university'),\n",
      "   (0.07685469, 'state'),\n",
      "   (0.052629944, 'college'),\n",
      "   (0.023360979, 'california'),\n",
      "   (0.021621106, 'texas')],\n",
      "  -2.51880602100133),\n",
      " ([(0.029725539, 'csv'),\n",
      "   (0.016636861, 'row'),\n",
      "   (0.013031646, 'column'),\n",
      "   (0.012335189, 'txt'),\n",
      "   (0.011352292, 'facility')],\n",
      "  -2.9514039358574133),\n",
      " ([(0.027528122, 'de'),\n",
      "   (0.02298573, 'restaurant'),\n",
      "   (0.014791479, 'news'),\n",
      "   (0.014265349, 'list'),\n",
      "   (0.014026198, 'number')],\n",
      "  -3.713996582948451),\n",
      " ([(0.038244065, 'csv'),\n",
      "   (0.022232909, 'review'),\n",
      "   (0.02054789, 'numeric'),\n",
      "   (0.019334015, 'score'),\n",
      "   (0.016939491, 'class')],\n",
      "  -4.571858993325582),\n",
      " ([(0.21230243, 'description'),\n",
      "   (0.18484655, 'yet'),\n",
      "   (0.0592083, 'weapon'),\n",
      "   (0.043750055, 'integer'),\n",
      "   (0.02659033, 'damage')],\n",
      "  -7.58582099537783)]\n"
     ]
    }
   ],
   "source": [
    "pprint(ldamodel.top_topics(corpus,topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the 15 topics with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.038*\"csv\" + 0.022*\"review\" + 0.021*\"numeric\" + 0.019*\"score\" + 0.017*\"class\" + 0.012*\"time\" + 0.012*\"attack\" + 0.011*\"pokemon\" + 0.011*\"activity\" + 0.011*\"new\"\n",
      "Topic #1: 0.030*\"csv\" + 0.017*\"row\" + 0.013*\"column\" + 0.012*\"txt\" + 0.011*\"facility\" + 0.011*\"coordinate\" + 0.010*\"name\" + 0.010*\"http\" + 0.010*\"number\" + 0.010*\"file\"\n",
      "Topic #2: 0.014*\"information\" + 0.014*\"column\" + 0.010*\"file\" + 0.010*\"csv\" + 0.009*\"activity\" + 0.009*\"database\" + 0.008*\"feature\" + 0.007*\"contains\" + 0.007*\"datasets\" + 0.007*\"record\"\n",
      "Topic #3: 0.036*\"player\" + 0.030*\"game\" + 0.028*\"team\" + 0.017*\"match\" + 0.015*\"season\" + 0.011*\"back\" + 0.011*\"sport\" + 0.009*\"set\" + 0.008*\"goal\" + 0.008*\"result\"\n",
      "Topic #4: 0.027*\"instance\" + 0.025*\"cell\" + 0.019*\"number\" + 0.017*\"group\" + 0.016*\"class\" + 0.015*\"attribute\" + 0.011*\"set\" + 0.010*\"woman\" + 0.010*\"car\" + 0.010*\"vehicle\"\n",
      "Topic #5: 0.212*\"description\" + 0.185*\"yet\" + 0.059*\"weapon\" + 0.044*\"integer\" + 0.027*\"damage\" + 0.026*\"strongly\" + 0.014*\"enjoy\" + 0.014*\"type\" + 0.013*\"fire\" + 0.013*\"interested\"\n",
      "Topic #6: 0.029*\"language\" + 0.018*\"file\" + 0.018*\"question\" + 0.016*\"used\" + 0.016*\"kaggle\" + 0.016*\"dog\" + 0.015*\"com\" + 0.015*\"http\" + 0.015*\"datasets\" + 0.013*\"kera\"\n",
      "Topic #7: 0.028*\"de\" + 0.023*\"restaurant\" + 0.015*\"news\" + 0.014*\"list\" + 0.014*\"number\" + 0.011*\"city\" + 0.011*\"contains\" + 0.011*\"null\" + 0.011*\"information\" + 0.010*\"vote\"\n",
      "Topic #8: 0.018*\"state\" + 0.015*\"year\" + 0.013*\"crime\" + 0.013*\"number\" + 0.012*\"population\" + 0.011*\"health\" + 0.010*\"service\" + 0.010*\"department\" + 0.009*\"day\" + 0.009*\"per\"\n",
      "Topic #9: 0.031*\"text\" + 0.018*\"would\" + 0.018*\"like\" + 0.016*\"movie\" + 0.016*\"one\" + 0.012*\"rating\" + 0.010*\"inspiration\" + 0.010*\"arabic\" + 0.009*\"make\" + 0.009*\"post\"\n",
      "Topic #10: 0.018*\"company\" + 0.017*\"price\" + 0.017*\"name\" + 0.016*\"year\" + 0.015*\"http\" + 0.012*\"time\" + 0.012*\"www\" + 0.011*\"survey\" + 0.010*\"country\" + 0.009*\"age\"\n",
      "Topic #11: 0.011*\"time\" + 0.011*\"word\" + 0.010*\"contains\" + 0.009*\"file\" + 0.009*\"tweet\" + 0.007*\"set\" + 0.007*\"user\" + 0.006*\"http\" + 0.006*\"number\" + 0.006*\"use\"\n",
      "Topic #12: 0.045*\"image\" + 0.045*\"model\" + 0.037*\"trained\" + 0.026*\"feature\" + 0.024*\"pre\" + 0.019*\"song\" + 0.018*\"network\" + 0.013*\"recognition\" + 0.012*\"large\" + 0.010*\"accuracy\"\n",
      "Topic #13: 0.026*\"city\" + 0.020*\"new\" + 0.020*\"others\" + 0.019*\"inspiration\" + 0.018*\"world\" + 0.016*\"york\" + 0.015*\"property\" + 0.014*\"research\" + 0.014*\"science\" + 0.013*\"help\"\n",
      "Topic #14: 0.422*\"university\" + 0.077*\"state\" + 0.053*\"college\" + 0.023*\"california\" + 0.022*\"texas\" + 0.017*\"institute\" + 0.012*\"north\" + 0.012*\"technology\" + 0.011*\"new\" + 0.011*\"florida\"\n"
     ]
    }
   ],
   "source": [
    "for idx in range(15):\n",
    "    print(\"Topic #%s:\" % idx, ldamodel.print_topic(idx, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.970*\"university\" + 0.174*\"state\" + 0.076*\"college\" + 0.051*\"texas\" + '\n",
      "  '0.049*\"california\" + 0.039*\"institute\" + 0.031*\"new\" + 0.028*\"technology\" + '\n",
      "  '0.027*\"florida\" + 0.027*\"north\"'),\n",
      " (1,\n",
      "  '0.389*\"player\" + 0.247*\"team\" + 0.221*\"shot\" + 0.200*\"number\" + '\n",
      "  '0.177*\"time\" + 0.173*\"file\" + 0.159*\"year\" + 0.156*\"csv\" + 0.146*\"goal\" + '\n",
      "  '0.126*\"ice\"'),\n",
      " (2,\n",
      "  '-0.437*\"player\" + -0.307*\"shot\" + -0.259*\"team\" + 0.250*\"integer\" + '\n",
      "  '0.224*\"strongly\" + -0.175*\"ice\" + -0.174*\"goal\" + 0.154*\"file\" + '\n",
      "  '-0.151*\"attempt\" + 0.133*\"csv\"'),\n",
      " (3,\n",
      "  '-0.595*\"integer\" + -0.535*\"strongly\" + -0.263*\"interested\" + -0.261*\"enjoy\" '\n",
      "  '+ -0.119*\"much\" + -0.116*\"player\" + 0.098*\"file\" + 0.093*\"year\" + '\n",
      "  '-0.090*\"shot\" + 0.088*\"csv\"'),\n",
      " (4,\n",
      "  '0.402*\"year\" + -0.325*\"date\" + -0.265*\"element\" + -0.198*\"tag\" + '\n",
      "  '-0.192*\"registration\" + -0.186*\"zero\" + -0.180*\"end\" + -0.174*\"start\" + '\n",
      "  '-0.171*\"one\" + -0.165*\"application\"'),\n",
      " (5,\n",
      "  '-0.535*\"csv\" + 0.436*\"year\" + 0.193*\"number\" + -0.174*\"file\" + 0.166*\"date\" '\n",
      "  '+ 0.155*\"total\" + 0.122*\"element\" + 0.120*\"child\" + -0.104*\"numeric\" + '\n",
      "  '-0.103*\"text\"'),\n",
      " (6,\n",
      "  '0.680*\"numeric\" + -0.471*\"csv\" + 0.431*\"text\" + 0.104*\"word\" + '\n",
      "  '-0.096*\"year\" + -0.063*\"file\" + 0.063*\"reading\" + 0.060*\"language\" + '\n",
      "  '0.058*\"real\" + 0.053*\"use\"'),\n",
      " (7,\n",
      "  '0.552*\"csv\" + 0.488*\"numeric\" + 0.282*\"year\" + 0.198*\"text\" + '\n",
      "  '-0.139*\"image\" + -0.125*\"http\" + -0.112*\"model\" + -0.098*\"word\" + '\n",
      "  '0.096*\"total\" + -0.088*\"trained\"'),\n",
      " (8,\n",
      "  '0.367*\"de\" + 0.242*\"value\" + -0.240*\"year\" + 0.215*\"name\" + -0.198*\"image\" '\n",
      "  '+ 0.186*\"el\" + 0.179*\"per\" + 0.169*\"en\" + 0.159*\"number\" + -0.155*\"total\"'),\n",
      " (9,\n",
      "  '-0.410*\"de\" + -0.267*\"el\" + -0.252*\"en\" + -0.235*\"com\" + -0.217*\"http\" + '\n",
      "  '-0.209*\"per\" + 0.198*\"name\" + 0.181*\"value\" + 0.178*\"station\" + '\n",
      "  '-0.149*\"year\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsamodel = LsiModel(corpus, num_topics=10, id2word = id2word)\n",
    "pprint(lsamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.970*\"university\" + 0.174*\"state\" + 0.076*\"college\" + 0.051*\"texas\" + 0.049*\"california\" + 0.039*\"institute\" + 0.031*\"new\" + 0.028*\"technology\" + 0.027*\"florida\" + 0.027*\"north\"\n",
      "Topic #1: 0.389*\"player\" + 0.247*\"team\" + 0.221*\"shot\" + 0.200*\"number\" + 0.177*\"time\" + 0.173*\"file\" + 0.159*\"year\" + 0.156*\"csv\" + 0.146*\"goal\" + 0.126*\"ice\"\n",
      "Topic #2: -0.437*\"player\" + -0.307*\"shot\" + -0.259*\"team\" + 0.250*\"integer\" + 0.224*\"strongly\" + -0.175*\"ice\" + -0.174*\"goal\" + 0.154*\"file\" + -0.151*\"attempt\" + 0.133*\"csv\"\n",
      "Topic #3: -0.595*\"integer\" + -0.535*\"strongly\" + -0.263*\"interested\" + -0.261*\"enjoy\" + -0.119*\"much\" + -0.116*\"player\" + 0.098*\"file\" + 0.093*\"year\" + -0.090*\"shot\" + 0.088*\"csv\"\n",
      "Topic #4: 0.402*\"year\" + -0.325*\"date\" + -0.265*\"element\" + -0.198*\"tag\" + -0.192*\"registration\" + -0.186*\"zero\" + -0.180*\"end\" + -0.174*\"start\" + -0.171*\"one\" + -0.165*\"application\"\n",
      "Topic #5: -0.535*\"csv\" + 0.436*\"year\" + 0.193*\"number\" + -0.174*\"file\" + 0.166*\"date\" + 0.155*\"total\" + 0.122*\"element\" + 0.120*\"child\" + -0.104*\"numeric\" + -0.103*\"text\"\n",
      "Topic #6: 0.680*\"numeric\" + -0.471*\"csv\" + 0.431*\"text\" + 0.104*\"word\" + -0.096*\"year\" + -0.063*\"file\" + 0.063*\"reading\" + 0.060*\"language\" + 0.058*\"real\" + 0.053*\"use\"\n",
      "Topic #7: 0.552*\"csv\" + 0.488*\"numeric\" + 0.282*\"year\" + 0.198*\"text\" + -0.139*\"image\" + -0.125*\"http\" + -0.112*\"model\" + -0.098*\"word\" + 0.096*\"total\" + -0.088*\"trained\"\n",
      "Topic #8: 0.367*\"de\" + 0.242*\"value\" + -0.240*\"year\" + 0.215*\"name\" + -0.198*\"image\" + 0.186*\"el\" + 0.179*\"per\" + 0.169*\"en\" + 0.159*\"number\" + -0.155*\"total\"\n",
      "Topic #9: -0.410*\"de\" + -0.267*\"el\" + -0.252*\"en\" + -0.235*\"com\" + -0.217*\"http\" + -0.209*\"per\" + 0.198*\"name\" + 0.181*\"value\" + 0.178*\"station\" + -0.149*\"year\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, lsamodel.print_topic(idx, 10))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the topics and documents with the trained Topic Model\n",
    "- Use pyLDAvis from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable the notebook for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subashgandyer/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subashgandyer/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el310631406290521664322178703634\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el310631406290521664322178703634_data = {\"mdsDat\": {\"x\": [0.110351547300138, 0.0653611477540671, -0.0009030825069310622, 0.027218113193265052, 0.08776349303651636, 0.07011395753404934, -0.013335395178232262, 0.011622980035050048, 0.03334052087025681, 0.05435434012636712, 0.06605749169815284, 0.09074319385643478, 0.0012030017589216267, -0.4535510776045753, -0.15034023187348097], \"y\": [-0.0038923063699550396, 0.05057820494983944, 0.0976070204465612, 0.04584782400536584, -0.011657030908634076, -0.0012160275836988765, 0.07265635498541272, -0.16685137281183374, 0.04401948261189273, -0.03324718375952712, 0.018123116006759175, 0.010414373528111696, 0.1070316792416976, 0.12018427684167932, -0.3495984111836701], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [24.790895566900108, 15.858511401015244, 11.5948945173266, 9.469565753973765, 5.609096391892285, 5.5857509661783, 4.343613927611283, 3.963539652000077, 3.6962918240271465, 3.579707994730514, 3.256439722061371, 3.111094564079571, 3.103238489852169, 1.176575475749058, 0.860783752602519]}, \"tinfo\": {\"Term\": [\"university\", \"description\", \"state\", \"csv\", \"yet\", \"model\", \"image\", \"player\", \"text\", \"language\", \"number\", \"name\", \"trained\", \"game\", \"feature\", \"new\", \"column\", \"team\", \"city\", \"company\", \"http\", \"file\", \"price\", \"word\", \"type\", \"inspiration\", \"activity\", \"time\", \"class\", \"instance\", \"solar\", \"vector\", \"instrument\", \"tag\", \"simulation\", \"ground\", \"translation\", \"permission\", \"tweet\", \"notice\", \"encoded\", \"observed\", \"design\", \"clear\", \"debate\", \"wiki\", \"graph\", \"deal\", \"intelligence\", \"expert\", \"modify\", \"shall\", \"warranty\", \"hard\", \"presidential\", \"granted\", \"token\", \"facebook\", \"array\", \"embedded\", \"sentiment\", \"url\", \"english\", \"wikipedia\", \"software\", \"almost\", \"space\", \"word\", \"otherwise\", \"sun\", \"timestamp\", \"twitter\", \"message\", \"numerical\", \"subject\", \"parameter\", \"conference\", \"start\", \"user\", \"output\", \"corpus\", \"character\", \"sentence\", \"version\", \"json\", \"come\", \"medium\", \"contains\", \"value\", \"time\", \"following\", \"line\", \"set\", \"based\", \"many\", \"file\", \"first\", \"analysis\", \"used\", \"use\", \"number\", \"original\", \"http\", \"one\", \"id\", \"inspiration\", \"learning\", \"information\", \"model\", \"name\", \"find\", \"available\", \"column\", \"administration\", \"sensor\", \"job\", \"trump\", \"journal\", \"pakistan\", \"denotes\", \"ieee\", \"president\", \"employment\", \"conflict\", \"extraction\", \"vol\", \"agreement\", \"disclosure\", \"commission\", \"band\", \"fatality\", \"asset\", \"debt\", \"press\", \"worker\", \"retrieved\", \"disaster\", \"employee\", \"society\", \"wang\", \"former\", \"executive\", \"partial\", \"employer\", \"left\", \"government\", \"activity\", \"salary\", \"federal\", \"paper\", \"member\", \"international\", \"label\", \"database\", \"record\", \"speech\", \"public\", \"specific\", \"report\", \"column\", \"found\", \"information\", \"raw\", \"feature\", \"datasets\", \"collection\", \"right\", \"part\", \"published\", \"csv\", \"source\", \"please\", \"file\", \"original\", \"open\", \"also\", \"contains\", \"available\", \"see\", \"new\", \"use\", \"center\", \"crime\", \"violent\", \"caput\", \"route\", \"housing\", \"household\", \"nominal\", \"fbi\", \"monitor\", \"surface\", \"winter\", \"murder\", \"bike\", \"unemployment\", \"dc\", \"producer\", \"involving\", \"poverty\", \"transport\", \"percent\", \"mortality\", \"transportation\", \"prevention\", \"govt\", \"birth\", \"accident\", \"arrest\", \"illness\", \"agriculture\", \"population\", \"justice\", \"death\", \"plane\", \"health\", \"district\", \"bureau\", \"station\", \"drug\", \"department\", \"police\", \"service\", \"plan\", \"state\", \"considered\", \"food\", \"enforcement\", \"rate\", \"area\", \"census\", \"law\", \"mean\", \"per\", \"statistic\", \"year\", \"united\", \"variable\", \"national\", \"number\", \"day\", \"may\", \"value\", \"total\", \"month\", \"time\", \"file\", \"http\", \"set\", \"company\", \"formula\", \"nationality\", \"exchange\", \"headline\", \"quantity\", \"usd\", \"forecasting\", \"sold\", \"forecast\", \"analyse\", \"correlate\", \"equity\", \"bitcoin\", \"occupation\", \"opened\", \"trade\", \"respondent\", \"club\", \"gdp\", \"determining\", \"corruption\", \"employed\", \"au\", \"decrease\", \"policing\", \"finished\", \"voter\", \"hint\", \"exam\", \"price\", \"revenue\", \"demand\", \"education\", \"british\", \"mile\", \"educational\", \"passenger\", \"market\", \"product\", \"survey\", \"close\", \"sex\", \"higher\", \"age\", \"html\", \"stock\", \"www\", \"country\", \"region\", \"name\", \"gov\", \"transaction\", \"year\", \"student\", \"high\", \"http\", \"table\", \"date\", \"time\", \"code\", \"open\", \"inspiration\", \"file\", \"state\", \"using\", \"csv\", \"com\", \"wine\", \"nice\", \"film\", \"rated\", \"sparse\", \"fan\", \"tv\", \"apart\", \"everyday\", \"flavor\", \"conversation\", \"movie\", \"episode\", \"director\", \"subreddit\", \"tar\", \"arabic\", \"gz\", \"bar\", \"watch\", \"bad\", \"speaker\", \"prefer\", \"cast\", \"cool\", \"automatic\", \"impossible\", \"book\", \"upcoming\", \"difficulty\", \"genre\", \"comment\", \"sometimes\", \"rating\", \"text\", \"feel\", \"ever\", \"reddit\", \"would\", \"post\", \"good\", \"improving\", \"figure\", \"like\", \"go\", \"mainly\", \"google\", \"interesting\", \"take\", \"kernel\", \"script\", \"make\", \"one\", \"need\", \"know\", \"help\", \"great\", \"speech\", \"want\", \"system\", \"people\", \"see\", \"inspiration\", \"could\", \"get\", \"day\", \"file\", \"find\", \"time\", \"word\", \"player\", \"match\", \"league\", \"penalty\", \"football\", \"shot\", \"shooting\", \"win\", \"baseball\", \"scored\", \"opponent\", \"premier\", \"pipeline\", \"nba\", \"athlete\", \"winner\", \"awarded\", \"severity\", \"went\", \"predictor\", \"award\", \"fifa\", \"prize\", \"disability\", \"vast\", \"participating\", \"soccer\", \"card\", \"recommend\", \"offensive\", \"season\", \"ball\", \"game\", \"pain\", \"team\", \"betting\", \"sport\", \"played\", \"play\", \"stats\", \"back\", \"direction\", \"allowed\", \"mass\", \"percentage\", \"goal\", \"competition\", \"problem\", \"lower\", \"statistic\", \"score\", \"base\", \"result\", \"set\", \"may\", \"point\", \"com\", \"time\", \"source\", \"every\", \"contains\", \"year\", \"world\", \"number\", \"facility\", \"gas\", \"lat\", \"lon\", \"gene\", \"protein\", \"lab\", \"emission\", \"museum\", \"da\", \"dioxide\", \"matter\", \"postcode\", \"carbon\", \"driver\", \"tract\", \"sort\", \"li\", \"interactive\", \"molecular\", \"ray\", \"manner\", \"grouped\", \"breakdown\", \"proper\", \"partner\", \"intelligent\", \"unzip\", \"uc\", \"neither\", \"street\", \"txt\", \"coordinate\", \"energy\", \"driving\", \"refers\", \"chemical\", \"chest\", \"ca\", \"row\", \"csv\", \"california\", \"county\", \"school\", \"latitude\", \"column\", \"small\", \"id\", \"election\", \"longitude\", \"name\", \"number\", \"http\", \"file\", \"www\", \"user\", \"code\", \"inspiration\", \"information\", \"source\", \"set\", \"unit\", \"time\", \"one\", \"song\", \"transferable\", \"convolutional\", \"layer\", \"bird\", \"benefit\", \"imagenet\", \"vision\", \"vgg\", \"horizontal\", \"cnn\", \"mnist\", \"whichever\", \"residual\", \"pre\", \"convolution\", \"beneficial\", \"handwritten\", \"secured\", \"pushing\", \"karen\", \"visual\", \"convnet\", \"localisation\", \"simonyan\", \"generalise\", \"zisserman\", \"trained\", \"inception\", \"coco\", \"bias\", \"achieve\", \"image\", \"architecture\", \"depth\", \"saving\", \"learned\", \"model\", \"compute\", \"network\", \"else\", \"art\", \"feature\", \"accuracy\", \"deep\", \"recognition\", \"large\", \"scale\", \"previously\", \"spent\", \"weight\", \"digit\", \"representation\", \"using\", \"use\", \"contains\", \"learn\", \"time\", \"training\", \"example\", \"http\", \"like\", \"different\", \"pokemon\", \"exercise\", \"hotel\", \"float\", \"dot\", \"execution\", \"mistake\", \"potentially\", \"planet\", \"star\", \"leader\", \"zurich\", \"racing\", \"reviewer\", \"launch\", \"baseline\", \"numeric\", \"leaderboard\", \"km\", \"specification\", \"attack\", \"amazon\", \"fit\", \"correspond\", \"supervised\", \"argentina\", \"int\", \"defense\", \"horse\", \"boolean\", \"ranking\", \"review\", \"specie\", \"listing\", \"participant\", \"score\", \"performed\", \"recommendation\", \"corresponds\", \"weight\", \"csv\", \"class\", \"race\", \"share\", \"taken\", \"string\", \"activity\", \"system\", \"id\", \"new\", \"time\", \"used\", \"number\", \"file\", \"one\", \"year\", \"cell\", \"investment\", \"cancer\", \"frame\", \"diagnosis\", \"motor\", \"breast\", \"critical\", \"divide\", \"diabetes\", \"treatment\", \"tested\", \"revised\", \"clinical\", \"coin\", \"coded\", \"instance\", \"fuel\", \"carry\", \"highway\", \"selecting\", \"initially\", \"dependent\", \"linear\", \"logistic\", \"profit\", \"averaged\", \"consistent\", \"woman\", \"robust\", \"vehicle\", \"normal\", \"yes\", \"risk\", \"medical\", \"group\", \"attribute\", \"car\", \"growth\", \"class\", \"size\", \"applied\", \"family\", \"missing\", \"removed\", \"within\", \"number\", \"patient\", \"body\", \"sample\", \"two\", \"set\", \"total\", \"different\", \"month\", \"result\", \"method\", \"status\", \"information\", \"variable\", \"point\", \"value\", \"restaurant\", \"null\", \"startup\", \"de\", \"marketing\", \"joining\", \"sheet\", \"assignment\", \"earned\", \"scraper\", \"pt\", \"menu\", \"recall\", \"lang\", \"datafiniti\", \"nan\", \"el\", \"searching\", \"determines\", \"curated\", \"la\", \"filled\", \"fantasy\", \"newspaper\", \"un\", \"ch\", \"crypto\", \"medicine\", \"derive\", \"tower\", \"vote\", \"en\", \"business\", \"email\", \"news\", \"idea\", \"excel\", \"list\", \"object\", \"positive\", \"city\", \"net\", \"non\", \"double\", \"number\", \"page\", \"web\", \"patient\", \"contains\", \"information\", \"review\", \"database\", \"game\", \"name\", \"website\", \"product\", \"com\", \"collected\", \"inspiration\", \"user\", \"dog\", \"kera\", \"transcript\", \"talk\", \"join\", \"ex\", \"truly\", \"chinese\", \"precipitation\", \"classic\", \"german\", \"retrieve\", \"numerous\", \"purchase\", \"stack\", \"imdb\", \"overflow\", \"awesome\", \"snow\", \"youtube\", \"exists\", \"command\", \"wind\", \"calculation\", \"campaign\", \"scrapped\", \"matching\", \"french\", \"centre\", \"humidity\", \"extension\", \"language\", \"answer\", \"package\", \"combine\", \"video\", \"question\", \"kernel\", \"kaggle\", \"train\", \"park\", \"transaction\", \"datasets\", \"github\", \"customer\", \"response\", \"zip\", \"used\", \"com\", \"file\", \"weather\", \"http\", \"provides\", \"information\", \"code\", \"inspiration\", \"www\", \"one\", \"using\", \"could\", \"date\", \"also\", \"use\", \"answered\", \"owe\", \"neighborhood\", \"borough\", \"nyc\", \"fraud\", \"plate\", \"robot\", \"intersection\", \"advertisement\", \"fair\", \"theft\", \"bill\", \"predicts\", \"luck\", \"friendly\", \"chicago\", \"york\", \"queen\", \"receiving\", \"estate\", \"inside\", \"ny\", \"island\", \"hand\", \"approximate\", \"planning\", \"describing\", \"pay\", \"angeles\", \"largest\", \"attribution\", \"along\", \"property\", \"others\", \"acquired\", \"insurance\", \"front\", \"city\", \"past\", \"citation\", \"want\", \"started\", \"science\", \"easy\", \"world\", \"thanks\", \"new\", \"without\", \"include\", \"help\", \"question\", \"community\", \"inspiration\", \"research\", \"see\", \"make\", \"represents\", \"time\", \"column\", \"year\", \"university\", \"texas\", \"florida\", \"carolina\", \"massachusetts\", \"georgia\", \"virginia\", \"southern\", \"college\", \"pennsylvania\", \"eastern\", \"hill\", \"carnegie\", \"boston\", \"northern\", \"enrollment\", \"western\", \"san\", \"lee\", \"christian\", \"st\", \"reserve\", \"mexico\", \"francisco\", \"harvard\", \"tech\", \"california\", \"north\", \"john\", \"institute\", \"technology\", \"state\", \"michigan\", \"washington\", \"south\", \"west\", \"new\", \"school\", \"international\", \"community\", \"science\", \"set\", \"yet\", \"weapon\", \"strongly\", \"rock\", \"damage\", \"description\", \"enjoy\", \"integer\", \"promptcloud\", \"magazine\", \"extracting\", \"swedish\", \"hit\", \"categorical\", \"mine\", \"fire\", \"crawling\", \"crawled\", \"impact\", \"interested\", \"credit\", \"battle\", \"cost\", \"technical\", \"mentioned\", \"situation\", \"habit\", \"info\", \"bigger\", \"leading\", \"previously\", \"go\", \"range\", \"type\", \"size\", \"rate\", \"name\", \"www\", \"different\", \"http\", \"column\", \"time\", \"much\", \"following\", \"created\"], \"Freq\": [1119.0, 606.0, 866.0, 1246.0, 359.0, 755.0, 540.0, 458.0, 635.0, 433.0, 1255.0, 966.0, 349.0, 438.0, 671.0, 600.0, 1076.0, 412.0, 435.0, 384.0, 1355.0, 1624.0, 402.0, 712.0, 510.0, 1217.0, 445.0, 1697.0, 417.0, 239.0, 179.98873722800332, 141.35005235279155, 121.5083557599287, 207.15623650236955, 89.94341959934363, 84.68178004027844, 66.47519060385254, 57.16115430236318, 490.9938471879305, 53.114471285874764, 52.3062553268903, 51.45382786966694, 49.06672872748928, 49.06295532827915, 48.66543535929418, 48.64688362570985, 44.611572808908114, 43.999810868232174, 41.57970992107635, 41.5703495509427, 40.16900788820812, 40.16898188717997, 40.1682018563355, 42.106386747263116, 38.941522100192024, 38.34374546253174, 38.144853847834256, 38.136741527051754, 37.330537397620326, 36.929783550759325, 111.56777268313216, 236.91804036236158, 219.50647786339155, 150.4742291467367, 190.35642619368988, 55.96122635405654, 207.73825651697152, 620.4104286599008, 68.83917808213423, 79.2190940313079, 78.07149315215119, 154.98980170435064, 86.34295972769183, 97.3769185394403, 129.70364582324973, 133.42737006909903, 144.1143126589135, 144.93212299728495, 380.7670645310705, 116.23237013358228, 194.9183975860358, 145.9217481296652, 99.15392630700059, 196.33891175690178, 156.29548233439644, 158.99949825829708, 175.46489634343797, 572.6462279377943, 324.72868662623875, 634.1527520675079, 293.60178978688043, 177.17000476790915, 403.07482863872815, 276.68042467160404, 307.4743523444859, 493.8855775345531, 262.43352531173696, 266.493403850011, 335.9179690798925, 339.92355747139135, 355.82674832537987, 229.31380567092432, 358.6643965334524, 302.78621096304687, 227.43749347660582, 320.4652720396141, 224.5495332790649, 302.63920114989224, 254.12533878419282, 256.9880259823729, 216.0854835882814, 224.88302246610434, 222.65452634550726, 118.33458735987719, 147.89627913846925, 167.8079563411384, 108.88532021826691, 86.36459167442392, 77.56260797681936, 74.68943059422837, 67.72164673924789, 59.938444420036504, 57.68503596608509, 56.254347697534726, 63.41147316611613, 49.687809221845654, 48.66762238687188, 45.59877883606731, 44.56490751113284, 45.9574072789846, 43.544616722269964, 42.923483918605434, 42.31618945665747, 42.10363286054345, 41.90728475481741, 39.6383954877017, 39.44646750134005, 81.38209008632525, 38.569329534074605, 35.551460812327555, 34.731264627353774, 33.91055075201229, 30.634146629003702, 35.148065666609625, 127.64085553164591, 211.21939981516422, 337.1853336530602, 69.76468578691188, 145.29059590330215, 222.01523573826773, 116.02575485041493, 148.9513196333794, 205.94512876745978, 320.08257369689795, 256.4355992322663, 183.01848939378272, 248.2324564663484, 171.54859982115326, 164.4274590308216, 490.2813696915293, 235.10639016525093, 511.9388734566235, 128.65902981932678, 300.580591245626, 257.97153873444944, 169.06889206694925, 149.5842408080675, 200.00734915273935, 152.11920213340426, 359.8304159971963, 230.6403980286742, 186.48800449841087, 371.1558013476701, 190.89057644632743, 184.30557160177918, 207.78827280270173, 259.1138505983341, 192.87865715190372, 183.4223627273905, 170.36746741692426, 175.77905739397002, 223.76503730812686, 349.9335410238517, 107.24382282554143, 87.35277129284844, 68.02160681027331, 70.02043033313647, 64.61807361967257, 60.897524452399445, 55.4568123141729, 54.138487478331776, 51.507772317519574, 48.69676902704274, 48.326138757750726, 48.13616144461126, 46.82486559910393, 40.25411861566913, 38.747017353877915, 35.73482137597901, 35.35438943504506, 34.05956854568775, 113.70661461098192, 30.87462722576183, 30.846830487603558, 30.312311039278622, 30.12222123797205, 96.62884957453595, 98.60330876844796, 27.871056354081347, 28.550804106582426, 28.53249893755137, 318.13317266545096, 71.7261644164929, 143.2146078934614, 45.54625212647218, 276.3679092944162, 174.8956171858998, 135.51479893545277, 149.9529531252229, 90.91776452611676, 252.4706312130895, 136.2467381572706, 263.13122601640856, 75.00956024462374, 476.6818513159345, 86.61969502847661, 105.3308794579331, 66.46164523060418, 191.91229202694467, 171.94618960524642, 161.69503956931263, 99.04704114525563, 183.81658552513528, 231.10561378437004, 161.68256250342597, 378.6962667568216, 150.94953748137317, 190.44581115436188, 178.43286536476228, 339.4460199063988, 232.21125693864056, 171.68646962850082, 181.64448158139615, 171.36869359664527, 141.033808918427, 146.486067814996, 144.98014919906979, 139.79131934854016, 136.73030350611697, 383.7079019198331, 92.53853018878931, 112.99987475001033, 52.96030015173335, 49.002593350195895, 40.96928420550454, 38.81514678636228, 38.476717903462195, 60.93699301783622, 37.68802051673137, 37.196393476261385, 35.05492407980829, 34.57589802231508, 44.74267152520984, 30.455647230710262, 29.27896868181481, 28.988709073742214, 28.983812691785523, 28.48698883212925, 26.364752384274798, 22.569681442040196, 22.413166060183205, 21.918785425600657, 21.26512091733469, 21.25479431867451, 20.454237110231652, 19.945347343821556, 18.469324064348672, 17.97545119351366, 35.856575028744004, 366.69924159196336, 77.99199020172817, 38.47108160171692, 179.3623569301377, 30.838230454551443, 58.227481240599175, 101.57596014563973, 70.00131334821145, 145.37163365955035, 182.6952337485933, 225.52266511123443, 78.25281937606209, 75.14140204010204, 75.45801818695266, 197.84191820686337, 105.70115697824744, 99.5538338520002, 253.24236457145565, 215.9738661600917, 115.43507492186637, 363.24185961284024, 145.26562152769293, 89.14051063351032, 331.6660358960167, 109.21867606233559, 122.13935404683032, 320.81775902354843, 139.83374621205866, 191.65706277348764, 254.31730414942038, 142.80626719575844, 123.62760586643337, 147.70856851075834, 150.694298800751, 129.04393790060269, 124.72354128913733, 131.16494528706332, 117.76831514003604, 81.60588738564375, 30.154018563130087, 30.656847377101386, 23.711389445183972, 23.880349174591654, 20.419052882651073, 19.549947577929824, 19.692469570391488, 19.02654774556378, 18.167786043643005, 24.904821776318762, 200.34126433363903, 19.96760528685657, 15.376961996985797, 14.691606023515195, 13.27987116843896, 123.2452348321747, 12.590995752127247, 11.90160999454271, 18.687960577993774, 37.564347389968965, 52.897970088316285, 12.24253708420223, 9.817731416210302, 9.12708157123674, 49.80107805530922, 8.661524583509015, 106.20314250772503, 9.02611047687248, 8.896251419162999, 101.6125065714749, 97.22443052808754, 55.15752592851637, 146.74888890628534, 397.1675215301401, 56.06418029097954, 33.9087684405646, 44.08916498442567, 229.16086842133592, 117.91726017810835, 110.09719643322657, 38.38833529057957, 33.9729096612861, 227.42360201678434, 86.1026440592805, 38.05880778364527, 70.18004341037143, 78.51705492086455, 65.43948185639337, 77.8687656076215, 73.36803209593948, 118.79148213207499, 195.8901471711788, 97.26391652950758, 81.2577493355789, 104.37257336441871, 61.3960670712295, 85.08199092811684, 74.2256172185569, 104.50992723229045, 93.28184270632327, 106.31550583119386, 127.30875147473611, 87.85471183146807, 84.66395818453715, 98.22455235040793, 117.42795614322893, 91.15618747373964, 99.88177018205685, 79.31908785167485, 457.41675693789676, 212.35959065229892, 82.99688927120418, 77.31493310912737, 74.82733441079247, 73.72639242646628, 68.59516209745355, 69.22568535379025, 50.45886747556991, 33.98767107947918, 33.57255868541563, 29.671539120056746, 26.921252178506766, 25.132925195660185, 23.885990131728278, 23.882401854674132, 21.802610111241126, 21.791772049934735, 21.791060252935424, 21.77798720763531, 24.940367996714723, 18.34891980948048, 17.778744050180066, 16.2307437537883, 15.702796889305068, 15.698880541205975, 15.026421310627939, 51.654827036289795, 14.599575982898779, 24.547166061724635, 183.1367794915901, 40.217494169799785, 378.4377303205516, 78.9784642098395, 352.28578130880703, 28.599839396817504, 133.09714580258594, 72.36541313013645, 78.50338804469402, 65.34687795569796, 135.9620730699057, 68.56338607338553, 37.34520580840895, 55.79169746170453, 85.7214138481083, 100.86548963723149, 65.82993327732954, 95.20212807330955, 81.43580421268926, 90.56655450910141, 95.37272501668365, 52.44295327925597, 100.70071595490526, 112.64172868077235, 90.67881339058954, 78.19449158063885, 91.58285416383045, 95.78279624763685, 80.80492359325002, 76.91372565479934, 83.91118812301129, 78.38687009803306, 73.8776214613653, 74.10966142472935, 111.06135086113531, 73.73981520270112, 72.66037421644252, 58.146123775350446, 51.45144942025879, 50.345982055186035, 42.82146574566633, 42.269039568918494, 39.061175627953155, 39.062045755443705, 36.56098045271683, 43.73308601728196, 33.50106779681318, 32.105747753279076, 32.07004291449808, 31.539909768509396, 30.953519007961663, 29.304329018867218, 28.05050719447668, 27.50733580305371, 25.69355049336502, 25.36385189817027, 24.855781723054488, 20.247509087209796, 18.584559922443475, 18.57356943777883, 18.02021277129307, 17.87272502273485, 16.91773960147248, 16.90621496786403, 72.29604135817503, 120.67718898251869, 110.08747319535303, 88.51147304483213, 33.88396033778481, 45.45542821674015, 51.709818061614506, 55.53241568554875, 42.95324678189189, 162.76116473442917, 290.80985514608784, 67.52239438707178, 63.706484444583225, 63.96159580240102, 54.61689122932893, 127.49074279764098, 61.79337932083598, 90.8160990941955, 48.19796073155607, 52.40799287084979, 99.24496487179725, 96.97106206744047, 98.0285630851416, 93.50413702764003, 70.50789291996833, 66.99969832552456, 66.15252215590523, 68.27011635121389, 67.95902527295706, 62.062809059296676, 57.516247140661214, 54.04719917200949, 55.99570618439049, 54.97450907230286, 171.984907959937, 72.39445299143239, 57.27784481438521, 57.04578365107429, 52.97237527484872, 52.95718554479107, 48.324496628821166, 44.47596193827388, 42.39232090230097, 38.54913213207502, 38.54817601879827, 37.97203047225483, 37.62367267837665, 36.80857779591541, 215.30917741517555, 24.4849240742723, 38.533435246279275, 22.370654157329295, 21.68792276549902, 20.7589381637194, 20.74674979794791, 34.616964550404376, 19.833058851582102, 19.833058851582102, 19.833058851582102, 19.833058851582102, 19.833058851582102, 330.78057414222144, 17.73491399290226, 16.346070082610836, 45.355764904500134, 37.93857897861118, 405.86799084687357, 89.0346650881152, 88.79454763119406, 39.56401727624962, 75.51143215782123, 398.96708114834627, 39.33501567542431, 160.52551620675865, 42.2502216843041, 63.26355041336088, 234.56150761200718, 92.30737432221605, 85.79430573888263, 111.66525942480129, 104.18388919894242, 58.20888713821698, 44.78352695134864, 43.44218316434778, 62.49091111740176, 47.6922480180035, 53.18782916624386, 81.84094362007488, 82.67860693288176, 84.08730266470857, 52.741918719037045, 81.34359012156446, 55.10137765028224, 58.766891474587574, 64.13916726624065, 59.17640726206403, 54.12618367813567, 91.39154431530079, 64.91497751225918, 48.38523966261685, 84.77163211622312, 38.11630558374673, 36.1367781358572, 32.16813998553575, 28.02573556565373, 23.232619633556713, 70.0439686831501, 21.916116164074182, 18.754975757140077, 15.300816293917144, 14.644117185973707, 12.650860330332605, 11.988604911882337, 171.06518739921532, 11.33508956113669, 16.411602527769205, 22.866905272508443, 95.9462419001659, 22.92234044578068, 12.035531651535269, 32.62674058397713, 21.069252256345926, 9.890212349373881, 40.643171775901024, 49.36716271815061, 51.01625396694, 27.828887298704228, 73.9859427457925, 185.0933042644623, 61.297736991988806, 57.27194434259029, 86.79734294249594, 160.95944389476352, 60.30428069120277, 38.322171103959015, 26.241092825479992, 88.56684902516128, 318.38930380989507, 141.02456868930207, 65.90763133177893, 84.32925171883569, 83.07514019862714, 59.62764942175863, 91.22772958878646, 87.23709492298218, 86.90093668354848, 88.85029161116962, 96.45054904029612, 71.34059232686668, 71.19111371495501, 72.51896822379844, 60.99537213032455, 58.01469696700007, 204.69150103252548, 69.35839611051327, 66.97934610289269, 56.59704785276337, 38.63929799945087, 33.56888436135728, 33.155016718134156, 28.98265539290266, 28.17069253573578, 24.74884504362565, 24.226823063280335, 18.93356613700264, 20.518109585941072, 16.32025556972961, 14.992159581378251, 13.84524942130405, 220.36966367194952, 12.083791765467318, 11.76866700088127, 11.668714251895409, 10.94249262552118, 22.927106935181925, 12.585986514433374, 39.298920439945746, 5.01672974779172, 26.413262462262633, 12.161281661708982, 18.43660320907938, 83.12462194422584, 10.454049475065707, 79.12131583098927, 42.046093915988294, 51.14545653615395, 61.960439714807066, 40.76235858894381, 136.46452986795578, 118.82830329448745, 83.11070045701616, 31.197853851223176, 125.29291182683045, 69.90376699295055, 33.080926473097556, 43.30382219289755, 53.61579590771824, 43.22104415891726, 68.55308988679374, 154.3647841292702, 40.47470041552664, 56.231946713684394, 68.52738694681601, 77.20680347505508, 92.4275844376098, 74.47236508116335, 76.64326858098879, 62.254191354713264, 53.96351020557067, 46.37844346518578, 46.40967295024783, 57.37237482206833, 49.051778244099765, 48.17931999227013, 50.41712397229762, 168.58911150109387, 80.58090445297566, 71.27141687717786, 201.90533761564834, 35.79706209724434, 29.93675104155574, 28.722454015886072, 27.89165498065146, 25.427803773367327, 22.86644636196928, 22.44580079266824, 21.874240729406665, 19.568211659006476, 18.7402918044811, 18.556131824134184, 19.419141480091003, 45.882596459226, 12.885281446847937, 12.06881086104444, 12.056947475286982, 69.40993604060255, 11.94204237873175, 19.53143465084862, 17.78678092618338, 22.519868826417806, 18.819657120887875, 15.622670190801694, 17.30834169570634, 11.196734176632285, 6.478903287223189, 76.64435060521733, 66.78147971554652, 76.13591393207216, 35.096693705076255, 108.48828051168717, 54.412345203351585, 29.69875584658325, 104.629372327326, 50.48150873223295, 47.66602178461635, 82.46877948708476, 42.86957312408842, 72.31875557569406, 28.54077532235441, 102.87531259593094, 53.5043854912687, 48.922587778800285, 33.732170388189715, 80.83947742229353, 78.04106405043669, 46.79409110847859, 55.55726182934456, 52.262232477622106, 59.566929107046725, 46.23742218150679, 43.72320409650126, 53.130764832519425, 45.51659896932965, 46.10617525826291, 44.77185772333663, 108.70491913488812, 89.36873945727852, 56.43051368753887, 59.96166100726222, 33.227591433695494, 28.41178155650028, 20.910917592457736, 20.336749511665438, 19.867178820690334, 17.51846557631468, 15.98924103468419, 15.176821530998135, 15.163745225791859, 14.222704802137338, 12.71774361808985, 87.15335395015394, 10.374530089852728, 9.429234768633894, 8.300637873011961, 63.06920456042554, 28.68572321678549, 19.224516351691534, 21.709006183488526, 28.389407451746834, 40.288111023851556, 12.622591676680855, 12.384508296137223, 24.078488157156247, 13.180406626439451, 12.639477485649659, 24.020159515392702, 202.4102802089306, 67.43211909480702, 49.01047809390613, 23.18069824516965, 52.324687634056346, 124.28678472589273, 71.50224838498158, 110.32927196148025, 68.11525856095713, 30.77630057066239, 49.36035531869801, 103.0232343403969, 76.2616123473263, 53.26607719424413, 46.158574268176615, 45.337121264562036, 115.5925933003687, 107.38483716555994, 129.3179139553667, 48.964630267467356, 104.70878721201684, 46.01141811384152, 81.5298085458475, 63.92699713012429, 75.941614031274, 60.21508844899924, 66.23845718010443, 60.61528375303903, 53.95151378300565, 57.82837805574596, 51.87155735824826, 52.4731326569021, 84.63013943918217, 62.450383686567115, 48.59392656834539, 41.3879956458131, 54.24877572955226, 33.72573363358794, 24.0162991267366, 20.88638261775271, 14.546522576452343, 14.57130400461463, 12.329557103138587, 9.055665671534543, 17.60770430862441, 12.01693893121073, 10.448787797676598, 13.104338820665122, 32.8036750813227, 114.37719427658728, 11.640425749174135, 13.256236542274511, 11.210081960920952, 60.709408617558424, 11.990315332265693, 13.52720652007029, 53.9624743993322, 20.44708184364418, 21.75525255706443, 51.181825008488985, 24.668427583829892, 13.609751881995015, 72.35746749575772, 66.99975787803338, 73.40928842777107, 107.03816163292329, 137.76834981895104, 60.181655980768994, 29.203572145150954, 67.68499984315439, 178.4871069514784, 87.18905135460639, 72.53367796603696, 85.4098658880898, 58.47689291198799, 96.03861847406313, 54.00991517906605, 124.12745922949796, 79.34672203044553, 140.1631197610261, 77.09733386314758, 83.17156980211271, 93.14963902609442, 88.01218273206304, 80.29404696395898, 131.4950097183889, 96.16618391869048, 84.49619474590786, 75.6502078170183, 62.95058187486404, 83.28965099645572, 70.88196030258679, 65.25408950466297, 1118.3567690049479, 57.296207017156064, 29.563179930436227, 24.57241333535691, 16.9553413271405, 16.659307660677783, 15.143291385877705, 23.340976661540726, 139.47002231410147, 11.702246769594206, 9.591194754789479, 7.33986622411128, 6.895417139721058, 13.947640954130662, 14.548986795818779, 7.216593726258213, 9.876797965697387, 23.33422663588148, 7.3956619115304605, 8.7978094490874, 19.828046122998753, 7.462924004511932, 8.70281425251069, 11.948468455113371, 5.390999309376423, 12.375870948195208, 61.90689163724408, 32.37538623306408, 14.383164456145503, 43.91377643828212, 30.87775174660407, 203.66591058682238, 19.239457459604115, 20.119755824212838, 19.29351196124538, 12.953290602039235, 30.431763853804874, 17.384692319402586, 16.605715912166723, 14.884595109976575, 13.282973208113722, 13.983053291670599, 358.37159697484583, 114.79020293609118, 50.06448530512057, 17.866297519121705, 51.55205332808939, 411.6017383735717, 26.941828939879734, 84.82050169196008, 7.4156059146566715, 9.67346059571958, 8.209526115568167, 4.0103832656045775, 18.397390478869767, 11.42394433441812, 13.484096277514054, 25.328359803489523, 5.949047441373667, 6.068165805647402, 15.450882274735926, 25.308877342873746, 19.88944723187481, 8.585504201789846, 21.09272480614177, 7.742405684051514, 8.3918712938718, 7.724778480922551, 2.1940031282588635, 14.335602587454177, 6.238138086697132, 3.6429443292315704, 8.054463604540194, 17.299558337179327, 15.228634164237825, 26.225087450419647, 13.57514653703596, 16.543351774347695, 18.232307771809612, 14.683388885091574, 14.507404847543214, 14.95900250668602, 14.349589694141862, 15.349687232547685, 10.608287377054076, 11.37628182164012, 10.608600649242845], \"Total\": [1119.0, 606.0, 866.0, 1246.0, 359.0, 755.0, 540.0, 458.0, 635.0, 433.0, 1255.0, 966.0, 349.0, 438.0, 671.0, 600.0, 1076.0, 412.0, 435.0, 384.0, 1355.0, 1624.0, 402.0, 712.0, 510.0, 1217.0, 445.0, 1697.0, 417.0, 239.0, 181.04249207547966, 142.40381073428188, 122.56209946100557, 209.15455110496026, 90.9972107211028, 85.73626652792227, 67.53005292156014, 58.21499926979057, 500.27196893446796, 54.16823459771953, 53.360006010183, 52.533668939200815, 50.12048303110201, 50.11690149490728, 49.71979684891117, 49.711564948721126, 45.6660407235973, 45.053604336426346, 42.63347879238735, 42.62421153026809, 41.22275282164389, 41.22272868272767, 41.221952213709706, 43.221067942420895, 39.99584725984716, 39.39749862116136, 39.19880704507592, 39.19067634339077, 38.38537414068876, 37.983532891335386, 115.46162152606155, 249.35056179470763, 231.59780106115494, 157.7833657528704, 202.78138490780262, 58.0164889571214, 227.82110467767407, 712.4944651915881, 72.25062644290122, 84.10872769556256, 82.99535421444753, 174.12624272519363, 93.70518652728339, 108.62459658704685, 151.4361684370839, 157.2825150914715, 175.2240138976905, 176.56823370110828, 561.4915222809724, 137.18944010626856, 262.22983872859834, 184.49424821030888, 115.4443867021223, 276.1372209596057, 206.29381620907685, 213.58009758843133, 250.8491296537333, 1372.867539824624, 620.4418276269304, 1697.1876564434301, 546.9308815852461, 262.1442018781436, 964.8658252471366, 566.5637733535558, 688.4152101167917, 1624.1346823180238, 525.5962577870382, 547.0341613333982, 930.3519980663071, 955.1296173150913, 1255.093099081226, 481.3937276189018, 1355.1688557838484, 974.0815770475539, 490.50869392869237, 1217.481042791864, 502.19567648037497, 1346.706062153941, 755.3053798870493, 966.6721547006882, 524.8746589126063, 729.5715975317638, 1076.9714295155688, 119.38723549893183, 149.2657476063012, 169.44402341907698, 109.96215472864168, 87.42161954366671, 78.61491128259813, 75.74179062841952, 68.77495223937589, 60.99213363488132, 58.737646887664376, 57.30666505700401, 64.67064356534613, 50.74027908699317, 49.720111707422255, 46.65107172114712, 45.61790031812808, 47.04846043140514, 44.59778488320433, 43.97727663677118, 43.368514120224525, 43.15673133559265, 42.959678679173706, 40.69069957769727, 40.49888656566365, 83.64113859029338, 39.66423173732093, 36.60395210002018, 35.783659398041245, 34.96308015980461, 31.686440239167386, 36.37384558561637, 142.5582066095425, 246.69516830537117, 445.2825401706004, 77.48967473421037, 189.21196438103055, 314.0225077481077, 145.8596799327699, 203.47699593340164, 303.78198781501226, 525.5114777989521, 405.8871565936873, 269.3258187689904, 403.58039649407516, 257.998248083193, 245.36517011673865, 1076.9714295155688, 421.2006291188527, 1346.706062153941, 190.24527122426434, 671.2247505171377, 546.0218248298692, 290.86020384342265, 241.97578026467113, 398.57475851795465, 254.22233152378524, 1246.0430315734345, 595.512184126599, 396.63268247484774, 1624.1346823180238, 481.3937276189018, 439.64119278509725, 648.4444873344489, 1372.867539824624, 729.5715975317638, 637.7809674099449, 600.8917092089757, 955.1296173150913, 224.8309447408012, 352.893901949752, 108.30607665963514, 88.41548323352977, 69.08377922415326, 71.1240549405496, 65.70876304741478, 61.95965763583722, 56.51919488732569, 55.20503366551279, 52.570389054106684, 49.759019265120344, 49.38908432527127, 49.19916117204661, 47.886986317657744, 41.316289414275374, 39.80934951656794, 36.79694473417825, 36.43650838618121, 35.12169205423527, 117.34085239813152, 31.93675157085824, 31.924290172628734, 31.37442557998421, 31.184531406170386, 100.1227638645952, 102.2149116875616, 28.933178828376246, 29.651333262271518, 29.655479450098, 340.2425180608969, 75.68300472294688, 155.03418494805294, 47.970529434148375, 315.4086002069982, 197.4310130690584, 154.36867651559012, 172.56550054377786, 101.33993101033225, 308.7791757793533, 159.72397130933382, 339.99618128212995, 85.05966438413559, 866.9130601895259, 103.60079027893025, 133.79170257589945, 74.31283868297349, 299.5372774880547, 263.516361131311, 250.63779812722998, 127.29929319748508, 314.95803468919087, 446.2202428779219, 277.19519012758934, 1126.163652852863, 259.63188879802817, 404.0613378083462, 383.0279551382809, 1255.093099081226, 660.6493299521702, 525.4647919218363, 620.4418276269304, 546.5743644330552, 325.85609308179306, 1697.1876564434301, 1624.1346823180238, 1355.1688557838484, 964.8658252471366, 384.7885854099551, 93.61399231739779, 114.68419414718156, 54.03574546776859, 50.09882722892949, 42.04648411347558, 39.90919958439778, 39.56350145758644, 62.674993633996785, 38.76348097310774, 38.27265257806196, 36.130372990149596, 35.65133401632666, 46.23728196177184, 31.5311133154484, 30.35440907095797, 30.06423116984976, 30.05947715402654, 29.56243186747875, 27.440200239705856, 23.645128460475014, 23.489368280756224, 22.994226025642806, 22.340577624936408, 22.33025018076894, 21.529674816621107, 21.020797809305925, 19.546042200452312, 19.050909253119194, 38.06538838565838, 402.5840631721652, 84.20707794298305, 41.325600585966846, 207.14668127225835, 33.044493150729046, 65.47981170462167, 120.34590526020399, 80.95072017103197, 183.17523535202076, 242.8232932405871, 326.9154758314713, 97.50118182958695, 95.34761190781597, 96.88016565613364, 324.2865087203903, 150.05129040490118, 144.62639767373702, 512.365379337266, 417.7175316249317, 184.2779704397115, 966.6721547006882, 280.01075958396586, 139.52937861840306, 1126.163652852863, 201.49976168698387, 242.47262121066564, 1355.1688557838484, 321.0246803204595, 653.6735377003434, 1697.1876564434301, 605.4036817622259, 439.64119278509725, 1217.481042791864, 1624.1346823180238, 866.9130601895259, 807.4411182684781, 1246.0430315734345, 850.3812847429027, 82.67725725652032, 31.22406126067048, 31.747143090807928, 24.786804424318202, 24.966106576118563, 21.489044123373525, 20.621231337377136, 20.792854115519006, 20.096555420089615, 19.237777668682924, 26.560231778168745, 213.7046005007272, 21.358975376443492, 16.45179898470535, 15.761608753139846, 14.363947608212575, 133.73639209191623, 13.668654849200438, 12.971676131506557, 20.43567599874623, 41.13469189308106, 58.30128721808962, 13.499598584812185, 10.887760530368572, 10.197518891805688, 55.914896496211185, 10.096630071001616, 127.17482445220428, 10.840991163828775, 10.80616712942136, 125.76044144023759, 122.06210190374908, 71.54451016165775, 206.50898095417443, 635.4406234659215, 75.35055607750016, 44.43239383657405, 61.68919006556859, 492.94262689399955, 215.53518001934685, 206.52528555374363, 53.71042948194379, 45.92512358209121, 673.1941536655229, 182.02107308354434, 55.314967002900204, 139.0494579103534, 174.41537283803626, 131.28778434349894, 176.33531915266084, 164.27605288276172, 388.8628667052865, 974.0815770475539, 286.9514717927829, 224.42039441405288, 398.02148538433835, 134.31957492331625, 269.3258187689904, 203.62677845057232, 456.9024014199967, 371.15585279748035, 637.7809674099449, 1217.481042791864, 383.15367021368684, 354.998225947179, 660.6493299521702, 1624.1346823180238, 524.8746589126063, 1697.1876564434301, 712.4944651915881, 458.5460278709478, 214.50547247076597, 84.08730772874053, 78.40511578428209, 75.91886078833612, 74.81651221221897, 69.68692127225411, 70.71927739804747, 51.54900455673056, 35.07781108895013, 34.662675355016184, 30.77673097004879, 28.011382681317876, 26.223040331011674, 24.9762968320355, 24.97361512430332, 22.893169020108733, 22.88191104230208, 22.881217680161246, 22.868131485276177, 26.420443227925805, 19.439040882494336, 18.869752381692493, 17.320880767231685, 16.792970012968738, 16.789058531921164, 16.116550267644406, 55.473273300454935, 15.689705888690693, 26.54441599851471, 202.3462821081462, 43.85962588906041, 438.4564986403642, 87.9460230894244, 412.55172735176177, 31.483799806139825, 168.16424644678818, 88.77035434684025, 101.40336120905839, 82.35297995255237, 202.10786734372132, 91.17985062074801, 44.02350860114004, 76.64197577783841, 142.94131120578626, 212.22511202890885, 119.38889935653668, 219.34855280343658, 179.25821594832234, 277.19519012758934, 313.7340504134463, 89.25268659582322, 397.6271177600911, 964.8658252471366, 525.4647919218363, 325.03849770098157, 850.3812847429027, 1697.1876564434301, 595.512184126599, 368.354873245797, 1372.867539824624, 1126.163652852863, 436.220857260679, 1255.093099081226, 112.15101781154084, 74.83144389779872, 73.89022345377427, 59.235767171668684, 52.55184332131262, 51.43675009652475, 43.911137689274234, 43.358696943291726, 40.153553955565414, 40.15777159285249, 37.650750020898414, 45.095784842654126, 34.59080744453345, 33.1954428141771, 33.15970107968317, 32.63756605377339, 32.08131902438974, 30.39478947250319, 29.14035433012709, 28.597527729512436, 26.786022257691076, 26.45361524565615, 25.94542425764457, 21.338157891063965, 19.674204676307276, 19.671165217685424, 19.10989363576852, 19.000136064969798, 18.0073924072689, 17.995866478552283, 80.2432247592696, 137.5391181659057, 132.9419495281584, 107.55515780957889, 39.03312109261318, 55.65049512501432, 64.73771299979707, 74.39443449020945, 54.53722918373179, 464.41842209770914, 1246.0430315734345, 130.49801215370266, 140.14996775126872, 177.33291531601998, 126.85260627108715, 1076.9714295155688, 175.5948509527823, 490.50869392869237, 94.68351736064973, 123.06152339848605, 966.6721547006882, 1255.093099081226, 1355.1688557838484, 1624.1346823180238, 512.365379337266, 561.4915222809724, 605.4036817622259, 1217.481042791864, 1346.706062153941, 595.512184126599, 964.8658252471366, 213.89638546486074, 1697.1876564434301, 974.0815770475539, 173.19066060457433, 73.49717211594762, 58.38056321351767, 58.148515252798695, 54.07511504131648, 54.06289871129921, 49.42721345554113, 45.582415002458035, 43.49503448347058, 39.65186323628615, 39.65277155430536, 39.07590796887184, 38.72638937855596, 37.911418231174295, 224.3405412301514, 25.587637884354944, 40.310759894094964, 23.50348946493247, 22.79063669896372, 21.861652191503826, 21.849465133741838, 36.509957440847664, 20.93577124837709, 20.93577127191337, 20.93577131327063, 20.93577131451843, 20.93577132416045, 349.30773277563827, 18.83827409421839, 17.448785771446765, 48.87021065969699, 41.865862287150804, 540.7915197789725, 106.29332127442822, 106.40414373188447, 46.72445282769504, 102.1344892743841, 755.3053798870493, 47.262337283753624, 273.9136492951554, 52.9927358271091, 92.34016636803442, 671.2247505171377, 176.85897273198663, 158.90732585812196, 256.33640241729825, 238.16909894593277, 112.60887334535167, 61.669178875930264, 57.57797843309313, 197.2783668326562, 85.4602046063462, 133.9171116895307, 807.4411182684781, 955.1296173150913, 1372.867539824624, 144.6941785980849, 1697.1876564434301, 186.5573436762568, 403.79797198295864, 1355.1688557838484, 673.1941536655229, 648.1347788072895, 92.46633379827168, 65.99000246706372, 49.460425436281824, 87.14917200155506, 39.191206149509476, 37.21210992293405, 33.2429365127609, 29.100603832779065, 24.314307250501695, 73.37961023855863, 22.991154229222936, 19.8309441310387, 16.376500098365618, 15.718893976753764, 13.72564571476827, 13.063409847612162, 186.94458768015758, 12.40986713126451, 18.196981874391312, 25.396245002259057, 108.57524674856471, 26.01690677574881, 14.361253619089442, 39.40323676548141, 25.77670973505857, 12.164321388381216, 50.3812359994834, 61.50449880024229, 64.42430688035276, 36.21454300812545, 96.52262406261497, 251.96605158370522, 80.04536676831178, 74.83429775692096, 121.64538719360611, 313.7340504134463, 94.07878139289977, 56.27271843725598, 34.79436067805877, 197.2783668326562, 1246.0430315734345, 417.4942067386705, 133.3339467684957, 206.14165608817603, 234.46102507094002, 140.97657218736686, 445.2825401706004, 456.9024014199967, 490.50869392869237, 600.8917092089757, 1697.1876564434301, 930.3519980663071, 1255.093099081226, 1624.1346823180238, 974.0815770475539, 1126.163652852863, 205.89708983375294, 70.46840672940506, 68.08954785692877, 57.70902864146989, 39.76771534454424, 34.67873359510608, 34.26488919815295, 30.092595233340795, 29.28185228918532, 25.8587172638952, 25.337811202368098, 20.047713907399494, 21.768013982697358, 17.459969002372688, 16.102009104801883, 14.955126356050338, 239.92538114120723, 13.195402832571459, 12.879015055587168, 12.778653083967011, 12.05633954489532, 25.605337165811743, 14.119431495649632, 47.02553578725626, 6.129433942822404, 33.123199283973854, 15.539460125410491, 24.09564793801767, 110.17424935498615, 14.610727499584245, 115.12655660408674, 62.40899033714032, 81.2293649110734, 105.8521578898001, 65.81817384573617, 324.30008185370434, 284.09775839124904, 195.1028107923848, 49.988048691307874, 417.4942067386705, 178.1459223747311, 55.64279043601139, 91.4194858283454, 137.62474025628921, 93.08798556364945, 234.31102212480687, 1255.093099081226, 84.9309361225123, 186.94114880996298, 310.7682943299875, 477.5994213104641, 964.8658252471366, 546.5743644330552, 648.1347788072895, 325.85609308179306, 397.6271177600911, 164.21920247041368, 169.2331580282639, 1346.706062153941, 404.0613378083462, 325.03849770098157, 620.4418276269304, 169.69958325051954, 81.76081608361027, 72.38183839816485, 205.36409119179496, 36.90747936830152, 31.047163395385414, 29.83292724700181, 29.00206668430457, 26.539465205446028, 23.979982184186905, 23.5589844833489, 22.984711045430156, 20.681882510644698, 19.851379841676888, 19.6666061591889, 20.787533211917278, 49.80630385804394, 14.001905345026492, 13.179298120704406, 13.188155733137293, 76.33067756968885, 13.652789766487162, 22.61534818851378, 20.824165406365875, 26.63678602584923, 22.45037281604087, 18.94791007221932, 21.13879404078743, 13.894708713902576, 8.639482777800996, 107.07400115723648, 93.91597709958691, 117.57041102572632, 50.50951945452395, 217.75439011516872, 119.33033593297722, 53.229005446203, 410.6671045487138, 133.66441436477888, 121.34784537268837, 435.359238837577, 113.29419476334974, 341.42640036955225, 51.60190716107911, 1255.093099081226, 279.0317303097846, 225.33046896436358, 84.9309361225123, 1372.867539824624, 1346.706062153941, 251.96605158370522, 525.5114777989521, 438.4564986403642, 966.6721547006882, 299.9566074394278, 242.8232932405871, 850.3812847429027, 455.5074614401418, 1217.481042791864, 561.4915222809724, 109.80711746400294, 90.47094509496935, 57.535065835174215, 61.563746634251075, 34.32991622230001, 29.51409541820342, 22.013114081228654, 21.438970076955954, 20.969380107531748, 18.62069165779953, 17.101325232523326, 16.279071877415817, 16.267902979680755, 15.336920584047052, 13.81997553031174, 94.84172204532916, 11.476732925316242, 10.531489171246381, 9.62604308426358, 74.19304093378788, 34.379173776483626, 23.87416089355724, 27.161320000729916, 36.09395546850348, 52.57621435407986, 16.782480587727203, 16.675431173242053, 32.55221586075526, 18.310082424109346, 17.73414548597444, 34.327773648343396, 433.3340789222379, 123.25059487168913, 90.2056124069497, 35.555827155893795, 112.04045873527487, 373.60527056142814, 176.33531915266084, 327.6880723686547, 173.1105474244332, 58.16396100156237, 139.52937861840306, 546.0218248298692, 331.87393713489683, 168.28643357692255, 127.63852404577162, 125.10234314460787, 930.3519980663071, 850.3812847429027, 1624.1346823180238, 180.09925653709817, 1355.1688557838484, 182.43883982798422, 1346.706062153941, 605.4036817622259, 1217.481042791864, 512.365379337266, 974.0815770475539, 807.4411182684781, 383.15367021368684, 653.6735377003434, 648.4444873344489, 955.1296173150913, 85.73501074673783, 63.55519868363047, 49.699754296143055, 42.495861935305925, 55.72401808381939, 34.833220305067314, 25.16677528539528, 21.995209196997585, 15.651341097982025, 15.687047262563954, 13.434547696732528, 10.160483175191537, 22.210474260482936, 15.163621946744081, 13.318923732964613, 16.8864580594443, 42.49649395196431, 148.80763438420558, 15.334873337436107, 17.789380195170754, 15.730048400862913, 86.10696544499292, 17.088941936677752, 20.066882015177256, 80.3936187667893, 30.626791042632654, 33.02115478038013, 77.9786802279467, 37.94446603077501, 21.025124725405384, 111.81074604492156, 105.57139334042235, 116.20349868597138, 175.0819773088002, 242.01966682763577, 106.15863303772743, 46.24725491116268, 124.52341835599118, 435.359238837577, 181.33771616207858, 156.15512234658755, 203.62677845057232, 122.19574247394466, 245.35818999666253, 117.41706151302346, 436.220857260679, 226.26340886995712, 600.8917092089757, 235.49015884634622, 295.2353049275684, 398.02148538433835, 373.60527056142814, 307.3694229048399, 1217.481042791864, 601.0683349840923, 637.7809674099449, 388.8628667052865, 198.24004244925325, 1697.1876564434301, 1076.9714295155688, 1126.163652852863, 1119.5391568980624, 58.44416105977051, 30.711220098124265, 25.720277702832995, 18.103227790631504, 17.807174353934595, 16.291198223116027, 25.18722179791132, 152.9171179218341, 12.853913758476674, 10.739339677030912, 8.487717621131102, 8.0436995714855, 16.36370972008968, 17.370549555240284, 9.363151374834029, 13.702086169164957, 34.401271754692296, 11.563778977705562, 14.123151824039343, 32.37669309838082, 12.584713904871734, 15.429143790310235, 21.362250392890758, 9.752834167983833, 25.670443562465074, 130.49801215370266, 68.31406856008967, 32.14327508072481, 98.37027416206236, 77.40925856777861, 866.9130601895259, 46.88309154305928, 65.05302751070158, 65.62490265528167, 36.29887986697224, 600.8917092089757, 177.33291531601998, 203.47699593340164, 307.3694229048399, 245.35818999666253, 964.8658252471366, 359.4903790974365, 115.90872028062027, 51.18297337120244, 19.90300050331748, 74.95185920796519, 606.4057276147812, 40.3292264301715, 139.77168617239175, 12.648636293017514, 18.95351983325461, 17.390542949770477, 10.668313845237156, 49.594025229087016, 34.44199764068795, 42.50789497841619, 90.51788777987734, 25.56493928581171, 26.841797515845144, 74.96274526362502, 123.52653565096792, 97.32023012719915, 42.91740538942516, 110.66182402251327, 42.4747007665206, 46.25499224773248, 46.7311647716829, 14.281060986566578, 93.57460672527822, 42.42299507558085, 27.085521224150966, 61.669178875930264, 182.02107308354434, 176.59701799712255, 510.6818658100318, 178.1459223747311, 299.5372774880547, 966.6721547006882, 512.365379337266, 648.1347788072895, 1355.1688557838484, 1076.9714295155688, 1697.1876564434301, 179.94013054077627, 546.9308815852461, 255.06851609847237], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.7373, -5.9789, -6.1302, -5.5967, -6.431, -6.4913, -6.7334, -6.8843, -4.7338, -6.9577, -6.9731, -6.9895, -7.037, -7.0371, -7.0452, -7.0456, -7.1322, -7.146, -7.2026, -7.2028, -7.2371, -7.2371, -7.2371, -7.19, -7.2681, -7.2836, -7.2888, -7.289, -7.3104, -7.3212, -6.2156, -5.4625, -5.5388, -5.9164, -5.6813, -6.9055, -5.5939, -4.4998, -6.6984, -6.558, -6.5726, -5.8868, -6.4719, -6.3516, -6.0649, -6.0366, -5.9596, -5.9539, -4.988, -6.1746, -5.6576, -5.9471, -6.3335, -5.6503, -5.8784, -5.8613, -5.7627, -4.5799, -5.1472, -4.4779, -5.248, -5.7531, -4.9311, -5.3073, -5.2018, -4.7279, -5.3602, -5.3448, -5.1133, -5.1015, -5.0557, -5.4951, -5.0478, -5.2172, -5.5033, -5.1604, -5.5161, -5.2176, -5.3924, -5.3812, -5.5545, -5.5146, -5.5246, -5.7099, -5.4869, -5.3606, -5.7931, -6.0248, -6.1323, -6.1701, -6.268, -6.3901, -6.4284, -6.4535, -6.3338, -6.5777, -6.5984, -6.6635, -6.6865, -6.6557, -6.7096, -6.724, -6.7382, -6.7433, -6.748, -6.8036, -6.8085, -6.0843, -6.831, -6.9124, -6.9358, -6.9597, -7.0613, -6.9238, -5.6342, -5.1305, -4.6628, -6.2383, -5.5047, -5.0807, -5.7296, -5.4798, -5.1558, -4.7148, -4.9365, -5.2738, -4.9691, -5.3386, -5.3809, -4.2884, -5.0234, -4.2452, -5.6263, -4.7777, -4.9306, -5.3531, -5.4756, -5.1851, -5.4588, -4.5978, -5.0426, -5.2551, -4.5668, -5.2317, -5.2668, -5.1469, -4.9262, -5.2214, -5.2716, -5.3455, -5.3142, -4.7597, -4.3125, -5.4952, -5.7003, -5.9505, -5.9215, -6.0018, -6.0611, -6.1547, -6.1787, -6.2285, -6.2847, -6.2923, -6.2962, -6.3239, -6.4751, -6.5132, -6.5942, -6.6049, -6.6422, -5.4367, -6.7403, -6.7412, -6.7587, -6.765, -5.5994, -5.5792, -6.8427, -6.8186, -6.8192, -4.4078, -5.8974, -5.2059, -6.3515, -4.5485, -5.0061, -5.2612, -5.16, -5.6603, -4.639, -5.2558, -4.5976, -5.8527, -4.0034, -5.7087, -5.5132, -5.9737, -4.9132, -5.0231, -5.0846, -5.5747, -4.9563, -4.7274, -5.0846, -4.2335, -5.1533, -4.9209, -4.9861, -4.343, -4.7226, -5.0246, -4.9682, -5.0265, -5.2213, -5.1833, -5.1937, -5.2301, -5.2523, -4.0179, -5.4402, -5.2404, -5.9983, -6.0759, -6.255, -6.309, -6.3177, -5.858, -6.3385, -6.3516, -6.4109, -6.4246, -6.1669, -6.5515, -6.5909, -6.6009, -6.6011, -6.6183, -6.6958, -6.8512, -6.8581, -6.8805, -6.9107, -6.9112, -6.9496, -6.9748, -7.0517, -7.0788, -6.3883, -4.0633, -5.6112, -6.3179, -4.7784, -6.539, -5.9034, -5.347, -5.7193, -4.9885, -4.76, -4.5494, -5.6078, -5.6484, -5.6442, -4.6803, -5.3072, -5.3671, -4.4334, -4.5926, -5.2191, -4.0727, -4.9892, -5.4776, -4.1637, -5.2744, -5.1626, -4.1969, -5.0273, -4.7121, -4.4292, -5.0063, -5.1505, -4.9726, -4.9525, -5.1076, -5.1417, -5.0913, -5.1991, -5.0422, -6.0378, -6.0212, -6.2781, -6.271, -6.4276, -6.4711, -6.4639, -6.4983, -6.5445, -6.229, -4.1441, -6.45, -6.7112, -6.7568, -6.8579, -4.6299, -6.9111, -6.9674, -6.5162, -5.818, -5.4757, -6.9392, -7.1599, -7.2329, -5.5361, -7.2852, -4.7787, -7.244, -7.2585, -4.8229, -4.8671, -5.4339, -4.4554, -3.4597, -5.4176, -5.9204, -5.6579, -4.0097, -4.6741, -4.7427, -5.7963, -5.9185, -4.0173, -4.9886, -5.805, -5.193, -5.0808, -5.263, -5.0891, -5.1486, -4.6667, -4.1665, -4.8667, -5.0465, -4.7961, -5.3268, -5.0005, -5.137, -4.7948, -4.9085, -4.7777, -4.5975, -4.9684, -5.0054, -4.8568, -4.6783, -4.9315, -4.8401, -5.0706, -3.3143, -4.0816, -5.0211, -5.092, -5.1247, -5.1396, -5.2117, -5.2026, -5.5188, -5.9139, -5.9262, -6.0497, -6.147, -6.2158, -6.2666, -6.2668, -6.3579, -6.3584, -6.3584, -6.359, -6.2234, -6.5304, -6.5619, -6.653, -6.6861, -6.6863, -6.7301, -5.4953, -6.7589, -6.2393, -4.2297, -5.7456, -3.5039, -5.0708, -3.5755, -6.0865, -4.5489, -5.1582, -5.0768, -5.2602, -4.5276, -5.2122, -5.8197, -5.4183, -4.9888, -4.8261, -5.2529, -4.8839, -5.0401, -4.9338, -4.8821, -5.4802, -4.8278, -4.7157, -4.9326, -5.0807, -4.9227, -4.8778, -5.0479, -5.0972, -5.0102, -5.0783, -5.1375, -5.1344, -4.4783, -4.8879, -4.9026, -5.1255, -5.2478, -5.2695, -5.4314, -5.4444, -5.5233, -5.5233, -5.5894, -5.4103, -5.6768, -5.7194, -5.7205, -5.7372, -5.7559, -5.8107, -5.8544, -5.874, -5.9422, -5.9551, -5.9753, -6.1804, -6.2661, -6.2667, -6.2969, -6.3051, -6.3601, -6.3607, -4.9076, -4.3953, -4.4871, -4.7053, -5.6655, -5.3717, -5.2428, -5.1715, -5.4283, -4.0961, -3.5157, -4.976, -5.0341, -5.0301, -5.1881, -4.3404, -5.0646, -4.6796, -5.3131, -5.2294, -4.5908, -4.614, -4.6032, -4.6504, -4.9327, -4.9837, -4.9965, -4.9649, -4.9695, -5.0603, -5.1364, -5.1986, -5.1631, -5.1815, -3.9494, -4.8147, -5.0489, -5.053, -5.1271, -5.1274, -5.2189, -5.3019, -5.3499, -5.4449, -5.4449, -5.46, -5.4692, -5.4911, -3.7248, -5.8988, -5.4453, -5.9891, -6.0201, -6.0639, -6.0645, -5.5525, -6.1095, -6.1095, -6.1095, -6.1095, -6.1095, -3.2954, -6.2213, -6.3029, -5.2823, -5.4609, -3.0908, -4.6078, -4.6105, -5.4189, -4.7726, -3.108, -5.4247, -4.0184, -5.3532, -4.9495, -3.6391, -4.5717, -4.6449, -4.3813, -4.4507, -5.0328, -5.295, -5.3254, -4.9618, -5.2321, -5.123, -4.6921, -4.6819, -4.665, -5.1314, -4.6982, -5.0877, -5.0233, -4.9358, -5.0163, -5.1055, -4.5119, -4.854, -5.1478, -4.5871, -5.3864, -5.4397, -5.5561, -5.6939, -5.8815, -4.7779, -5.9398, -6.0956, -6.2991, -6.343, -6.4893, -6.5431, -3.885, -6.5991, -6.2291, -5.8974, -4.4633, -5.8949, -6.5392, -5.5419, -5.9792, -6.7355, -5.3222, -5.1278, -5.0949, -5.701, -4.7232, -3.8062, -4.9113, -4.9792, -4.5635, -3.9459, -4.9276, -5.381, -5.7597, -4.5433, -3.2638, -4.0781, -4.8388, -4.5923, -4.6073, -4.9389, -4.5137, -4.5584, -4.5623, -4.5401, -4.458, -4.7596, -4.7617, -4.7432, -4.9162, -4.9663, -3.6735, -4.7557, -4.7906, -4.959, -5.3407, -5.4814, -5.4938, -5.6283, -5.6567, -5.7862, -5.8075, -6.0541, -5.9737, -6.2026, -6.2875, -6.3671, -3.5997, -6.5031, -6.5296, -6.5381, -6.6023, -5.8627, -6.4624, -5.3238, -7.3822, -5.7211, -6.4967, -6.0807, -4.5747, -6.648, -4.624, -5.2562, -5.0603, -4.8685, -5.2872, -4.0789, -4.2173, -4.5748, -5.5546, -4.1643, -4.7479, -5.496, -5.2268, -5.0131, -5.2287, -4.7674, -3.9557, -5.2943, -4.9655, -4.7678, -4.6485, -4.4686, -4.6846, -4.6558, -4.8638, -5.0067, -5.1582, -5.1575, -4.9454, -5.1021, -5.1201, -5.0747, -3.7729, -4.5111, -4.6339, -3.5925, -5.3225, -5.5013, -5.5427, -5.572, -5.6645, -5.7707, -5.7892, -5.815, -5.9264, -5.9697, -5.9795, -5.9341, -5.0743, -6.3443, -6.4097, -6.4107, -4.6603, -6.4203, -5.9283, -6.0219, -5.7859, -5.9654, -6.1516, -6.0492, -6.4847, -7.0318, -4.5612, -4.6989, -4.5678, -5.3422, -4.2137, -4.9038, -5.5092, -4.2499, -4.9787, -5.0361, -4.4879, -5.1422, -4.6193, -5.549, -4.2668, -4.9206, -5.0101, -5.3819, -4.5079, -4.5431, -5.0546, -4.8829, -4.9441, -4.8132, -5.0666, -5.1225, -4.9276, -5.0823, -5.0694, -5.0988, -4.166, -4.3619, -4.8217, -4.761, -5.3513, -5.5079, -5.8144, -5.8423, -5.8656, -5.9914, -6.0828, -6.1349, -6.1358, -6.1998, -6.3117, -4.387, -6.5153, -6.6109, -6.7384, -4.7105, -5.4983, -5.8985, -5.777, -5.5087, -5.1586, -6.3192, -6.3382, -5.6734, -6.276, -6.3179, -5.6758, -3.5444, -4.6436, -4.9627, -5.7114, -4.8972, -4.0321, -4.585, -4.1512, -4.6335, -5.4279, -4.9555, -4.2197, -4.5205, -4.8794, -5.0226, -5.0406, -4.1046, -4.1783, -3.9924, -4.9636, -4.2035, -5.0258, -4.4537, -4.6969, -4.5247, -4.7568, -4.6614, -4.7501, -4.8666, -4.7972, -4.9059, -4.8944, -4.4139, -4.7178, -4.9687, -5.1292, -4.8586, -5.3339, -5.6734, -5.8131, -6.1748, -6.1731, -6.3402, -6.6488, -5.9838, -6.3658, -6.5057, -6.2792, -5.3616, -4.1127, -6.3977, -6.2677, -6.4353, -4.7461, -6.3681, -6.2475, -4.8639, -5.8343, -5.7723, -4.9168, -5.6466, -6.2414, -4.5705, -4.6475, -4.5561, -4.179, -3.9266, -4.7548, -5.4779, -4.6373, -3.6676, -4.3841, -4.5681, -4.4047, -4.7835, -4.2874, -4.863, -4.0308, -4.4783, -3.9094, -4.5071, -4.4313, -4.318, -4.3747, -4.4665, -3.9732, -4.2861, -4.4155, -4.526, -4.7098, -4.4298, -4.5911, -4.6739, -0.8627, -3.8341, -4.4958, -4.6807, -5.0517, -5.0694, -5.1648, -4.7321, -2.9445, -5.4225, -5.6215, -5.889, -5.9515, -5.247, -5.2048, -5.9059, -5.5921, -4.7324, -5.8814, -5.7078, -4.8952, -5.8724, -5.7187, -5.4017, -6.1976, -5.3666, -3.7567, -4.4049, -5.2163, -4.1001, -4.4523, -2.5658, -4.9254, -4.8806, -4.9226, -5.321, -4.4668, -5.0267, -5.0726, -5.182, -5.2958, -5.2445, -1.6882, -2.8267, -3.6565, -4.6869, -3.6272, -1.5497, -4.2761, -3.1293, -5.5662, -5.3004, -5.4645, -6.1809, -4.6576, -5.1341, -4.9683, -4.3379, -5.7866, -5.7667, -4.8321, -4.3386, -4.5796, -5.4197, -4.5209, -5.5231, -5.4425, -5.5254, -6.7841, -4.9071, -5.7391, -6.277, -5.4836, -4.7191, -4.8466, -4.3031, -4.9616, -4.7638, -4.6666, -4.8831, -4.8951, -4.8645, -4.9061, -4.8387, -5.2082, -5.1383, -5.2081], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3889, 1.3873, 1.3861, 1.3851, 1.383, 1.3823, 1.3789, 1.3764, 1.376, 1.375, 1.3747, 1.3739, 1.3734, 1.3734, 1.3733, 1.373, 1.3713, 1.371, 1.3697, 1.3697, 1.3688, 1.3688, 1.3688, 1.3686, 1.368, 1.3676, 1.3674, 1.3674, 1.3668, 1.3666, 1.3604, 1.3435, 1.3411, 1.3473, 1.3315, 1.3586, 1.3024, 1.2563, 1.3463, 1.3348, 1.3335, 1.2783, 1.3129, 1.2854, 1.2398, 1.2302, 1.1992, 1.1973, 1.0063, 1.2289, 1.0981, 1.1601, 1.2426, 1.0536, 1.1171, 1.0996, 1.0373, 0.5203, 0.7473, 0.4103, 0.7726, 1.0029, 0.5218, 0.678, 0.5887, 0.2043, 0.7002, 0.6755, 0.376, 0.3616, 0.1342, 0.6531, 0.0654, 0.2262, 0.6261, 0.0599, 0.5898, -0.0982, 0.3054, 0.0699, 0.5072, 0.2178, -0.1816, 1.8326, 1.8322, 1.8318, 1.8316, 1.8293, 1.828, 1.8275, 1.826, 1.824, 1.8234, 1.8229, 1.8218, 1.8205, 1.8201, 1.8186, 1.8181, 1.818, 1.8176, 1.8172, 1.8169, 1.8168, 1.8167, 1.8153, 1.8151, 1.8141, 1.8135, 1.8123, 1.8116, 1.8109, 1.8077, 1.8072, 1.7309, 1.6862, 1.5634, 1.7364, 1.5773, 1.4947, 1.6126, 1.5295, 1.4528, 1.3457, 1.3823, 1.4551, 1.3555, 1.4334, 1.4412, 1.0545, 1.2584, 0.8743, 1.4503, 1.0381, 1.0917, 1.2989, 1.3605, 1.1519, 1.3279, 0.5994, 0.8929, 1.0868, 0.3654, 0.9165, 0.9721, 0.7034, 0.1741, 0.5111, 0.5953, 0.581, 0.1488, 2.1499, 2.1462, 2.1447, 2.1425, 2.1391, 2.139, 2.1379, 2.1373, 2.1356, 2.1351, 2.1342, 2.133, 2.1328, 2.1328, 2.1322, 2.1286, 2.1276, 2.1253, 2.1245, 2.1239, 2.1231, 2.1208, 2.1203, 2.1202, 2.1199, 2.1191, 2.1186, 2.1172, 2.1168, 2.116, 2.0874, 2.1009, 2.0753, 2.1027, 2.0225, 2.0334, 2.0243, 2.0142, 2.0461, 1.9533, 1.9956, 1.8983, 2.0289, 1.5565, 1.9756, 1.9154, 2.0429, 1.7094, 1.7277, 1.7163, 1.9037, 1.6161, 1.4967, 1.6155, 1.0648, 1.6123, 1.4024, 1.3907, 0.847, 1.109, 1.036, 0.9262, 0.9948, 1.3171, -0.2952, -0.2615, -0.1169, 0.2006, 2.3543, 2.3455, 2.3423, 2.337, 2.335, 2.3311, 2.3293, 2.3292, 2.329, 2.329, 2.3286, 2.3269, 2.3265, 2.3242, 2.3224, 2.321, 2.3207, 2.3206, 2.32, 2.3171, 2.3105, 2.3102, 2.3092, 2.3078, 2.3077, 2.3058, 2.3046, 2.3004, 2.299, 2.2973, 2.2637, 2.2804, 2.2855, 2.2131, 2.288, 2.2397, 2.1875, 2.2118, 2.1259, 2.0726, 1.9858, 2.1372, 2.1189, 2.1072, 1.8629, 2.0067, 1.9836, 1.6524, 1.6974, 1.8894, 1.3783, 1.7008, 1.909, 1.1346, 1.7447, 1.6714, 0.9163, 1.526, 1.1302, 0.4589, 0.9127, 1.0884, 0.2478, -0.0204, 0.4523, 0.4893, 0.1058, 0.3801, 2.8677, 2.8459, 2.8458, 2.8364, 2.8363, 2.8297, 2.8274, 2.8264, 2.8261, 2.8236, 2.8164, 2.8162, 2.8134, 2.8132, 2.8105, 2.8023, 2.7991, 2.7987, 2.7947, 2.7914, 2.79, 2.7835, 2.783, 2.7773, 2.7699, 2.765, 2.7275, 2.7006, 2.6976, 2.6863, 2.6676, 2.6533, 2.6207, 2.5392, 2.4108, 2.5851, 2.6105, 2.5449, 2.1148, 2.2776, 2.2517, 2.5449, 2.5793, 1.7956, 2.1322, 2.5069, 2.197, 2.0827, 2.1845, 2.0634, 2.0747, 1.6949, 1.2768, 1.7989, 1.8649, 1.5422, 2.0979, 1.7285, 1.8716, 1.4056, 1.4998, 1.0892, 0.6229, 1.408, 1.4474, 0.9748, 0.2539, 1.1302, 0.048, 0.6855, 2.8825, 2.8749, 2.8719, 2.8709, 2.8705, 2.8703, 2.8692, 2.8636, 2.8636, 2.8534, 2.853, 2.8484, 2.8453, 2.8425, 2.8403, 2.8403, 2.8361, 2.8361, 2.8361, 2.8361, 2.8273, 2.8272, 2.8254, 2.8199, 2.8178, 2.8178, 2.8149, 2.8136, 2.8129, 2.8067, 2.7852, 2.7983, 2.7377, 2.7774, 2.727, 2.7889, 2.6511, 2.6806, 2.629, 2.6536, 2.4885, 2.5999, 2.7204, 2.5674, 2.3736, 2.1411, 2.2896, 2.0503, 2.0959, 1.7663, 1.6942, 2.3532, 1.5116, 0.7372, 1.128, 1.4602, 0.6565, 0.0103, 0.8876, 1.3186, 0.0901, 0.22, 1.1092, 0.0555, 3.1267, 3.1218, 3.1197, 3.1179, 3.1153, 3.115, 3.1113, 3.111, 3.1089, 3.1088, 3.1071, 3.1058, 3.1045, 3.1031, 3.1031, 3.1023, 3.1007, 3.0999, 3.0983, 3.0976, 3.0948, 3.0944, 3.0936, 3.084, 3.0795, 3.079, 3.0778, 3.0753, 3.074, 3.074, 3.0322, 3.0057, 2.9478, 2.9416, 2.995, 2.9341, 2.9118, 2.844, 2.8977, 2.088, 1.6814, 2.4776, 2.348, 2.1167, 2.2938, 1.0026, 2.0921, 1.4499, 2.4612, 2.2828, 0.8602, 0.5759, 0.51, 0.2817, 1.1532, 1.0106, 0.9225, 0.2554, 0.15, 0.8752, 0.3165, 1.7608, -0.275, 0.2618, 3.221, 3.2129, 3.209, 3.2089, 3.2074, 3.2074, 3.2055, 3.2035, 3.2024, 3.1998, 3.1998, 3.1994, 3.1991, 3.1985, 3.1869, 3.184, 3.1829, 3.1786, 3.1784, 3.1763, 3.1762, 3.1748, 3.1739, 3.1739, 3.1739, 3.1739, 3.1739, 3.1735, 3.1677, 3.1628, 3.1534, 3.1295, 2.941, 3.0509, 3.0471, 3.0617, 2.926, 2.5898, 3.0444, 2.6937, 3.0015, 2.8499, 2.1766, 2.5778, 2.6117, 2.397, 2.4012, 2.5682, 2.9081, 2.9463, 2.0784, 2.6448, 2.3046, 0.9389, 0.7811, 0.4352, 2.2188, 0.19, 2.0085, 1.3007, 0.1774, 0.7965, 0.7453, 3.2861, 3.2814, 3.2759, 3.2702, 3.27, 3.2685, 3.265, 3.2602, 3.2523, 3.2513, 3.25, 3.2421, 3.2299, 3.227, 3.2163, 3.212, 3.2091, 3.2073, 3.1946, 3.1929, 3.1742, 3.1712, 3.1212, 3.1091, 3.0962, 3.0909, 3.0831, 3.078, 3.0645, 3.0345, 3.0319, 2.9894, 3.031, 3.0304, 2.9603, 2.6304, 2.8531, 2.9137, 3.0157, 2.497, 1.9334, 2.2125, 2.5932, 2.404, 2.2603, 2.4374, 1.7125, 1.642, 1.5672, 1.3864, 0.4301, 0.7297, 0.4282, 0.189, 0.5271, 0.332, 3.324, 3.314, 3.3134, 3.3104, 3.3011, 3.2974, 3.297, 3.2923, 3.2912, 3.286, 3.2851, 3.2727, 3.2708, 3.2624, 3.2585, 3.2528, 3.2449, 3.2419, 3.2397, 3.239, 3.233, 3.2194, 3.2149, 3.1504, 3.1296, 3.1035, 3.0848, 3.0622, 3.0482, 2.9951, 2.9548, 2.9349, 2.8673, 2.7943, 2.8508, 2.4643, 2.4583, 2.4765, 2.8585, 2.1263, 2.3944, 2.8099, 2.5827, 2.3872, 2.5627, 2.1008, 1.2342, 2.5887, 2.1286, 1.8181, 1.5076, 0.9843, 1.3366, 1.195, 1.6747, 1.3327, 2.0655, 2.0361, 0.174, 1.2212, 1.4209, 0.8198, 3.418, 3.41, 3.4091, 3.4076, 3.394, 3.3881, 3.3866, 3.3855, 3.3817, 3.377, 3.3761, 3.375, 3.3692, 3.3669, 3.3664, 3.3564, 3.3425, 3.3414, 3.3365, 3.3349, 3.3295, 3.2907, 3.2779, 3.2669, 3.2566, 3.2481, 3.2316, 3.2246, 3.2086, 3.1367, 3.0902, 3.0836, 2.99, 3.0605, 2.7278, 2.6392, 2.841, 2.0572, 2.4508, 2.4901, 1.7608, 2.4527, 1.8725, 2.8323, 0.9231, 1.773, 1.8972, 2.5011, 0.5923, 0.5764, 1.741, 1.1776, 1.2975, 0.6378, 1.5547, 1.7101, 0.6516, 1.1212, 0.1509, 0.8955, 3.4601, 3.4579, 3.4508, 3.4438, 3.4376, 3.4321, 3.4188, 3.4174, 3.4162, 3.4092, 3.403, 3.4001, 3.3999, 3.3948, 3.3871, 3.3857, 3.3692, 3.3596, 3.3221, 3.3078, 3.2891, 3.2536, 3.2461, 3.2301, 3.204, 3.1853, 3.1727, 3.1687, 3.1415, 3.1315, 3.1131, 2.709, 2.8671, 2.8601, 3.0424, 2.7088, 2.3696, 2.5675, 2.3816, 2.5375, 2.8337, 2.4311, 1.8025, 1.9996, 2.3198, 2.4531, 2.4552, 1.3847, 1.4009, 0.9397, 2.1678, 0.9097, 2.0927, 0.6657, 1.222, 0.6956, 1.3291, 0.782, 0.8809, 1.5098, 1.0451, 0.9444, 0.5686, 3.4598, 3.4552, 3.4502, 3.4463, 3.4459, 3.4404, 3.4259, 3.421, 3.3995, 3.3989, 3.3869, 3.3576, 3.2405, 3.2401, 3.23, 3.2192, 3.2138, 3.2096, 3.1971, 3.1786, 3.134, 3.1232, 3.1184, 3.0784, 3.0741, 3.0687, 3.0554, 3.0517, 3.0421, 3.0378, 3.0375, 3.018, 3.0134, 2.9807, 2.9093, 2.9052, 3.013, 2.8631, 2.5811, 2.7404, 2.7059, 2.6039, 2.7357, 2.5348, 2.6962, 2.2159, 2.4249, 2.0171, 2.3561, 2.2059, 2.0204, 2.027, 2.1304, 1.2472, 1.6401, 1.4514, 1.8356, 2.3256, 0.4583, 0.7518, 0.6244, 4.4415, 4.4227, 4.4045, 4.3969, 4.3771, 4.3759, 4.3695, 4.3664, 4.3505, 4.3487, 4.3295, 4.2973, 4.2885, 4.2828, 4.2653, 4.1822, 4.1152, 4.0544, 3.9956, 3.9692, 3.9522, 3.92, 3.87, 3.8615, 3.8497, 3.713, 3.6968, 3.6958, 3.6384, 3.6361, 3.5235, 2.9941, 3.5519, 3.2691, 3.2184, 3.4121, 1.4596, 2.1201, 1.9368, 1.4148, 1.5263, 0.2084, 4.752, 4.7454, 4.733, 4.6471, 4.3808, 4.3676, 4.3517, 4.2556, 4.2211, 4.0825, 4.0045, 3.7767, 3.7634, 3.6515, 3.6069, 3.4815, 3.2971, 3.2682, 3.1758, 3.1698, 3.1673, 3.1459, 3.0975, 3.0529, 3.0482, 2.9551, 2.8819, 2.8791, 2.8381, 2.7489, 2.7195, 2.4016, 2.3044, 1.7861, 2.1807, 1.8588, 0.7844, 1.2028, 0.9556, 0.2487, 0.4369, 0.0494, 1.9241, 0.8823, 1.5752]}, \"token.table\": {\"Topic\": [3, 14, 1, 8, 10, 12, 4, 8, 7, 9, 11, 13, 2, 4, 9, 2, 13, 3, 4, 10, 13, 2, 3, 1, 6, 1, 2, 1, 7, 11, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 2, 9, 4, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 13, 14, 1, 3, 5, 6, 12, 13, 5, 1, 7, 10, 11, 1, 13, 5, 12, 1, 8, 1, 2, 3, 9, 12, 13, 7, 9, 1, 3, 1, 8, 13, 14, 2, 11, 6, 2, 9, 1, 3, 4, 6, 10, 15, 1, 7, 13, 4, 1, 5, 1, 2, 3, 4, 6, 8, 10, 11, 12, 15, 7, 10, 6, 6, 12, 2, 5, 6, 3, 5, 6, 14, 2, 5, 1, 6, 7, 9, 11, 6, 1, 2, 4, 6, 7, 8, 9, 10, 11, 9, 6, 7, 15, 4, 8, 8, 4, 6, 6, 8, 1, 3, 13, 15, 3, 7, 13, 8, 3, 6, 4, 2, 5, 6, 10, 1, 5, 12, 1, 9, 13, 13, 14, 7, 10, 1, 4, 3, 4, 2, 4, 10, 11, 14, 7, 11, 4, 12, 7, 14, 7, 11, 12, 10, 3, 3, 7, 8, 9, 10, 13, 7, 6, 12, 14, 14, 10, 5, 1, 10, 11, 15, 10, 3, 4, 13, 3, 7, 12, 9, 11, 1, 5, 12, 2, 3, 7, 2, 7, 13, 14, 12, 1, 8, 14, 1, 2, 13, 2, 3, 7, 11, 13, 14, 1, 2, 6, 7, 9, 10, 13, 12, 1, 10, 4, 6, 8, 9, 4, 8, 8, 1, 2, 3, 4, 7, 10, 11, 12, 10, 10, 1, 2, 5, 6, 7, 9, 10, 11, 1, 2, 5, 13, 7, 14, 1, 2, 4, 5, 6, 7, 9, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14, 4, 12, 1, 3, 11, 12, 9, 12, 1, 5, 11, 2, 1, 2, 3, 4, 5, 9, 12, 13, 14, 4, 1, 2, 6, 8, 9, 8, 9, 1, 2, 10, 2, 1, 3, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 5, 12, 8, 8, 8, 5, 1, 2, 6, 7, 1, 5, 12, 14, 4, 1, 9, 4, 7, 9, 4, 2, 3, 4, 8, 10, 11, 14, 15, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 1, 2, 3, 4, 6, 7, 9, 11, 3, 4, 7, 14, 5, 15, 5, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 1, 2, 4, 5, 10, 14, 15, 3, 7, 10, 4, 11, 1, 2, 4, 6, 7, 9, 11, 12, 11, 3, 11, 12, 7, 1, 15, 1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 11, 1, 2, 3, 6, 7, 8, 11, 12, 13, 1, 2, 3, 4, 10, 11, 12, 1, 3, 4, 5, 6, 10, 12, 13, 3, 7, 11, 1, 2, 3, 1, 2, 4, 1, 8, 12, 2, 9, 4, 12, 2, 2, 3, 7, 12, 13, 10, 7, 8, 13, 3, 11, 1, 13, 1, 2, 3, 4, 7, 10, 11, 12, 15, 1, 11, 4, 10, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 15, 4, 5, 4, 8, 12, 7, 6, 7, 13, 5, 6, 2, 2, 3, 7, 12, 13, 10, 12, 9, 5, 6, 11, 7, 7, 10, 3, 4, 11, 14, 3, 5, 13, 3, 4, 4, 12, 11, 14, 1, 3, 4, 7, 1, 8, 9, 11, 13, 1, 7, 4, 2, 4, 2, 2, 1, 11, 1, 2, 7, 9, 3, 4, 1, 6, 1, 15, 14, 15, 5, 4, 10, 13, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 5, 12, 4, 7, 1, 2, 3, 4, 6, 8, 10, 11, 12, 3, 11, 4, 9, 2, 9, 1, 12, 1, 1, 12, 1, 15, 2, 1, 7, 13, 3, 10, 12, 5, 5, 11, 2, 3, 1, 2, 5, 6, 8, 10, 11, 12, 15, 2, 3, 2, 5, 6, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 11, 12, 5, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 4, 3, 15, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 4, 9, 5, 3, 9, 14, 1, 2, 3, 6, 9, 10, 11, 12, 14, 15, 3, 9, 11, 6, 4, 4, 2, 4, 1, 2, 3, 4, 5, 6, 7, 10, 12, 13, 10, 2, 14, 13, 2, 12, 2, 13, 7, 9, 11, 13, 10, 6, 11, 15, 7, 4, 7, 8, 5, 8, 9, 14, 12, 1, 5, 6, 7, 10, 11, 13, 1, 2, 3, 5, 7, 8, 11, 12, 1, 4, 5, 6, 11, 13, 15, 1, 3, 4, 6, 7, 12, 13, 1, 3, 4, 5, 7, 10, 13, 1, 3, 5, 10, 3, 4, 7, 13, 2, 3, 4, 3, 1, 1, 1, 2, 5, 7, 11, 13, 1, 1, 2, 3, 5, 7, 10, 7, 1, 2, 4, 10, 5, 2, 4, 15, 1, 8, 13, 8, 1, 7, 14, 4, 3, 9, 10, 1, 3, 4, 5, 10, 11, 12, 13, 1, 2, 3, 4, 6, 8, 14, 1, 3, 4, 5, 10, 14, 4, 5, 6, 15, 8, 6, 9, 9, 3, 3, 1, 4, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 7, 12, 1, 2, 5, 6, 7, 9, 11, 12, 1, 2, 5, 10, 11, 12, 13, 2, 3, 1, 2, 7, 8, 9, 10, 8, 5, 12, 2, 3, 9, 10, 15, 5, 5, 8, 8, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 1, 2, 4, 5, 6, 7, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 9, 10, 11, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 3, 10, 1, 4, 14, 1, 7, 13, 1, 3, 4, 9, 1, 15, 1, 7, 7, 1, 6, 8, 15, 1, 2, 3, 4, 5, 6, 12, 13, 1, 2, 4, 6, 7, 10, 14, 13, 10, 3, 7, 13, 14, 2, 4, 1, 2, 7, 14, 12, 11, 2, 1, 7, 12, 3, 14, 1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 8, 12, 1, 3, 5, 12, 7, 9, 1, 5, 6, 10, 11, 13, 11, 14, 7, 1, 2, 7, 8, 9, 11, 1, 5, 12, 1, 2, 3, 4, 6, 8, 9, 10, 3, 12, 13, 7, 1, 3, 7, 9, 2, 3, 4, 8, 9, 9, 1, 2, 4, 6, 15, 6, 1, 4, 6, 8, 11, 1, 8, 1, 2, 3, 4, 6, 8, 9, 10, 12, 13, 4, 14, 2, 7, 7, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 3, 6, 8, 12, 1, 10, 1, 2, 4, 5, 11, 12, 13, 2, 9, 11, 8, 10, 7, 1, 3, 7, 2, 3, 6, 7, 8, 7, 13, 1, 15, 1, 5, 11, 1, 2, 3, 4, 5, 6, 9, 10, 11, 13, 7, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 4, 6, 11, 2, 6, 14, 1, 6, 1, 12, 7, 1, 2, 3, 4, 6, 9, 11, 12, 13, 1, 3, 7, 3, 10, 11, 7, 11, 1, 2, 3, 4, 11, 1, 2, 1, 8, 15, 11, 1, 12, 1, 3, 6, 7, 10, 11, 7, 14, 3, 14, 3, 4, 5, 6, 15, 3, 6, 10, 11, 9, 8, 1, 4, 6, 8, 10, 13, 1, 7, 3, 1, 2, 3, 4, 5, 10, 3, 10, 5, 12, 1, 2, 3, 4, 5, 6, 13, 15, 3, 7, 1, 2, 3, 4, 7, 9, 11, 13, 15, 11, 1, 2, 3, 4, 7, 10, 14, 4, 6, 6, 1, 2, 3, 5, 6, 10, 12, 13, 13, 7, 2, 4, 6, 8, 11, 12, 1, 2, 4, 6, 8, 9, 1, 2, 4, 5, 7, 8, 9, 13, 14, 1, 4, 5, 9, 10, 11, 14, 11, 12, 5, 3, 1, 2, 3, 4, 9, 10, 11, 6, 10, 11, 4, 14, 7, 14, 1, 11, 1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 3, 6, 9, 1, 10, 12, 3, 13, 13, 1, 8, 9, 11, 1, 4, 6, 13, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 1, 2, 3, 4, 6, 7, 11, 13, 4, 6, 1, 2, 4, 6, 10, 11, 1, 2, 3, 5, 6, 7, 13, 1, 9, 1, 3, 12, 13, 1, 3, 4, 7, 12, 1, 2, 5, 9, 11, 6, 13, 2, 1, 2, 4, 6, 8, 1, 8, 9, 12, 13, 14, 1, 2, 4, 5, 6, 11, 12, 2, 4, 9, 6, 7, 4, 12, 2, 4, 10, 13, 7, 10, 11, 13, 2, 13, 6, 14, 1, 2, 3, 5, 11, 13, 1, 3, 4, 5, 6, 7, 10, 11, 12, 15, 3, 14, 1, 3, 6, 10, 1, 2, 9, 1, 6, 1, 3, 1, 3, 9, 2, 3, 13, 13, 3, 6, 3, 6, 6, 1, 2, 4, 6, 7, 9, 12, 13, 1, 3, 6, 9, 10, 11, 14, 9, 3, 4, 4, 3, 4, 1, 11, 1, 2, 5, 6, 11, 7, 9, 3, 8, 9, 15, 12, 6, 7, 13, 5, 6, 2, 1, 2, 3, 1, 6, 8, 15, 4, 11, 6, 1, 2, 5, 6, 8, 12, 3, 1, 4, 9, 11, 1, 9, 10, 9, 15, 7, 1, 2, 7, 11, 13, 7, 1, 2, 4, 9, 11, 12, 11, 1, 2, 3, 4, 5, 7, 13, 14, 1, 2, 3, 4, 11, 12, 8, 4, 12, 13, 1, 2, 3, 4, 5, 6, 12, 13, 15, 4, 9, 9, 1, 3, 4, 7, 11, 15, 6, 9, 11, 14, 1, 3, 4, 7, 9, 11, 14, 15, 5, 3, 5, 9, 11, 1, 2, 4, 9, 11, 7, 11, 13, 14, 2, 7, 8, 9, 10, 6, 2, 5, 9, 1, 2, 3, 4, 6, 10, 5, 11, 1, 7, 1, 2, 3, 4, 7, 14, 1, 7, 10, 12, 2, 3, 4, 7, 10, 1, 8, 9, 11, 1, 4, 7, 9, 13, 1, 2, 3, 4, 8, 9, 13, 7, 14, 8, 4, 1, 2, 3, 12, 11, 1, 4, 6, 8, 9, 10, 12, 2, 4, 5, 9, 11, 12, 9, 10, 1, 2, 4, 7, 12, 13, 3, 10, 12, 13, 1, 7, 10, 1, 15, 3, 1, 4, 5, 7, 9, 13, 15, 2, 6, 1, 2, 3, 4, 5, 7, 10, 11, 2, 14, 3, 8, 10, 1, 4, 5, 8, 10, 11, 1, 2, 4, 6, 7, 14, 15, 1, 6, 10, 11, 13, 14, 4, 6, 9, 11, 14, 6, 11, 11, 12, 1, 5, 7, 11, 11, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 10, 2, 1, 2, 1, 9, 1, 2, 3, 7, 12, 15, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 6, 3, 4, 1, 1, 2, 4, 6, 9, 12, 13, 11, 6, 6, 8, 1, 1, 5, 6, 15, 1, 2, 8, 10, 14, 15, 1, 5, 6, 7, 8, 10, 11, 12, 6, 2, 1, 7, 1, 4, 11, 1, 3, 5, 10, 8, 7, 1, 2, 3, 4, 5, 6, 7, 10, 2, 4, 7, 14, 4, 14, 1, 13, 5, 2, 5, 7, 9, 1, 2, 8, 9, 1, 9, 2, 5, 8, 11, 2, 6, 9, 3, 6, 14, 12, 5, 9, 1, 2, 6, 9, 1, 5, 6, 13, 11, 2, 3, 4, 7, 8, 14, 3, 7, 12, 14, 2, 3, 4, 6, 9, 14, 6, 9, 13, 1, 2, 3, 4, 6, 10, 11, 13, 4, 6, 11, 2, 3, 7, 13, 1, 9, 11, 15, 1, 2, 4, 6, 7, 1, 2, 3, 5, 1, 8, 7, 9, 3, 3, 4, 7, 12, 15, 1, 2, 3, 5, 6, 9, 11, 1, 3, 4, 9, 12, 1, 12, 1, 2, 3, 5, 6, 12, 1, 2, 3, 6, 9, 10, 13, 11, 12, 5, 4, 6, 8, 9, 11, 12, 2, 5, 14, 1, 4, 11, 15, 1, 2, 14, 10, 14, 1, 2, 5, 9, 12, 13, 1, 4, 5, 6, 11, 13, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 4, 5, 1, 1, 3, 4, 5, 7, 9, 10, 11, 12, 15, 11, 13, 7, 4, 1, 5, 7, 8, 10, 12, 1, 8, 10, 1, 2, 4, 8, 9, 4, 12, 12, 8, 1, 3, 3, 10, 12, 2, 5, 1, 13, 1, 13, 1, 2, 4, 5, 6, 8, 10, 1, 7, 1, 2, 3, 7, 9, 10, 11, 13, 15, 7, 4, 11, 3, 2, 3, 4, 7, 2, 3, 4, 14, 7, 5, 9, 1, 9, 11, 4, 1, 2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 3, 5, 7, 11, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 1, 3, 4, 7, 9, 10, 11, 3, 4, 5, 6, 7, 10, 11, 12, 6, 1, 3, 10, 1, 2, 3, 4, 7, 8, 1, 6, 10, 12, 3, 14, 8, 8, 9, 2, 5, 11, 4, 2, 2, 5, 11, 13, 1, 3, 14, 5, 6, 15, 3, 7, 12, 13, 1, 2, 3, 4, 5, 11, 15, 1, 2, 3, 4, 5, 7, 9, 11, 3, 7, 8, 9, 6, 4, 14, 7, 14, 8, 1, 1, 2, 5, 6, 7, 12, 5, 6, 3, 1, 2, 3, 6, 7, 10, 12, 13, 1, 4, 7, 13, 10, 11, 14, 1, 5, 12, 2, 1, 2, 3, 4, 6, 7, 8, 13, 1, 3, 4, 5, 7, 8, 10, 1, 2, 4, 6, 7, 9, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 3, 10, 12, 15, 2, 13, 14, 6, 12, 1, 2, 3, 7, 8, 12, 14, 8, 9], \"Freq\": [0.9685475276113474, 0.02934992507913174, 0.19789779087453627, 0.5201884788702096, 0.21486045866378223, 0.06219644856056854, 0.07165742770144114, 0.9076607508849212, 0.19781716662211415, 0.14129797615865297, 0.07535892061794826, 0.5651919046346119, 0.7568228475135937, 0.03593224201844956, 0.20436462647993187, 0.9883803700359219, 0.956202894587846, 0.2528628166603838, 0.6105711914482438, 0.10176186524137396, 0.030836928861022413, 0.9855166916828395, 0.9778968520404133, 0.13629081803454038, 0.8404600445463324, 0.9652428302131185, 0.017236479110948544, 0.11187270733673202, 0.13768948595290095, 0.10326711446467571, 0.6282082796601105, 0.1835160947842606, 0.3207676278582034, 0.10949279604775213, 0.04780671376732839, 0.020047976741137712, 0.09715557959166737, 0.0030843041140211865, 0.07248114667949788, 0.007710760285052966, 0.020047976741137712, 0.03855380142526483, 0.08019190696455085, 0.07687308938141192, 0.8840405278862372, 0.9667477299759607, 0.48625848036916713, 0.05484118199652261, 0.06763745779571122, 0.14258707319095879, 0.06763745779571122, 0.027420590998261305, 0.03473274859779765, 0.003656078799768174, 0.03838882739756583, 0.051185103196754435, 0.02376451219849313, 0.6658700094693525, 0.2853728612011511, 0.12981681765233594, 0.19472522647850393, 0.008113551103270996, 0.11358971544579395, 0.5436079239191568, 0.991426947517286, 0.9618689136607154, 0.2156613625227849, 0.017971780210232075, 0.5930687469376584, 0.16174602189208867, 0.2938603651774015, 0.6530230337275589, 0.9197197417697857, 0.06729656647095993, 0.15052686103100665, 0.8373056644849745, 0.015179323146492556, 0.06451212337259336, 0.6527108952991799, 0.0683069541592165, 0.0683069541592165, 0.12522941595856357, 0.08220762737781268, 0.8220762737781268, 0.9639088019407825, 0.9677471032854147, 0.04331809392737555, 0.682259979356165, 0.22741999311872166, 0.03248857044553167, 0.9777776908551441, 0.9654484387194768, 0.9609110654553373, 0.1105224290006841, 0.8841794320054728, 0.1830355870967046, 0.16895592655080424, 0.06687838759302668, 0.14079660545900352, 0.41886990124053547, 0.02111949081885053, 0.3315292040064305, 0.028416788914836903, 0.6346416190980241, 0.9399936005486239, 0.08942160878968634, 0.8942160878968634, 0.30840016354968375, 0.26453880695595094, 0.14392007632318574, 0.09183471536812804, 0.04797335877439525, 0.03700801962596205, 0.04797335877439525, 0.021930678296866398, 0.02604268047752885, 0.009594671754879049, 0.1287046000220788, 0.7722276001324729, 0.9462369644721014, 0.9609853481043101, 0.8545799984841904, 0.19791411648499907, 0.12369632280312441, 0.6729079960489968, 0.07293114064881585, 0.9237944482183341, 0.9120004831134894, 0.06840003623351171, 0.977715308390723, 0.9250924767427334, 0.044816578106089064, 0.5826155153791579, 0.1008373007387004, 0.0336124335795668, 0.2352870350569676, 0.9699508347435526, 0.4889123043649708, 0.21886326982403023, 0.08472126573833429, 0.01588523732593768, 0.03000544828232673, 0.01765026369548631, 0.044125659238715774, 0.08119121299923703, 0.01765026369548631, 0.9185963037203076, 0.06990170940620746, 0.6990170940620747, 0.20970512821862242, 0.024807272366663767, 0.9674836222998868, 0.9803395907982073, 0.06352473374608263, 0.921108639318198, 0.040924726392665006, 0.9208063438349626, 0.09428848653598351, 0.2121490947059629, 0.5185866759479093, 0.14143272980397528, 0.9756263898920307, 0.18009520882302066, 0.810428439703593, 0.9801181182787123, 0.9688106506047077, 0.029963215998083746, 0.9732405991598986, 0.540276983654747, 0.09093771012010593, 0.06419132479066302, 0.29955951568976075, 0.02358957453192696, 0.8334983001280859, 0.13367425568091945, 0.19329251230450184, 0.7731700492180074, 0.964799821272406, 0.06111083715768331, 0.8555517202075663, 0.9372880312398306, 0.9630849762613231, 0.030262228427550792, 0.9381290812540746, 0.8810077476194789, 0.11660396659669574, 0.11057203837754202, 0.12758312120485618, 0.00850554141365708, 0.646421147437938, 0.09356095555022786, 0.7884522305879579, 0.20169708224343108, 0.19393828991971748, 0.7757531596788699, 0.5210807343173052, 0.4751030224657783, 0.13314005365349793, 0.07608003065914166, 0.7608003065914167, 0.9839983097080016, 0.9839905502773639, 0.08713354733823005, 0.17939259746106187, 0.046129525061415905, 0.2050201113840707, 0.4254167311219467, 0.056380530630619444, 0.9639877431107335, 0.9373883476887518, 0.05408009698204337, 0.870246326058054, 0.9719957260510582, 0.931748270205971, 0.9184625223991293, 0.05806864110684759, 0.31937752608766173, 0.26130888498081417, 0.31937752608766173, 0.9956430183909968, 0.646351034083713, 0.3112060534477137, 0.03989821198047611, 0.9963041353504112, 0.21845887458884944, 0.7099913424137607, 0.1336280704370526, 0.8463111127679998, 0.7913525837053279, 0.02168089270425556, 0.18428758798617226, 0.01544694666620575, 0.16991641332826327, 0.803241226642699, 0.24195358326662003, 0.7527444812739289, 0.7765346486536366, 0.21178217690553727, 0.9328806341073886, 0.21241717411078387, 0.07080572470359463, 0.6372515223323516, 0.4802916412407971, 0.04482721984914106, 0.4674838641410425, 0.16308371033901167, 0.19064715433997137, 0.025266490334213073, 0.18835020067322472, 0.40885775268090246, 0.020672583000719787, 0.011976214087036991, 0.21557185356666583, 0.03592864226111098, 0.04311437071333317, 0.33772923725444315, 0.2994053521759248, 0.05030009916555536, 0.9666665627031343, 0.9777140752602038, 0.916381924723103, 0.7999903030542623, 0.07179400155615175, 0.051281429682965535, 0.06153771561955864, 0.9471480602650433, 0.9835378076054185, 0.9169692498708099, 0.19160768838131934, 0.13709860461766815, 0.1816969458788373, 0.2362060296424885, 0.10901816752730238, 0.019821485004964067, 0.018169694587883728, 0.1057145866931417, 0.9361338491356894, 0.9315607699865698, 0.17562829760695983, 0.27002850757070074, 0.14050263808556784, 0.07025131904278392, 0.08781414880347992, 0.05707919672226194, 0.0944002099637409, 0.1009862711240019, 0.2853604546212894, 0.5810351425421435, 0.09626617746260366, 0.03438077766521559, 0.07847388286597176, 0.9089891431975062, 0.20706213172275828, 0.45497957194686794, 0.06406855196802835, 0.023213243466676938, 0.017642065034674474, 0.11792327681071885, 0.01485647581867324, 0.008356767648003697, 0.012070886602672007, 0.0659256114453625, 0.012999416341339085, 0.24224427759164566, 0.12582591117624314, 0.024694804997206596, 0.13876128522239897, 0.07408441499161979, 0.108186764749667, 0.036454235948257356, 0.055269325469938575, 0.06232498404056903, 0.12582591117624314, 0.002351886190210152, 0.003527829285315228, 0.30937263677682775, 0.6468700587151853, 0.7444513875370208, 0.10768793656195898, 0.05150292618180646, 0.09364168396692085, 0.1675451555275165, 0.7958394887557034, 0.11469571457191166, 0.7946774509625308, 0.08192551040850833, 0.9864548715785033, 0.003253414053191606, 0.12688314807447262, 0.29606067884043613, 0.0553080389042573, 0.1106160778085146, 0.04880121079787409, 0.045547796744682484, 0.2602731242553285, 0.04880121079787409, 0.9979506008237876, 0.1507677857574158, 0.00837598809763421, 0.552815214443858, 0.0753838928787079, 0.2177756905384895, 0.8251813651502633, 0.1481094757962011, 0.8218051669794443, 0.11984658685116896, 0.05136282293621527, 0.9771987245165245, 0.15443897635261564, 0.8397619339173475, 0.20750635188818026, 0.747022866797449, 0.4173745706546442, 0.18865621954546744, 0.03787692438750698, 0.0488029602685186, 0.04443254591611395, 0.06118580093366511, 0.03569171721130465, 0.06118580093366511, 0.0014568047841348837, 0.007284023920674418, 0.059000593757462784, 0.01675325501755116, 0.018210059801686045, 0.0021852071762023255, 0.9412568462805666, 0.03765027385122267, 0.955302757310666, 0.9379529329150904, 0.9763523484953635, 0.8825676221332657, 0.10530912213706226, 0.04513248091588383, 0.007522080152647304, 0.8274288167912035, 0.743622468539213, 0.24024725906651495, 0.0038134485566113483, 0.011440345669834045, 0.9687140514586502, 0.15227175462032616, 0.8374946504117939, 0.1149611581316506, 0.08622086859873794, 0.7472475278557289, 0.9365939405881598, 0.5512289404120976, 0.08132886006080128, 0.09940194007431269, 0.027109620020267094, 0.0090365400067557, 0.0090365400067557, 0.027109620020267094, 0.18976734014186966, 0.23489269970924856, 0.05480829659882466, 0.10961659319764933, 0.22967286193793193, 0.0913471609980411, 0.03392894551355813, 0.046978539941849716, 0.013049594428291587, 0.023489269970924858, 0.14093561982554914, 0.01826943219960822, 0.014363773473092808, 0.17236528167711368, 0.24657811128809318, 0.5170958450313411, 0.01196981122757734, 0.0023939622455154677, 0.007181886736546404, 0.026333584700670147, 0.4994649740072189, 0.014270427828777684, 0.4566536905208859, 0.021405641743166524, 0.7451065819341525, 0.22353197458024573, 0.7432053637046896, 0.2346964306435862, 0.5057464636293958, 0.2038667915405316, 0.027443606553533104, 0.0078410304438666, 0.023523091331599803, 0.0078410304438666, 0.078410304438666, 0.0509666978851329, 0.0039205152219333, 0.047046182663199605, 0.0431256674412663, 0.07192749124052505, 0.195231761938568, 0.3493621003111217, 0.11302891480653937, 0.03082606767451074, 0.02055071178300716, 0.20550711783007158, 0.9917995127324019, 0.005667425787042296, 0.9636922231243696, 0.10555253810985316, 0.8444203048788252, 0.08185913119806144, 0.2889145806990404, 0.10513280575437303, 0.004815243011650674, 0.23353928606505767, 0.2552078796174857, 0.01605081003883558, 0.01364318853301024, 0.9099073625471478, 0.5704559658168706, 0.10696049359066324, 0.314939231128064, 0.9711694263170081, 0.29352173825278566, 0.6937786540520389, 0.11036866453027194, 0.6089305629256383, 0.009514540045713098, 0.058990148283421216, 0.0019029080091426199, 0.024737804118854057, 0.009514540045713098, 0.049475608237708114, 0.1065628485119867, 0.022834896109711438, 0.966104667282543, 0.009157143126207294, 0.4725085853122964, 0.11904286064069483, 0.05677428738248523, 0.05494285875724377, 0.049448572881519395, 0.027471429378621886, 0.18863714839987028, 0.021977143502897506, 0.3166718370277562, 0.06119262551260989, 0.15604119505715522, 0.2937246024605275, 0.021417418929413463, 0.06119262551260989, 0.08872930699328434, 0.26034991969560084, 0.35116965912429876, 0.03935522041910245, 0.14833890773354, 0.0787104408382049, 0.009081973942869796, 0.06508747992390021, 0.04540986971434898, 0.9681411512762669, 0.009738800918862428, 0.9836188928051052, 0.9766144273705868, 0.07095209358946063, 0.9223772166629881, 0.98552293262383, 0.968444523682993, 0.9404283351059557, 0.32723475597611795, 0.5411959425758874, 0.12585952152927612, 0.17884870561625757, 0.7966896886542383, 0.9195268661843444, 0.048396150851807596, 0.9902063230580505, 0.03562416400729159, 0.8161172118034072, 0.05181696582878777, 0.03562416400729159, 0.058294086557386236, 0.92071695691186, 0.009398130231843082, 0.8364335906340342, 0.15037008370948932, 0.14393968532775844, 0.7916682693026714, 0.3334244683802931, 0.6540249187459596, 0.01154342658921379, 0.02638497506106009, 0.09729459553765908, 0.08410210800712904, 0.013192487530530045, 0.016490609413162556, 0.028034036002376347, 0.044524645415538903, 0.6794131078222974, 0.9776442092466128, 0.9105188979030869, 0.9727162209520914, 0.966791962063247, 0.9806950100629915, 0.2190902334562438, 0.12188822847213564, 0.05245822491205837, 0.14194578505615796, 0.0370293352320412, 0.00462866690400515, 0.08331600427209271, 0.07868733736808756, 0.1188024505361322, 0.043200891104048074, 0.07714444840008584, 0.023143334520025753, 0.09253974957294106, 0.8328577461564696, 0.2691311131999331, 0.5616649318955125, 0.1521175857217013, 0.9827161472072347, 0.7567461399667946, 0.19741203651307684, 0.03290200608551281, 0.9117543931788229, 0.9237405542488013, 0.9629894376668009, 0.986043799271347, 0.8863855646568943, 0.005065060369467967, 0.05065060369467967, 0.055715664064147644, 0.9562236611083942, 0.992649679887394, 0.9696052694840476, 0.058137386097674285, 0.3682034452852705, 0.5619947322775181, 0.9650267933086493, 0.8710551205815393, 0.10247707300959287, 0.8979678503108708, 0.0888100071736026, 0.941993359944189, 0.9311559463369803, 0.1362663976923434, 0.40028254322125867, 0.4598990922116589, 0.13034242129379417, 0.8641219782070058, 0.8475568801403115, 0.14956886120123145, 0.9235778694019833, 0.060233339308825, 0.2957220092843397, 0.01056150033158356, 0.16898400530533697, 0.5069520159160109, 0.11322306550798278, 0.7925614585558795, 0.07548204367198852, 0.692938685182149, 0.2771754740728596, 0.974106334601652, 0.9686637966756992, 0.9567619269057345, 0.9684229718197561, 0.011955839158268594, 0.9622298505011622, 0.9874416677080182, 0.27684320392503653, 0.713403640883748, 0.9745126338643316, 0.018595110087987612, 0.8274823989154487, 0.15805843574789472, 0.8881372474756758, 0.09419637473226865, 0.9499226633067537, 0.047496133165337684, 0.2975509589993633, 0.6694896577485675, 0.7476115380142598, 0.1068016482877514, 0.9363745052142207, 0.9817304447561941, 0.190717785702136, 0.6992985475744986, 0.22506102274797102, 0.7652074773431015, 0.029862507052159683, 0.032577280420537834, 0.20089322925998332, 0.06786933420945383, 0.20360800262836148, 0.2090375493651178, 0.0027147733683781533, 0.0054295467367563065, 0.11673525484026058, 0.03529205378891599, 0.09773184126161351, 0.9454356531670378, 0.9486992436410715, 0.9457410400037705, 0.02627058444454918, 0.35166100340393186, 0.12382429697321544, 0.12630078291267974, 0.06191214848660772, 0.047053232849821866, 0.1461126704283942, 0.054482690668214796, 0.04457674691035756, 0.04457674691035756, 0.4320952421935711, 0.5636024898177014, 0.9808322165484624, 0.9674270035898443, 0.9724543674240744, 0.9849976901037724, 0.14543688666014853, 0.8435339426288615, 0.9853554703334552, 0.2621783775492334, 0.6991423401312891, 0.46002013986030177, 0.46002013986030177, 0.9741668943860434, 0.9696183772650923, 0.9897368937527165, 0.8932195017565474, 0.44848206734594864, 0.4703592413628242, 0.06563152205062663, 0.9307068236807285, 0.08843551659380551, 0.8843551659380552, 0.986596085774891, 0.9731207266778252, 0.005959255818439716, 0.4484340003375886, 0.09832772100425531, 0.02979627909219858, 0.3501062793333333, 0.01936758140992908, 0.010428697682269504, 0.02830646513758865, 0.010428697682269504, 0.766336317443449, 0.22725835620736765, 0.23888343944650514, 0.7431929227224605, 0.9259716108838347, 0.23952031354553652, 0.7403355145952947, 0.30416196721748795, 0.22842933165523893, 0.08927831021565942, 0.09297258512113499, 0.07203836065677346, 0.001231424968491854, 0.05787697351911714, 0.020318511980115594, 0.04494701134995267, 0.007388549810951125, 0.0794269104677246, 0.002462849936983708, 0.8789412424305995, 0.07324510353588329, 0.9764658165091946, 0.41152682137006136, 0.05144085267125767, 0.0038104335312042718, 0.06477737003047263, 0.17337472566979437, 0.13527039035775165, 0.024767817952827766, 0.0019052167656021359, 0.017146950890419222, 0.01905216765602136, 0.0743034538584833, 0.005715650296806408, 0.015241734124817087, 0.9514386742802874, 0.7070425699242573, 0.276188503876663, 0.4984814791169185, 0.09703265433191925, 0.12366906924656375, 0.06659103728661125, 0.05898063302528425, 0.045662425567961996, 0.022831212783980998, 0.02092861171864925, 0.0190260106533175, 0.022831212783980998, 0.022831212783980998, 0.06963180419505771, 0.8355816503406925, 0.9356590095799945, 0.011474578323958791, 0.9753391575364972, 0.9768416853563007, 0.5375450717791959, 0.1188449988627474, 0.10604630667752844, 0.05485153793665264, 0.02742576896832632, 0.05302315333876422, 0.01645546138099579, 0.047537999545098956, 0.014627076783107372, 0.020112230576772635, 0.7848020316538984, 0.08221735569707507, 0.12706318607729783, 0.9878968048414487, 0.9803041173304996, 0.9604812162729689, 0.9781000766488365, 0.9934412334930013, 0.17568824660781543, 0.557928891254549, 0.007122496484100626, 0.004748330989400417, 0.01661915846290146, 0.0023741654947002084, 0.04985747538870438, 0.0854699578092075, 0.07834746132510688, 0.023741654947002085, 0.9877137311411895, 0.3744923803843418, 0.5617385705765127, 0.9760797222372776, 0.21503912452359822, 0.7372769983666225, 0.1776571492635871, 0.7698476468088775, 0.10439803349146118, 0.22485730290468564, 0.12045926941322445, 0.5460820213399508, 0.9094076287219718, 0.8621151725933192, 0.11859785443082697, 0.01596509578876517, 0.9888891105865301, 0.9475149515264144, 0.9704702399909299, 0.9553027542926257, 0.8110658553029273, 0.07156463429143477, 0.11132276445334296, 0.9546713960400884, 0.9356000065755826, 0.4112696608847946, 0.23943781626854482, 0.08169054907985647, 0.05070447873922126, 0.02535223936961063, 0.022535323884098336, 0.1718318446162498, 0.42787331004628193, 0.10546173134943569, 0.05423746183685264, 0.10847492367370529, 0.018079153945617547, 0.018079153945617547, 0.03615830789123509, 0.22900261664448893, 0.021975477521572865, 0.17580382017258292, 0.47247276671381655, 0.10987738760786432, 0.049444824423538945, 0.06592643256471858, 0.09339577946668468, 0.21203899750501815, 0.07067966583500605, 0.06125571039033857, 0.4759097499557074, 0.018847910889334945, 0.04240779950100363, 0.11779944305834342, 0.009684044230407536, 0.038736176921630144, 0.11136650864968667, 0.5326224326724145, 0.08231437595846407, 0.062946287497649, 0.15978672980172437, 0.3523925999883495, 0.04315011428428769, 0.5034179999833563, 0.09349191428262332, 0.3107023463286296, 0.5178372438810493, 0.13213777947309535, 0.03571291337110685, 0.855306577139014, 0.028375099715512313, 0.11350039886204925, 0.9620154174920196, 0.9645282398611283, 0.985414966722676, 0.3350219059708217, 0.007444931243796038, 0.4541408058715583, 0.03722465621898019, 0.09678410616934849, 0.07444931243796038, 0.9914124260626338, 0.28368787165925635, 0.021584946756682547, 0.12642611671771206, 0.13876037200724495, 0.009250691467149663, 0.41936467984411807, 0.9635610407347257, 0.02000478166642028, 0.14003347166494196, 0.2000478166642028, 0.6201482316590287, 0.9510811519803976, 0.6302052773576007, 0.14004561719057793, 0.14004561719057793, 0.11194918375434433, 0.19902077111883434, 0.6716951025260659, 0.9360312234838278, 0.971748316259848, 0.3076028924851676, 0.5126714874752794, 0.9780668073544251, 0.8750554037488677, 0.05706883067927398, 0.06340981186585998, 0.2110438835202269, 0.06532310680387976, 0.13315864079252412, 0.26129242721551904, 0.02763669903241067, 0.02512427184764606, 0.0427112621409983, 0.23365572818310837, 0.13197366300666863, 0.25982314904437886, 0.004124176968958395, 0.5031495902129242, 0.0742351854412511, 0.012372530906875184, 0.012372530906875184, 0.15483045366830797, 0.04128812097821546, 0.7741522683415398, 0.010322030244553864, 0.9390661066662837, 0.8247211220331758, 0.944836792871336, 0.40327438451739045, 0.20163719225869522, 0.3629469460656514, 0.9835603378231766, 0.18626510056656265, 0.7916266774078912, 0.9704728492850665, 0.9892135688674684, 0.9841958541102702, 0.21992479978647428, 0.7064251144656446, 0.06664387872317402, 0.264911637002128, 0.10478399012340439, 0.10330815927659588, 0.23687085091276627, 0.011806646774468101, 0.0036895771170212814, 0.07231571149361711, 0.047226587097872405, 0.016972054738297895, 0.019185801008510663, 0.07748111945744691, 0.028778701512765997, 0.0022137462702127688, 0.011068731351063845, 0.22555357985325625, 0.7330491345230828, 0.46278486560933435, 0.03669659727298686, 0.040773996969985404, 0.020386998484992702, 0.1855216862134336, 0.1773668868194365, 0.026503098030490512, 0.04892879636398249, 0.06704079006777673, 0.11732138261860928, 0.15922187641096974, 0.050280592550832545, 0.4525253329574929, 0.11732138261860928, 0.033520395033888366, 0.988732057033226, 0.9780335927389722, 0.1516295966207353, 0.005547424266612267, 0.011094848533224533, 0.7507514174148602, 0.06102166693273493, 0.02034055564424498, 0.9711249460416197, 0.07380717946743294, 0.9173178019523808, 0.253459234092639, 0.09337971782360384, 0.253459234092639, 0.1734194759581214, 0.20009939533629395, 0.8913865256734292, 0.7074976008667874, 0.2606570108456585, 0.955501544885385, 0.1016138635836932, 0.22016337109800194, 0.027097030288984855, 0.09822673479757009, 0.07451683329470835, 0.07112970450858525, 0.033871287861231066, 0.01016138635836932, 0.03725841664735417, 0.04064554543347728, 0.28113168924821785, 0.20304653863821526, 0.17098655885323388, 0.04274663971330847, 0.1602998989249068, 0.0641199595699627, 0.21373319856654238, 0.14961323899657966, 0.2249934180257401, 0.3801868977860691, 0.05643399264011963, 0.01930636590319882, 0.040097836875874475, 0.04975101982747389, 0.0504935723622123, 0.017078708298983572, 0.04232549448008972, 0.05791909770959646, 0.06088930784855013, 0.07810871565754661, 0.8982502300617861, 0.27872309604652995, 0.708421202451597, 0.2628377681070029, 0.09856416304012608, 0.08624364266011032, 0.12156246774948884, 0.10431373921746677, 0.031211984962706593, 0.05585302572273811, 0.00821368025334384, 0.01807009655735645, 0.00410684012667192, 0.037782929165381667, 0.062423969925413186, 0.1075992113188043, 0.0750233256455938, 0.9169517578905909, 0.3456328681567556, 0.20331345185691505, 0.4472895940852131, 0.9954137578951606, 0.3459664801886022, 0.6270642453418416, 0.09924329764460858, 0.019848659528921717, 0.05954597858676515, 0.8137950406857903, 0.3863443411092388, 0.6081346110052833, 0.9851412830871201, 0.9419204702588664, 0.9608668337657061, 0.5262027276767611, 0.17809938475213455, 0.0809542657964248, 0.202385664491062, 0.10893535180321276, 0.017200318705770438, 0.034400637411540876, 0.19493694533206493, 0.4529417259186215, 0.11466879137180291, 0.017200318705770438, 0.05733439568590146, 0.004914560466222441, 0.7322695094671436, 0.03931648372977953, 0.08846208839200392, 0.004914560466222441, 0.0491456046622244, 0.08354752792578149, 0.9583843266909566, 0.9791621976775541, 0.9783420949773034, 0.09966670449785536, 0.6976669314849876, 0.14950005674678304, 0.9914778734006714, 0.005901654008337329, 0.12444282637517105, 0.31110706593792764, 0.09333211978137829, 0.43554989231309865, 0.9612607204256409, 0.9662718496356721, 0.9837383526971081, 0.7562029869179184, 0.11633892106429514, 0.12118637610864076, 0.9513364362788017, 0.03963901817828341, 0.2044627365155478, 0.009155047903681246, 0.003051682634560415, 0.04272355688384581, 0.10070552694049369, 0.11596394011329576, 0.05493028742208747, 0.01831009580736249, 0.11291225747873536, 0.33568508980164563, 0.9611219254777079, 0.9837412431866909, 0.0056710136392713155, 0.14177534098178288, 0.4423390638631626, 0.4083129820275347, 0.05495416805395097, 0.8792666888632156, 0.31191461089249695, 0.36092976403274646, 0.12030992134424882, 0.11139807531874892, 0.044559230127499565, 0.04901515314024952, 0.9039615813314897, 0.0786053548983904, 0.9792504194329542, 0.04608568171107395, 0.6781178880343738, 0.12508970750148643, 0.12508970750148643, 0.023042840855536977, 0.9571123091459133, 0.475383797444112, 0.05538452009057616, 0.4661530440956827, 0.1301596224581484, 0.07137785747704913, 0.008397394997299896, 0.07557655497569907, 0.10496743746624872, 0.43666453985959464, 0.07977525247434902, 0.09237134497029886, 0.2504231569007741, 0.08943684175027647, 0.6439452606019905, 0.9879520806385003, 0.015766329591415336, 0.5439383709038291, 0.4335740637639217, 0.9471321255226993, 0.14139905688302445, 0.7776948128566344, 0.07069952844151223, 0.9802485885012615, 0.956889757715464, 0.8863914402667056, 0.2584406606788282, 0.18460047191344872, 0.36920094382689744, 0.03692009438268975, 0.147680377530759, 0.9870693002533973, 0.1036669211251774, 0.31100076337553223, 0.11748917727520106, 0.36628978797562683, 0.09675579305016557, 0.2545663094290417, 0.7441169044848911, 0.44803253101840007, 0.12744036437856712, 0.01593004554732089, 0.04978139233537778, 0.11151031883124624, 0.04978139233537778, 0.009956278467075556, 0.0696939492695289, 0.06372018218928356, 0.05376390372220801, 0.2594307627103444, 0.6053384463241368, 0.8978788597599542, 0.098205500286245, 0.9541109020095371, 0.25252744557324935, 0.04456366686586753, 0.06238913361221455, 0.33719841261839767, 0.0475345779902587, 0.026738200119520522, 0.08764187816953949, 0.05793276692562779, 0.026738200119520522, 0.0014854555621955845, 0.026738200119520522, 0.028223655681716104, 0.6752008960407127, 0.019073471639568153, 0.07629388655827261, 0.16403185610028612, 0.06484980357453173, 0.14885529495438463, 0.8293366433172857, 0.29707760531262184, 0.1728894260425914, 0.12662324160865848, 0.08766224419060972, 0.255681545555945, 0.007305187015884143, 0.05113630911118901, 0.09353999716463716, 0.7616828340549027, 0.12026571064024778, 0.9553027562367017, 0.8157360119452831, 0.979138158739679, 0.008126016746614584, 0.5525691387697916, 0.4225528708239583, 0.23429888430948134, 0.14504216647729798, 0.4518621340254283, 0.13946362161278653, 0.02231417945804584, 0.15016228338705465, 0.7508114169352732, 0.422085188945418, 0.5276064861817725, 0.1265480281247024, 0.686975009819813, 0.1627046075889031, 0.03600240907191171, 0.030859207775924324, 0.08743442203178559, 0.10286402591974775, 0.3060204771112495, 0.09772082462376036, 0.06171841555184865, 0.05657521425586126, 0.023144405831943243, 0.19544164924752072, 0.9450504124991066, 0.44595179695102394, 0.1641451239591717, 0.033410069478415474, 0.03922051634422686, 0.08570409127071796, 0.0435783514935854, 0.06391491552392525, 0.026147010896151242, 0.013073505448075621, 0.01888395231388701, 0.06682013895683095, 0.7915917221088499, 0.2019923704691548, 0.9754120469933548, 0.26095360664988426, 0.7306700986196759, 0.9390590560208038, 0.004661885724786279, 0.9883197736546911, 0.1799053930799642, 0.7196215723198568, 0.9757009475169912, 0.2740430990120842, 0.08754154551774912, 0.3273292571533228, 0.03615846445298333, 0.1731800139590254, 0.04186769568240175, 0.03044923322356491, 0.015224616611782454, 0.013321539535309648, 0.2794023022363623, 0.5842048137669394, 0.13335109879462748, 0.2430939520974952, 0.6229282522498315, 0.1215469760487476, 0.094612776686361, 0.8042086018340685, 0.697630485071111, 0.011959379744076189, 0.11560733752606983, 0.09168857803791745, 0.07972919829384126, 0.19882122333853175, 0.795284893354127, 0.7566750808766112, 0.04323857605009207, 0.17295430420036828, 0.9571579976148563, 0.9177720378899206, 0.06403060729464562, 0.4079912640671298, 0.12178843703496411, 0.11569901518321592, 0.006089421851748206, 0.2801134051804175, 0.06089421851748206, 0.38887446261069275, 0.5833116939160392, 0.5759005882792976, 0.4052633769372835, 0.09163129587277831, 0.885769193436857, 0.42345074977576946, 0.23525041654209414, 0.30582554150472235, 0.3778390418987453, 0.15258884384372406, 0.39237131274100473, 0.07266135421129717, 0.9626105078808613, 0.9724662068062778, 0.33628782048127864, 0.059578550872667474, 0.04633887290096359, 0.528263151070985, 0.01588761356604466, 0.013239677971703884, 0.9703379144295796, 0.9791056158712702, 0.9781716704890702, 0.17185500344762145, 0.01534419673639477, 0.43270634796633256, 0.01534419673639477, 0.17185500344762145, 0.19026803953129517, 0.9706685393854204, 0.9804279590186112, 0.9358712893002012, 0.05615227735801208, 0.13337769583623457, 0.14449250382258746, 0.01111480798635288, 0.027787019965882204, 0.32232943160423355, 0.2056239477475283, 0.08891846389082304, 0.06113144392494085, 0.9718746693880188, 0.971271435727907, 0.2658605596015903, 0.06310309002216734, 0.03724116788193482, 0.3755151094761761, 0.10241321167532076, 0.019655060826576712, 0.06206861313655804, 0.05379279805168363, 0.01862058394096741, 0.9140093635115636, 0.08354481590892589, 0.07832326491461801, 0.4647180384934002, 0.18275428480077538, 0.14098187684631244, 0.039161632457309005, 0.007832326491461802, 0.9853145051093952, 0.008719597390348631, 0.9533600865661145, 0.18818512991979067, 0.21257949861309686, 0.05924346682660077, 0.33803625189295733, 0.04530382757328294, 0.03136418831996511, 0.0731831060799186, 0.048788737386612394, 0.9859203670912844, 0.9446613765589353, 0.017653155169844524, 0.2559707499627456, 0.044132887924611314, 0.15005181894367847, 0.3795428361516573, 0.1412252413587562, 0.0803172826787245, 0.05476178364458489, 0.08761885383133582, 0.04380942691566791, 0.5877764777852111, 0.1423806374759207, 0.08653805536517337, 0.28291287330922066, 0.04659741442740105, 0.09652321559961645, 0.03494806082055078, 0.018306127096478983, 0.14811321014423903, 0.23298707213700523, 0.04992580117221541, 0.09643892822961343, 0.18828552654353098, 0.03673863932556702, 0.1239929077237887, 0.0413309692412629, 0.4959716308951548, 0.013776989747087633, 0.8643803796572544, 0.09604226440636161, 0.9607975000288544, 0.9845115729741838, 0.10251111209360725, 0.17866222393457265, 0.30753333628082175, 0.06443555617312456, 0.07322222292400518, 0.061506667256164355, 0.2108800020211349, 0.3044433165375675, 0.6729799628725176, 0.016023332449345656, 0.5123395625194492, 0.468424742874925, 0.11513740504523343, 0.8635305378392507, 0.9784332163232672, 0.9906946124066052, 0.28364429719245926, 0.009561043725588515, 0.2700994852478755, 0.003983768218995215, 0.05895976964112917, 0.07728510344850716, 0.05656950870973205, 0.12270006114505261, 0.08206562531130142, 0.0023902609313971286, 0.03187014575196172, 0.02139671466094315, 0.05884096531759366, 0.9147095517553196, 0.8929837536590396, 0.09206018078959172, 0.9220610682726338, 0.23406949446149483, 0.7022084833844845, 0.9690614901239508, 0.40399683234036016, 0.20199841617018008, 0.01496284564223556, 0.374071141055889, 0.9708059808086926, 0.9514411907968298, 0.9418176689741026, 0.0376727067589641, 0.3110622427727198, 0.12627279162060903, 0.003079824185868513, 0.07494238852280048, 0.20121518014340953, 0.04209093054020301, 0.05646344340758941, 0.06262309177932643, 0.022585377363035765, 0.03079824185868513, 0.06775613208910729, 0.0010266080619561711, 0.025020403411963613, 0.41852311161830047, 0.10690536003293544, 0.28204818391668074, 0.034118731925404926, 0.08870870300605281, 0.004549164256720657, 0.04094247831048591, 0.9553801535786173, 0.9808821636463706, 0.47570208513661655, 0.3967646212274836, 0.002077301681819286, 0.058164447090940014, 0.02700492186365072, 0.03946873195456644, 0.0454508517600518, 0.18593530265475736, 0.1032973903637541, 0.020659478072750818, 0.008263791229100327, 0.06197843421825246, 0.5702015948079227, 0.9550090206419158, 0.027681420888171472, 0.8455461288430438, 0.14578381531776619, 0.8713281092340528, 0.9755299532399851, 0.14411520140623138, 0.08868627778845008, 0.04434313889422504, 0.15520098612978764, 0.5432034514542567, 0.4479777259067397, 0.29387338819482123, 0.0035838218072539174, 0.06450879253057051, 0.19352637759171154, 0.8982782532380345, 0.0909648864038516, 0.9921781851233324, 0.20699153212335838, 0.7069556943290086, 0.031844851095901294, 0.038213821315081546, 0.009553455328770387, 0.8456121134803228, 0.14623367375975507, 0.017192776811970192, 0.5329760811710759, 0.30946998261546343, 0.12034943768379133, 0.22078668585842184, 0.5017879224055042, 0.05017879224055042, 0.07777712797285315, 0.06774136952474306, 0.05017879224055042, 0.03010727534433025, 0.9783364671453727, 0.2795009394469427, 0.7151935803495298, 0.9530016212391588, 0.9658807594640093, 0.8647236226200905, 0.12353194608858435, 0.022058290380280423, 0.34190350089434657, 0.14889346006689286, 0.4797678157710992, 0.011774272669707852, 0.47097090678831405, 0.40032527077006697, 0.09419418135766282, 0.31625165024768964, 0.6588576046826868, 0.9820787741944286, 0.9335678008642659, 0.16165715708850414, 0.23709716372980608, 0.2694285951475069, 0.2505685934871814, 0.002694285951475069, 0.078134292592777, 0.049303007541992704, 0.5176815791909234, 0.002241045797363305, 0.07843660290771566, 0.10757019827343862, 0.03137464116308627, 0.022410457973633047, 0.08740078609716888, 0.08291869450244227, 0.015687320581543134, 0.9715286506800191, 0.02556654343894787, 0.1539093199468947, 0.16790107630570328, 0.6016455234287701, 0.06995878179404304, 0.04251755752760935, 0.3082522920751678, 0.6377633629141403, 0.9791291027221387, 0.9638938679741641, 0.10580808265778419, 0.8817340221482016, 0.020846132235683403, 0.9589220828414365, 0.945945108081392, 0.060567233741574696, 0.24226893496629878, 0.6662395711573217, 0.9536382682261093, 0.21695533301547734, 0.7790668776464869, 0.16897533090149167, 0.8110815883271599, 0.9966284129030054, 0.15127346447000084, 0.46894773985700255, 0.09328530308983385, 0.08067918105066711, 0.007563673223500041, 0.05042448815666694, 0.03529714170966686, 0.10841264953683392, 0.3230386577056928, 0.126138904437461, 0.23997157429565752, 0.0984498766341159, 0.14767481495117385, 0.055378055606690194, 0.009229675934448366, 0.9841419710500181, 0.8514689365982008, 0.1439984231011663, 0.9289503984779099, 0.9346274587089791, 0.06172068123549863, 0.6015764002714632, 0.3955570851100032, 0.1577468698935742, 0.004639613820399241, 0.5474744308071104, 0.1020715040487833, 0.19022416663636887, 0.9829201025306848, 0.96217934723611, 0.9605750262633284, 0.958364452635563, 0.004457509082025875, 0.031202563574181122, 0.9537716373798019, 0.962037498086141, 0.13189460981183576, 0.7913676588710146, 0.8889153203044631, 0.9747623953042743, 0.9837334164956328, 0.9751012335511413, 0.9731969660399498, 0.9561928049812315, 0.11350888932837938, 0.016215555618339914, 0.729700002825296, 0.1297244449467193, 0.9116108499383204, 0.08693836443553464, 0.9539075890292906, 0.3555984254425314, 0.004558954172340146, 0.1185328084808438, 0.4331006463723139, 0.004558954172340146, 0.07750222092978248, 0.9796693609316298, 0.020591105298312734, 0.7536344539182461, 0.04530043165628802, 0.18120172662515208, 0.06038064085698597, 0.12076128171397194, 0.7849483311408175, 0.3162396251529613, 0.5534193440176822, 0.9657315410000187, 0.22846440630179854, 0.01713483047263489, 0.10280898283580935, 0.03426966094526978, 0.6111422868573111, 0.9720676346419908, 0.016443866902621198, 0.3453212049550452, 0.14799480212359079, 0.2137702697340756, 0.021925155870161597, 0.2521392925068584, 0.9338263292099553, 0.13132451541356785, 0.614499619293676, 0.08424591554832656, 0.04707859986524131, 0.07433463136617048, 0.009911284182156064, 0.029733852546468195, 0.007433463136617049, 0.039335647423501, 0.5979018408372152, 0.196678237117505, 0.1258740717552032, 0.039335647423501, 0.9128299206662339, 0.9605861357615646, 0.9751112575631458, 0.1956325255505228, 0.7825301022020912, 0.0883285183595239, 0.034796082990115475, 0.15792068433975484, 0.005353243536940842, 0.026766217684704212, 0.09368176189646474, 0.33190109929033224, 0.23554271562539708, 0.024089595916233792, 0.4949977226324373, 0.4949977226324373, 0.9159466253413333, 0.23782961046762605, 0.16421568341812273, 0.22084178114850989, 0.1868661225102776, 0.10192697591469688, 0.08493914659558073, 0.05180132687603335, 0.7666596377652936, 0.08288212300165336, 0.09324238837686002, 0.013353930547624465, 0.6409886662859744, 0.17360109711911806, 0.060092687464310096, 0.003338482636906116, 0.0400617916428734, 0.01001544791071835, 0.05675420482740398, 0.9682571254103949, 0.004842404409626649, 0.7118334482151174, 0.22275060284282586, 0.06295125732514643, 0.052563724373531635, 0.6780720444185581, 0.13666568337118226, 0.03153823462411898, 0.09987107630971011, 0.9706555064380495, 0.9670299591783417, 0.7307730712017209, 0.16863993950808942, 0.42132135343065064, 0.0039011236428763947, 0.4369258480021562, 0.10142921471478626, 0.031208989143011158, 0.956040865674363, 0.07108240211391237, 0.2310178068702152, 0.6752828200821674, 0.0024637389573798425, 0.6307171730892397, 0.05666599601973638, 0.15275181535755025, 0.12811442578375182, 0.029564867488558112, 0.7132530019154573, 0.27557502346733576, 0.16172362851008298, 0.808618142550415, 0.0054265846189529236, 0.05969243080848216, 0.1899304616633523, 0.6240572311795862, 0.09767852314115262, 0.016279753856858772, 0.21485049739662573, 0.021485049739662573, 0.4619285694027453, 0.2900481714854447, 0.6683915240373068, 0.2730623909176802, 0.004075558073398212, 0.008151116146796424, 0.044831138807380336, 0.4181691144133146, 0.39576719756974416, 0.1194768898323756, 0.05227113930166433, 0.35310726902169143, 0.21186436141301487, 0.01008877911490547, 0.10593218070650744, 0.3177965421195223, 0.3377319818475757, 0.15638820834321238, 0.04492001729007164, 0.15971561703136583, 0.05822965204268546, 0.08152151285975964, 0.15971561703136583, 0.3178459224608626, 0.5562303643065095, 0.9759592683761737, 0.9647539726457078, 0.13318862880225518, 0.12535400357859314, 0.3682273855121173, 0.36039276028845524, 0.9958775193367047, 0.4853793702180213, 0.030179028200084224, 0.25400682068404223, 0.0679028134501895, 0.025149190166736855, 0.135805626900379, 0.9214284519997549, 0.9830256155616491, 0.9262879309600806, 0.05937743147180003, 0.7342258960570387, 0.18653306548476117, 0.07540698391937155, 0.9542656132284552, 0.9647182336749771, 0.16117315525273695, 0.6198967509720652, 0.05785703009072608, 0.07025496511016739, 0.04959174007776521, 0.03719380505832391, 0.30230843317633976, 0.5857225892791583, 0.1039185239043668, 0.9547533652403981, 0.13688572318229267, 0.06844286159114633, 0.6844286159114633, 0.050243680586417996, 0.9043862505555239, 0.9843121028362277, 0.14211322561643397, 0.20455691566001857, 0.09904861179327215, 0.35097660265876873, 0.0882824583374817, 0.10120184248443025, 0.015072614838106633, 0.9033461585701584, 0.09033461585701584, 0.3990111033280999, 0.0032178314784524183, 0.1544559109657161, 0.08366361843976287, 0.0707922925259532, 0.009653494435357256, 0.22203037201321688, 0.057920966612143535, 0.29068692783533534, 0.6685799340212714, 0.10701034891598225, 0.856082791327858, 0.0428041395663929, 0.07992265380720556, 0.10656353840960742, 0.035521179469869144, 0.5150571023131025, 0.13320442301200927, 0.1154438332770747, 0.00563911103710176, 0.02255644414840704, 0.3947377725971232, 0.10150399866783168, 0.3609031063745126, 0.09586488763072992, 0.01127822207420352, 0.3301295954339319, 0.044832414194731496, 0.13042156856649162, 0.04075674017702863, 0.39126470569947486, 0.05298376223013722, 0.07968532573067666, 0.3028042377765713, 0.5131734977055576, 0.08606015178913079, 0.01593706514613533, 0.9692737073525762, 0.9591333230917438, 0.17875784120935367, 0.7746173119071993, 0.20696869326574735, 0.4443739590705752, 0.2739291528517244, 0.06696045958597709, 0.9284450708429928, 0.08895641576641226, 0.9043902269585247, 0.9653087050876613, 0.10348380301787423, 0.28693236291319674, 0.037630473824681536, 0.11759523070212981, 0.16620125939234345, 0.054877774327660576, 0.0015679364093617307, 0.040766346643405, 0.034494601005958074, 0.02195110973106423, 0.13170665838638537, 0.9123830627893542, 0.991520173739794, 0.8575557706018807, 0.12993269251543646, 0.970019288831136, 0.025982659522262568, 0.020588466533956087, 0.07647144712612261, 0.7735380997757787, 0.044118142572763044, 0.07353023762127174, 0.011764838019403479, 0.4176746542938002, 0.00518206767113896, 0.1419886541892075, 0.038347300766428304, 0.11711472936774049, 0.06011198498521193, 0.022801097753011423, 0.02072827068455584, 0.09535004514895687, 0.006218481205366752, 0.014509789479189088, 0.04560219550602285, 0.014509789479189088, 0.9614581561534926, 0.19927085345744777, 0.786595474174136, 0.9703384826332471, 0.31531715245460423, 0.029106198688117316, 0.009702066229372438, 0.004851033114686219, 0.4074867816336424, 0.0776165298349795, 0.14553099344058656, 0.9720802708998153, 0.9901427519007414, 0.9890864705119786, 0.9553027543495629, 0.9890413045279031, 0.021398995828281975, 0.4493789123939215, 0.3423839332525116, 0.1711919666262558, 0.2245350298608561, 0.19085477538172768, 0.09542738769086384, 0.39293630225649817, 0.01684012723956421, 0.07858726045129963, 0.10250870058166013, 0.005694927810092229, 0.14237319525230574, 0.3530855242257182, 0.19932247335322803, 0.08542391715138345, 0.10820362839175236, 0.831078765175922, 0.9307202689718287, 0.9832536341124706, 0.936969634004552, 0.05424561038973722, 0.9942417271021379, 0.9732749293318121, 0.015955326710357577, 0.01397731283281497, 0.04193193849844491, 0.7687522058048234, 0.18170506682659462, 0.9931251454297941, 0.9662944337304937, 0.13601736817324353, 0.3879013833088797, 0.09403669898397082, 0.11418742019482173, 0.0016792267675709077, 0.13601736817324353, 0.10411205958939627, 0.026867628281134523, 0.15238117841528215, 0.4723816530873747, 0.07619058920764107, 0.28952423898903606, 0.03970267177632613, 0.9131614508555009, 0.9129970653697015, 0.08339877039434773, 0.9613032743742792, 0.06860911981303368, 0.9090708375226964, 0.22487247827972734, 0.762067843059076, 0.2054277515206609, 0.6666711936142202, 0.03488395780539525, 0.0891478921693434, 0.07875179971771792, 0.9056456967537561, 0.6794744032950109, 0.3156028649184477, 0.7468132985941307, 0.22578076469124883, 0.11893134493561063, 0.7908934438218107, 0.08325194145492744, 0.308864156373651, 0.0308864156373651, 0.617728312747302, 0.9406673674267175, 0.027255527707192213, 0.9539434697517274, 0.8212122699570839, 0.10760712502885926, 0.028317664481278752, 0.03964473027379025, 0.040917955885951846, 0.43373033239108955, 0.032734364708761475, 0.4746482882770414, 0.9809090452971988, 0.009228145666938074, 0.5502281853911827, 0.14880384887937645, 0.026530918792446963, 0.029991473417548743, 0.2353177145069209, 0.8692351572436505, 0.005794901048291003, 0.10430821886923806, 0.01738470314487301, 0.021645397227990422, 0.5844257251557413, 0.00360756620466507, 0.3282885246245214, 0.05411349306997605, 0.010822698613995211, 0.7892853426487993, 0.084999959977563, 0.10928566282829529, 0.283632359989308, 0.10045312749621327, 0.2304512924913128, 0.0354540449986635, 0.005909007499777251, 0.27181434498975354, 0.0354540449986635, 0.029545037498886255, 0.6914367059434765, 0.2834890494368254, 0.020743101178304295, 0.012462111324663348, 0.024924222649326696, 0.8972720153757611, 0.04984844529865339, 0.5320032884635627, 0.42560263077085014, 0.04256026307708501, 0.976887365205945, 0.16377190585100168, 0.00496278502578793, 0.5409435678108844, 0.06451620533524309, 0.2183625411346689, 0.8584474986502988, 0.0726378652704099, 0.06603442297309992, 0.9516795039727066, 0.9392604330664237, 0.047557490281844236, 0.155178843270274, 0.8146889271689385, 0.9891499936681156, 0.26918272919378405, 0.6913101908840363, 0.036706735799152374, 0.5624131504791346, 0.3749421003194231, 0.2801473566393866, 0.05033897814613978, 0.17509209789961663, 0.22980837849324684, 0.037207070803668536, 0.19041265646583308, 0.03501841957992333, 0.40183826324887895, 0.15263623952864394, 0.43610354151041125, 0.006230050593005875, 0.0031150252965029377, 0.9896987605883888, 0.004781153432794149, 0.1828045169625747, 0.015233709746881223, 0.15233709746881224, 0.4950955667736398, 0.015233709746881223, 0.1294865328484904, 0.1279530360789091, 0.0042651012026303035, 0.19619465532099395, 0.19619465532099395, 0.3540033998183152, 0.05118121443156364, 0.06397651803945455, 0.016243325896667386, 0.974599553800043, 0.9050436798145421, 0.019391507705841717, 0.8532263390570356, 0.05090270772783451, 0.004847876926460429, 0.021815446169071932, 0.048478769264604295, 0.3505977595634405, 0.11686591985448018, 0.4674636794179207, 0.35315139905172205, 0.23543426603448137, 0.21189083943103323, 0.1883474128275851, 0.1679385675631728, 0.4133872432324253, 0.40046889188141205, 0.9477389834951311, 0.975289900075842, 0.2974314090420063, 0.012589689271619315, 0.6247633301041084, 0.0031474223179048287, 0.045637623609620016, 0.014163400430571729, 0.1546869649617802, 0.11932994439908758, 0.1900439855244728, 0.15910659253211676, 0.022098137851682884, 0.34915057805658956, 0.8857846467355958, 0.3735591627672978, 0.027692871687796525, 0.08602466524294239, 0.14965934912128334, 0.05892100359105643, 0.056564163447414176, 0.0329957620109916, 0.047726012908755715, 0.056564163447414176, 0.02180077132869088, 0.023568401436422574, 0.005892100359105644, 0.04890443298057684, 0.008838150538658465, 0.9398116405221877, 0.012048867186181894, 0.03614660155854568, 0.9694172569155645, 0.18844645249112396, 0.3128577026794388, 0.1811281436565172, 0.0493985846335956, 0.0329323897557304, 0.0018295772086516888, 0.135388713440225, 0.04208027579898884, 0.04208027579898884, 0.012807040460561822, 0.6944860189335521, 0.11574766982225869, 0.9804652695999775, 0.9646014174173516, 0.26572615409283523, 0.1444163880939322, 0.08087317733260203, 0.05198989971381559, 0.057766555237572875, 0.39281257561549554, 0.0028628052177771396, 0.9475885270842332, 0.04866768870221137, 0.2733743898525008, 0.05896310369367664, 0.11792620738735328, 0.2948155184683832, 0.25193326123661836, 0.6378584989144462, 0.35118052187424564, 0.973319473734994, 0.9796295276016102, 0.9773426370132217, 0.968062698901205, 0.9710474322958885, 0.9472009957101952, 0.9539767941287066, 0.9912501284554124, 0.9698741880534011, 0.9814661433975276, 0.015991301725417965, 0.8901587582327914, 0.10337327514961449, 0.2805698541935484, 0.17378580520943668, 0.09212741480982187, 0.07537697575349062, 0.1381911222147328, 0.07956458551757342, 0.16122297591718826, 0.11633054081894069, 0.879749714943239, 0.35442809333537223, 0.1253226407373692, 0.19190029362909658, 0.019581662615213935, 0.05091232279955624, 0.035246992707385086, 0.07441031793781296, 0.09595014681454829, 0.05091232279955624, 0.9440567304535301, 0.11262620036398908, 0.8634675361239164, 0.9814775080692293, 0.1870064326382517, 0.39738866935628486, 0.15428030692655764, 0.2524586840616398, 0.08473535397307899, 0.581592656815224, 0.33123820189476333, 0.9986251870793631, 0.9473616366982902, 0.8301823942102927, 0.09224248824558808, 0.9504690837437296, 0.004010418074868058, 0.04010418074868058, 0.9772182956845562, 0.3559726280457662, 0.1842681839295731, 0.10993272336707485, 0.04606704598239327, 0.02198654467341497, 0.04606704598239327, 0.08689920037587821, 0.04187913271126661, 0.04187913271126661, 0.05444287252464659, 0.010469783177816652, 0.36115362862482175, 0.14188178267403712, 0.0483687895479672, 0.07953978725665718, 0.03547044566850928, 0.03762016964841894, 0.041919617608238244, 0.0763152012867927, 0.01934751581918688, 0.021497239799096533, 0.1246839908347599, 0.012898343879457921, 0.6785498709797906, 0.010685824739839221, 0.042743298959356885, 0.06233397764906212, 0.11932504292820463, 0.08014368554879416, 0.005342912369919611, 0.25017304101775256, 0.09907843208623865, 0.049539216043119325, 0.15481005013474788, 0.05573161804850924, 0.08173970647114688, 0.05820857885066521, 0.10155539288839462, 0.024769608021559662, 0.008669362807545882, 0.07554730446575697, 0.03963137283449546, 0.5238202608664569, 0.2933393460852159, 0.02739982902993775, 0.04674088481577616, 0.009670527892919206, 0.08058773244099338, 0.01934105578583841, 0.4702256371039402, 0.20046461371273241, 0.002474871774231264, 0.07424615322693794, 0.0346482048392377, 0.12126871693733195, 0.05939692258155035, 0.03712307661346897, 0.952779644556243, 0.9901420423579723, 0.3040132618607093, 0.686201362485601, 0.7097920349849235, 0.1195058018086861, 0.13761274147666883, 0.0036213879335965484, 0.02534971553517584, 0.9656274675667003, 0.22313368119162286, 0.1785069449532983, 0.11602951421964389, 0.46411805687857555, 0.9879408736802493, 0.9207425871668599, 0.9652845290805083, 0.9586425855660322, 0.02738978815902949, 0.9854104253994351, 0.27084072404667087, 0.7191288190204709, 0.9209025446380886, 0.9835003581479431, 0.1473283633335195, 0.3634099628893481, 0.06384229077785845, 0.4174303627783053, 0.9703567602190537, 0.6763712879121845, 0.3074414945055384, 0.9297465863701151, 0.048934030861585, 0.9921600352551541, 0.47751446426590377, 0.01110498754106753, 0.27207219475615446, 0.23875723213295189, 0.36834787759273957, 0.03550340988845683, 0.11538608213748469, 0.09319645095719917, 0.1553274182619986, 0.21745838556679806, 0.01331377870817131, 0.11668354399250158, 0.17335840821743093, 0.11334972844985869, 0.2667052434114322, 0.11334972844985869, 0.05000723313964354, 0.006667631085285805, 0.1533555149615735, 0.1672763239569631, 0.06589673368001576, 0.3142767298585367, 0.4511391767324156, 0.9614872909091158, 0.6060793082493281, 0.35813777305642114, 0.21894476234948596, 0.7298158744982866, 0.981243038914488, 0.9856861285808418, 0.950670555696846, 0.01901341111393692, 0.01901341111393692, 0.9756887024117848, 0.14726824763643692, 0.809975362000403, 0.9918084213362447, 0.9610142496608016, 0.9847460967613485, 0.3798370183054971, 0.0896244649934319, 0.08535663332707799, 0.0085356633327078, 0.042678316663538995, 0.2944803849784191, 0.0512139799962468, 0.0469461483298929, 0.5775188256963976, 0.025478771721899895, 0.06794339125839971, 0.32697757043104864, 0.7533520807804232, 0.20876021515602086, 0.027229593281220113, 0.870182198304212, 0.11087805430005282, 0.016842236096210553, 0.9776609437342243, 0.004584833500533034, 0.24987342577905036, 0.1742236730202553, 0.08481941975986114, 0.16963883951972228, 0.004584833500533034, 0.02521658425293169, 0.2842596770330481, 0.12171801894686249, 0.08317397961368936, 0.17243386017472184, 0.4645571056471918, 0.024343603789372496, 0.08723124691191811, 0.04260130663140187, 0.01366212527679828, 0.05074503674239361, 0.4937882421471378, 0.01366212527679828, 0.13857298495038256, 0.0468415723775941, 0.06050369765439238, 0.11710393094398525, 0.02732425055359656, 0.007806928729599017, 0.02927598273599631, 0.012431585733152006, 0.07902936644646633, 0.33654078520461506, 0.2948061759576047, 0.0035518816380434304, 0.06926169194184689, 0.020423319418749725, 0.05150228375162974, 0.030190993923369158, 0.03285490515190173, 0.011543615323641149, 0.057718076618205744, 0.3447029289303598, 0.6278517634088697, 0.012310818890369994, 0.9958541891964442, 0.20832264505973716, 0.7660897269938721, 0.02016025597352295, 0.13478353055948067, 0.8491362425247283, 0.15986910794214038, 0.19983638492767547, 0.00799345539710702, 0.0879280093681772, 0.15187565254503335, 0.35970549286981585, 0.023980366191321056, 0.9553027538526587, 0.958098609650252], \"Term\": [\"accident\", \"accident\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"achieve\", \"achieve\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"activity\", \"activity\", \"activity\", \"administration\", \"advertisement\", \"age\", \"age\", \"age\", \"age\", \"agreement\", \"agriculture\", \"allowed\", \"allowed\", \"almost\", \"almost\", \"along\", \"along\", \"along\", \"along\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"amazon\", \"amazon\", \"analyse\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"angeles\", \"angeles\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"answered\", \"apart\", \"applied\", \"applied\", \"applied\", \"applied\", \"approximate\", \"approximate\", \"arabic\", \"arabic\", \"architecture\", \"architecture\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"argentina\", \"argentina\", \"array\", \"arrest\", \"art\", \"art\", \"art\", \"art\", \"asset\", \"assignment\", \"athlete\", \"attack\", \"attack\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribution\", \"attribution\", \"attribution\", \"au\", \"automatic\", \"automatic\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"averaged\", \"averaged\", \"award\", \"awarded\", \"awesome\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"ball\", \"ball\", \"band\", \"bar\", \"base\", \"base\", \"base\", \"base\", \"base\", \"baseball\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"baseline\", \"battle\", \"battle\", \"battle\", \"beneficial\", \"beneficial\", \"benefit\", \"betting\", \"betting\", \"bias\", \"bias\", \"bigger\", \"bigger\", \"bigger\", \"bigger\", \"bike\", \"bill\", \"bill\", \"bird\", \"birth\", \"birth\", \"bitcoin\", \"body\", \"body\", \"body\", \"body\", \"book\", \"book\", \"book\", \"boolean\", \"boolean\", \"borough\", \"boston\", \"boston\", \"breakdown\", \"breast\", \"british\", \"british\", \"bureau\", \"bureau\", \"business\", \"business\", \"business\", \"business\", \"business\", \"ca\", \"ca\", \"calculation\", \"calculation\", \"california\", \"california\", \"campaign\", \"campaign\", \"campaign\", \"cancer\", \"caput\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"carbon\", \"card\", \"card\", \"carnegie\", \"carolina\", \"carry\", \"cast\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"cell\", \"census\", \"census\", \"census\", \"center\", \"centre\", \"centre\", \"ch\", \"ch\", \"character\", \"character\", \"character\", \"chemical\", \"chemical\", \"chemical\", \"chest\", \"chest\", \"chicago\", \"chicago\", \"chinese\", \"christian\", \"christian\", \"christian\", \"citation\", \"citation\", \"citation\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classic\", \"clear\", \"clinical\", \"close\", \"close\", \"close\", \"close\", \"club\", \"cnn\", \"coco\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"coded\", \"coin\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collection\", \"collection\", \"collection\", \"collection\", \"college\", \"college\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"combine\", \"combine\", \"come\", \"come\", \"come\", \"come\", \"command\", \"command\", \"comment\", \"comment\", \"comment\", \"commission\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"company\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"compute\", \"compute\", \"conference\", \"conference\", \"conference\", \"conflict\", \"considered\", \"considered\", \"consistent\", \"consistent\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"conversation\", \"conversation\", \"convnet\", \"convolution\", \"convolutional\", \"cool\", \"coordinate\", \"coordinate\", \"coordinate\", \"coordinate\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"correlate\", \"correspond\", \"correspond\", \"corresponds\", \"corresponds\", \"corresponds\", \"corruption\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"county\", \"county\", \"county\", \"county\", \"crawled\", \"crawled\", \"crawling\", \"crawling\", \"created\", \"created\", \"created\", \"created\", \"created\", \"created\", \"created\", \"created\", \"created\", \"created\", \"created\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"crime\", \"crime\", \"critical\", \"crypto\", \"crypto\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"curated\", \"customer\", \"customer\", \"customer\", \"da\", \"damage\", \"damage\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"datafiniti\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dc\", \"de\", \"de\", \"deal\", \"death\", \"death\", \"debate\", \"debt\", \"decrease\", \"deep\", \"deep\", \"deep\", \"defense\", \"defense\", \"demand\", \"demand\", \"denotes\", \"department\", \"department\", \"department\", \"department\", \"department\", \"dependent\", \"depth\", \"depth\", \"depth\", \"derive\", \"derive\", \"describing\", \"describing\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"design\", \"determines\", \"determining\", \"diabetes\", \"diagnosis\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"difficulty\", \"difficulty\", \"digit\", \"digit\", \"digit\", \"dioxide\", \"direction\", \"direction\", \"direction\", \"director\", \"disability\", \"disaster\", \"disclosure\", \"district\", \"district\", \"district\", \"district\", \"divide\", \"dog\", \"dot\", \"double\", \"double\", \"double\", \"driver\", \"driving\", \"driving\", \"drug\", \"drug\", \"earned\", \"eastern\", \"easy\", \"easy\", \"easy\", \"education\", \"education\", \"educational\", \"educational\", \"el\", \"el\", \"election\", \"election\", \"election\", \"election\", \"else\", \"else\", \"else\", \"email\", \"email\", \"embedded\", \"emission\", \"employed\", \"employee\", \"employee\", \"employer\", \"employment\", \"en\", \"en\", \"encoded\", \"energy\", \"energy\", \"energy\", \"enforcement\", \"enforcement\", \"english\", \"english\", \"enjoy\", \"enjoy\", \"enrollment\", \"enrollment\", \"episode\", \"equity\", \"estate\", \"estate\", \"ever\", \"ever\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"everyday\", \"ex\", \"exam\", \"exam\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"excel\", \"excel\", \"exchange\", \"execution\", \"executive\", \"exercise\", \"exists\", \"exists\", \"expert\", \"extension\", \"extension\", \"extracting\", \"extracting\", \"extraction\", \"facebook\", \"facility\", \"fair\", \"family\", \"family\", \"family\", \"fan\", \"fantasy\", \"fantasy\", \"fatality\", \"fbi\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"federal\", \"federal\", \"feel\", \"feel\", \"fifa\", \"figure\", \"figure\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"filled\", \"filled\", \"film\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finished\", \"fire\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fit\", \"fit\", \"flavor\", \"float\", \"float\", \"florida\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"food\", \"food\", \"food\", \"football\", \"forecast\", \"forecasting\", \"former\", \"formula\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"frame\", \"francisco\", \"francisco\", \"fraud\", \"french\", \"french\", \"friendly\", \"friendly\", \"front\", \"front\", \"front\", \"front\", \"fuel\", \"game\", \"game\", \"game\", \"gas\", \"gdp\", \"gene\", \"generalise\", \"genre\", \"genre\", \"genre\", \"georgia\", \"german\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"google\", \"google\", \"google\", \"google\", \"gov\", \"gov\", \"gov\", \"gov\", \"government\", \"government\", \"government\", \"govt\", \"granted\", \"graph\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"ground\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"grouped\", \"growth\", \"growth\", \"growth\", \"growth\", \"gz\", \"habit\", \"habit\", \"habit\", \"hand\", \"hand\", \"hand\", \"handwritten\", \"hard\", \"harvard\", \"harvard\", \"headline\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"higher\", \"higher\", \"higher\", \"higher\", \"highway\", \"hill\", \"hint\", \"hit\", \"hit\", \"hit\", \"horizontal\", \"horse\", \"horse\", \"hotel\", \"household\", \"housing\", \"html\", \"html\", \"html\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"humidity\", \"humidity\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"ieee\", \"illness\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"imagenet\", \"imdb\", \"imdb\", \"impact\", \"impact\", \"impact\", \"impact\", \"impact\", \"impossible\", \"improving\", \"improving\", \"inception\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"info\", \"info\", \"info\", \"info\", \"info\", \"info\", \"info\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"initially\", \"initially\", \"inside\", \"inside\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"instance\", \"instance\", \"institute\", \"institute\", \"institute\", \"instrument\", \"insurance\", \"insurance\", \"int\", \"int\", \"int\", \"int\", \"integer\", \"integer\", \"intelligence\", \"intelligent\", \"interactive\", \"interested\", \"interested\", \"interested\", \"interested\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"intersection\", \"investment\", \"involving\", \"island\", \"island\", \"island\", \"job\", \"job\", \"john\", \"john\", \"john\", \"john\", \"join\", \"joining\", \"journal\", \"json\", \"json\", \"json\", \"justice\", \"justice\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"karen\", \"kera\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"km\", \"km\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"la\", \"la\", \"lab\", \"label\", \"label\", \"label\", \"label\", \"label\", \"lang\", \"language\", \"language\", \"language\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"largest\", \"largest\", \"largest\", \"lat\", \"latitude\", \"latitude\", \"latitude\", \"launch\", \"law\", \"law\", \"law\", \"layer\", \"leader\", \"leaderboard\", \"leading\", \"leading\", \"leading\", \"leading\", \"leading\", \"league\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learned\", \"learned\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"lee\", \"lee\", \"left\", \"left\", \"li\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"line\", \"linear\", \"linear\", \"list\", \"list\", \"list\", \"list\", \"list\", \"list\", \"list\", \"listing\", \"listing\", \"listing\", \"localisation\", \"logistic\", \"lon\", \"longitude\", \"longitude\", \"longitude\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"luck\", \"luck\", \"magazine\", \"magazine\", \"mainly\", \"mainly\", \"mainly\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"manner\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"market\", \"market\", \"marketing\", \"mass\", \"mass\", \"massachusetts\", \"match\", \"match\", \"matching\", \"matching\", \"matter\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"medical\", \"medical\", \"medical\", \"medicine\", \"medicine\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"member\", \"member\", \"mentioned\", \"mentioned\", \"mentioned\", \"menu\", \"message\", \"message\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"mexico\", \"mexico\", \"michigan\", \"michigan\", \"mile\", \"mile\", \"mine\", \"mine\", \"mine\", \"missing\", \"missing\", \"missing\", \"missing\", \"mistake\", \"mnist\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modify\", \"molecular\", \"monitor\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"mortality\", \"motor\", \"movie\", \"movie\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"murder\", \"museum\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"nan\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"nationality\", \"nationality\", \"nba\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"neighborhood\", \"neither\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"news\", \"news\", \"news\", \"news\", \"news\", \"news\", \"news\", \"newspaper\", \"newspaper\", \"nice\", \"nominal\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"normal\", \"normal\", \"normal\", \"north\", \"north\", \"northern\", \"northern\", \"notice\", \"null\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"numeric\", \"numeric\", \"numeric\", \"numerical\", \"numerical\", \"numerous\", \"ny\", \"ny\", \"nyc\", \"object\", \"object\", \"object\", \"object\", \"observed\", \"occupation\", \"offensive\", \"offensive\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"opened\", \"opponent\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"others\", \"others\", \"others\", \"others\", \"others\", \"others\", \"others\", \"otherwise\", \"otherwise\", \"output\", \"output\", \"overflow\", \"owe\", \"package\", \"package\", \"package\", \"package\", \"package\", \"page\", \"page\", \"page\", \"page\", \"page\", \"pain\", \"pain\", \"pakistan\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"parameter\", \"parameter\", \"park\", \"park\", \"park\", \"park\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"partial\", \"participant\", \"participant\", \"participating\", \"partner\", \"passenger\", \"passenger\", \"past\", \"past\", \"past\", \"past\", \"patient\", \"patient\", \"patient\", \"patient\", \"pay\", \"pay\", \"penalty\", \"pennsylvania\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"percent\", \"percent\", \"percentage\", \"percentage\", \"percentage\", \"percentage\", \"performed\", \"performed\", \"performed\", \"permission\", \"pipeline\", \"plan\", \"plan\", \"plane\", \"plane\", \"planet\", \"planning\", \"planning\", \"planning\", \"plate\", \"play\", \"play\", \"played\", \"played\", \"player\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pokemon\", \"police\", \"police\", \"policing\", \"population\", \"population\", \"positive\", \"positive\", \"post\", \"post\", \"post\", \"post\", \"post\", \"postcode\", \"potentially\", \"poverty\", \"pre\", \"pre\", \"pre\", \"precipitation\", \"predictor\", \"predicts\", \"predicts\", \"prefer\", \"premier\", \"president\", \"presidential\", \"press\", \"prevention\", \"previously\", \"previously\", \"previously\", \"previously\", \"price\", \"price\", \"prize\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"producer\", \"product\", \"product\", \"product\", \"product\", \"profit\", \"profit\", \"profit\", \"promptcloud\", \"promptcloud\", \"proper\", \"property\", \"property\", \"property\", \"property\", \"property\", \"protein\", \"provides\", \"provides\", \"provides\", \"provides\", \"provides\", \"provides\", \"pt\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"published\", \"published\", \"published\", \"published\", \"published\", \"purchase\", \"pushing\", \"quantity\", \"queen\", \"queen\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"race\", \"race\", \"racing\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rated\", \"rating\", \"rating\", \"rating\", \"rating\", \"raw\", \"raw\", \"raw\", \"raw\", \"raw\", \"ray\", \"recall\", \"receiving\", \"receiving\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recommend\", \"recommendation\", \"recommendation\", \"recommendation\", \"record\", \"record\", \"record\", \"record\", \"record\", \"record\", \"reddit\", \"reddit\", \"refers\", \"refers\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"removed\", \"removed\", \"removed\", \"removed\", \"report\", \"report\", \"report\", \"report\", \"report\", \"representation\", \"representation\", \"representation\", \"representation\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"reserve\", \"reserve\", \"residual\", \"respondent\", \"response\", \"response\", \"response\", \"response\", \"restaurant\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retrieve\", \"retrieved\", \"revenue\", \"revenue\", \"review\", \"review\", \"review\", \"reviewer\", \"revised\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"risk\", \"risk\", \"risk\", \"robot\", \"robust\", \"robust\", \"robust\", \"rock\", \"rock\", \"route\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"salary\", \"salary\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"san\", \"san\", \"saving\", \"saving\", \"saving\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"science\", \"science\", \"science\", \"science\", \"science\", \"science\", \"score\", \"score\", \"score\", \"score\", \"score\", \"scored\", \"scraper\", \"scrapped\", \"scrapped\", \"script\", \"script\", \"script\", \"script\", \"searching\", \"season\", \"season\", \"secured\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"selecting\", \"sensor\", \"sentence\", \"sentence\", \"sentiment\", \"sentiment\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"severity\", \"sex\", \"sex\", \"shall\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"sheet\", \"shooting\", \"shot\", \"simonyan\", \"simulation\", \"situation\", \"situation\", \"situation\", \"situation\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"snow\", \"soccer\", \"society\", \"software\", \"software\", \"solar\", \"sold\", \"sold\", \"sometimes\", \"sometimes\", \"sometimes\", \"sometimes\", \"song\", \"sort\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"south\", \"south\", \"south\", \"south\", \"southern\", \"southern\", \"space\", \"space\", \"sparse\", \"speaker\", \"speaker\", \"specie\", \"specie\", \"specific\", \"specific\", \"specific\", \"specific\", \"specification\", \"specification\", \"speech\", \"speech\", \"spent\", \"spent\", \"sport\", \"sport\", \"sport\", \"st\", \"st\", \"st\", \"stack\", \"star\", \"star\", \"start\", \"start\", \"start\", \"start\", \"started\", \"started\", \"started\", \"started\", \"startup\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"station\", \"station\", \"station\", \"station\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"stats\", \"stats\", \"stats\", \"status\", \"status\", \"status\", \"status\", \"status\", \"status\", \"status\", \"status\", \"stock\", \"stock\", \"stock\", \"street\", \"street\", \"street\", \"street\", \"string\", \"string\", \"string\", \"strongly\", \"student\", \"student\", \"student\", \"student\", \"student\", \"subject\", \"subject\", \"subject\", \"subreddit\", \"sun\", \"sun\", \"supervised\", \"supervised\", \"surface\", \"survey\", \"survey\", \"survey\", \"swedish\", \"swedish\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"table\", \"table\", \"table\", \"table\", \"table\", \"tag\", \"tag\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"taken\", \"taken\", \"taken\", \"taken\", \"taken\", \"taken\", \"taken\", \"talk\", \"talk\", \"tar\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"tech\", \"tech\", \"tech\", \"technical\", \"technical\", \"technical\", \"technical\", \"technology\", \"technology\", \"technology\", \"tested\", \"texas\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"theft\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timestamp\", \"timestamp\", \"timestamp\", \"token\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"tower\", \"tower\", \"tract\", \"trade\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trained\", \"trained\", \"trained\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transaction\", \"transaction\", \"transcript\", \"transferable\", \"translation\", \"transport\", \"transportation\", \"treatment\", \"truly\", \"trump\", \"tv\", \"tweet\", \"tweet\", \"twitter\", \"twitter\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"txt\", \"txt\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"uc\", \"un\", \"un\", \"unemployment\", \"unit\", \"unit\", \"unit\", \"unit\", \"united\", \"united\", \"united\", \"university\", \"unzip\", \"upcoming\", \"upcoming\", \"url\", \"url\", \"url\", \"usd\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"vast\", \"vector\", \"vehicle\", \"vehicle\", \"version\", \"version\", \"version\", \"version\", \"version\", \"vgg\", \"video\", \"video\", \"video\", \"video\", \"violent\", \"virginia\", \"vision\", \"visual\", \"visual\", \"vol\", \"vote\", \"vote\", \"voter\", \"wang\", \"want\", \"want\", \"want\", \"want\", \"warranty\", \"washington\", \"washington\", \"watch\", \"watch\", \"weapon\", \"weather\", \"weather\", \"weather\", \"weather\", \"web\", \"web\", \"web\", \"web\", \"web\", \"web\", \"web\", \"website\", \"website\", \"website\", \"website\", \"website\", \"website\", \"website\", \"website\", \"weight\", \"weight\", \"weight\", \"weight\", \"went\", \"west\", \"west\", \"western\", \"western\", \"whichever\", \"wiki\", \"wikipedia\", \"wikipedia\", \"wikipedia\", \"win\", \"wind\", \"wind\", \"wine\", \"winner\", \"winter\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"without\", \"without\", \"without\", \"without\", \"woman\", \"woman\", \"woman\", \"word\", \"word\", \"word\", \"worker\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yes\", \"yes\", \"yes\", \"yet\", \"york\", \"york\", \"york\", \"youtube\", \"youtube\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zisserman\", \"zurich\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [12, 3, 9, 11, 10, 4, 2, 13, 1, 5, 8, 7, 14, 15, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el310631406290521664322178703634\", ldavis_el310631406290521664322178703634_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el310631406290521664322178703634\", ldavis_el310631406290521664322178703634_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el310631406290521664322178703634\", ldavis_el310631406290521664322178703634_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "11     0.110352 -0.003892       1        1  24.790896\n",
       "2      0.065361  0.050578       2        1  15.858511\n",
       "8     -0.000903  0.097607       3        1  11.594895\n",
       "10     0.027218  0.045848       4        1   9.469566\n",
       "9      0.087763 -0.011657       5        1   5.609096\n",
       "3      0.070114 -0.001216       6        1   5.585751\n",
       "1     -0.013335  0.072656       7        1   4.343614\n",
       "12     0.011623 -0.166851       8        1   3.963540\n",
       "0      0.033341  0.044019       9        1   3.696292\n",
       "4      0.054354 -0.033247      10        1   3.579708\n",
       "7      0.066057  0.018123      11        1   3.256440\n",
       "6      0.090743  0.010414      12        1   3.111095\n",
       "13     0.001203  0.107032      13        1   3.103238\n",
       "14    -0.453551  0.120184      14        1   1.176575\n",
       "5     -0.150340 -0.349598      15        1   0.860784, topic_info=             Term         Freq        Total Category  logprob  loglift\n",
       "580    university  1119.000000  1119.000000  Default  30.0000  30.0000\n",
       "773   description   606.000000   606.000000  Default  29.0000  29.0000\n",
       "567         state   866.000000   866.000000  Default  28.0000  28.0000\n",
       "604           csv  1246.000000  1246.000000  Default  27.0000  27.0000\n",
       "401           yet   359.000000   359.000000  Default  26.0000  26.0000\n",
       "...           ...          ...          ...      ...      ...      ...\n",
       "109        column    14.349590  1076.971430  Topic15  -4.9061   0.4369\n",
       "74           time    15.349687  1697.187656  Topic15  -4.8387   0.0494\n",
       "339          much    10.608287   179.940131  Topic15  -5.2082   1.9241\n",
       "472     following    11.376282   546.930882  Topic15  -5.1383   0.8823\n",
       "1200      created    10.608601   255.068516  Topic15  -5.2081   1.5752\n",
       "\n",
       "[967 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "1999      3  0.968548   accident\n",
       "1999     14  0.029350   accident\n",
       "2         1  0.197898   accuracy\n",
       "2         8  0.520188   accuracy\n",
       "2        10  0.214860   accuracy\n",
       "...     ...       ...        ...\n",
       "848       8  0.151876        zip\n",
       "848      12  0.359705        zip\n",
       "848      14  0.023980        zip\n",
       "2919      8  0.955303  zisserman\n",
       "2888      9  0.958099     zurich\n",
       "\n",
       "[2390 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[12, 3, 9, 11, 10, 4, 2, 13, 1, 5, 8, 7, 14, 15, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
